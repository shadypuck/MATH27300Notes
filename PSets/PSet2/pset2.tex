\documentclass[../psets.tex]{subfiles}

\pagestyle{main}
\renewcommand{\leftmark}{Problem Set \thesection}
\stepcounter{section}
\setenumerate[1]{label={\textbf{\arabic*.}}}
\setenumerate[2]{label={(\arabic*)}}

\begin{document}




\section{Linear Algebra}
\subsection*{Required Problems}
\begin{enumerate}
    \item \marginnote{10/19:}This question helps to complete the computations omitted in class. In deriving the Kepler orbits for the two-body problem, we have successfully reduced the differential equation satisfied by the curve $r=r(\varphi)$ to
    \begin{equation*}
        \left( \dv{r}{\varphi} \right)^2+r^2 = \frac{2GMr^3}{l_0^2}+\frac{2Er^4}{ml_0^2}
    \end{equation*}
    Show that the function $\mu=1/r$ satisfies the differential equation
    \begin{equation*}
        \left( \dv{\mu}{\varphi} \right)^2+\mu^2 = \frac{2GM\mu}{l_0^2}+\frac{2E}{ml_0^2}
    \end{equation*}
    By differentiating with respect to $\varphi$ again, this reduces to either $\dv*{\mu}{\varphi}=0$ or
    \begin{equation*}
        \dv[2]{\mu}{\varphi}+\mu-\frac{GM}{l_0^2} = 0
    \end{equation*}
    Find the general solution of the latter, hence conclude that $r=r(\varphi)$ represents a conic section. \emph{Hint}: There is a very obvious particular solution.
    \begin{proof}
        We begin from the first differential equation and substitute $\mu=1/r$ in the last step to yield the desired result.
        \begin{align*}
            \left( \dv{r}{\varphi} \right)^2+r^2 &= \frac{2GMr^3}{l_0^2}+\frac{2Er^4}{ml_0^2}\\
            \left( -\frac{1}{r^2}\dv{r}{\varphi} \right)^2+\frac{1}{r^2} &= \frac{2GM}{l_0^2}\frac{1}{r}+\frac{2E}{ml_0^2}\\
            \left[ \dv{\varphi}(\frac{1}{r}) \right]^2+\left( \frac{1}{r} \right)^2 &= \frac{2GM}{l_0^2}\frac{1}{r}+\frac{2E}{ml_0^2}\\
            \left( \dv{\mu}{\varphi} \right)^2+\mu^2 &= \frac{2GM\mu}{l_0^2}+\frac{2E}{ml_0^2}
        \end{align*}
        The homogeneous version of the final differential equation is entirely analogous to the harmonic oscillator problem and thus has general (real) solution
        \begin{equation*}
            \mu(\varphi) = \epsilon\cos(\varphi-\varphi_0)
        \end{equation*}
        for $\epsilon,\varphi_0\in\R$. By inspection, we can take as our particular solution to the inhomogeneous system
        \begin{equation*}
            \mu(\varphi) = \frac{GM}{l_0^2}
        \end{equation*}
        since it's second derivative (as a constant) is zero and it is the opposite of the inhomogeneous term. Thus, the general solution to the original inhomogeneous system is
        \begin{align*}
            \mu(\varphi) &= \frac{GM}{l_0^2}+\epsilon\cos(\varphi-\varphi_0)\\
            r(\varphi) &= \frac{1}{GM/l_0^2+\epsilon\cos(\varphi-\varphi_0)}\\
            &= \frac{\epsilon(l_0^2/GM\epsilon)}{1+\epsilon\cos(\varphi-\varphi_0)}
        \end{align*}
        which is exactly the polar form of the conic section with eccentricity $\epsilon$ and directrix $l_0^2/GM\epsilon$.
    \end{proof}
    \item The general formula for the inverse of an $n\times n$ invertible matrix is very lengthy. However, for a $2\times 2$ matrix
    \begin{equation*}
        \begin{pmatrix}
            a & b\\
            c & d\\
        \end{pmatrix}
    \end{equation*}
    satisfying $ad-bc\neq 0$, there is a very simple formula. Try to find it; this could be very helpful if you can remember it.
    \begin{proof}
        Let $A$ be the matrix given in the problem statement. We can determine $A^{-1}$ by inspection as follows.\par
        Let's focus on the right column of $A^{-1}$ first, which we can denote $(x,y)^T$. We want $ax+by=0$. One nice solution to this equation is $x=-b$ and $y=a$. Similarly, we can take the left column of $A^{-1}$ to be $(d,-c)^T$. This choice of entries for $A^{-1}$ yield the 0s in the right places, but the elements that should be 1 are instead $\det A=ad-bc$. Thus, we divide $A^{-1}$ by $\det A$. This yields the following final formula
        \begin{equation*}
            \boxed{
                A^{-1} = \frac{1}{ad-bc}
                \begin{pmatrix}
                    d & -b\\
                    -c & a\\
                \end{pmatrix}
            }
        \end{equation*}
        As a quick check, we have that
        \begin{align*}
            AA^{-1} &= \frac{1}{ad-bc}
            \begin{pmatrix}
                a & b\\
                c & d\\
            \end{pmatrix}
            \begin{pmatrix}
                d & -b\\
                -c & a\\
            \end{pmatrix}&
                A^{-1}A &= \frac{1}{ad-bc}
                \begin{pmatrix}
                    d & -b\\
                    -c & a\\
                \end{pmatrix}
                \begin{pmatrix}
                    a & b\\
                    c & d\\
                \end{pmatrix}\\
            &= \frac{1}{ad-bc}
            \begin{pmatrix}
                ad-bc & 0\\
                0 & ad-bc\\
            \end{pmatrix}&
                &= \frac{1}{ad-bc}
                \begin{pmatrix}
                    ad-bc & 0\\
                    0 & ad-bc\\
                \end{pmatrix}\\
            &=
            \begin{pmatrix}
                1 & 0\\
                0 & 1\\
            \end{pmatrix}&
                &=
                \begin{pmatrix}
                    1 & 0\\
                    0 & 1\\
                \end{pmatrix}
        \end{align*}
        as expected.
    \end{proof}
    \item Compute the determinant of the following matrices. Determine whether they are invertible or not.
    \begin{align*}
        A &=
        \begin{pmatrix}
            1 & 2 & 3\\
            4 & 5 & 6\\
            7 & 8 & 9\\
        \end{pmatrix}&
        B &=
        \begin{pmatrix}
            2 & 2 & 3 & 6\\
            1 & 3 & 4 & 2\\
            0 & 0 & -1 & 2\\
            0 & 0 & 1 & 2\\
        \end{pmatrix}&
        C &=
        \begin{pmatrix}
            -1 & 2 & 1\\
            3 & -1 & 2\\
            2 & 1 & 3\\
        \end{pmatrix}
    \end{align*}
    \begin{proof}
        We have that
        \begin{align*}
            \det A &= 1[5\cdot 9-6\cdot 8]-2[4\cdot 9-6\cdot 7]+3[4\cdot 8-5\cdot 7]\\
            \Aboxed{\det A &= 0}
        \end{align*}
        so \fbox{$A$ is not invertible.}\par
        Since $B$ is block upper triangular, we know that
        \begin{align*}
            \det B &= \det B_1\cdot\det B_2\\
            &= [2\cdot 3-2\cdot 1]\cdot[-1\cdot 2-2\cdot 1]\\
            \Aboxed{\det B &= -16}
        \end{align*}
        so \fbox{$B$ is invertible.}\par
        We have that
        \begin{align*}
            \det C &= -1[(-1)(3)-(2)(1)]-2[(3)(3)-(2)(2)]+1[(3)(1)-(-1)(2)]\\
            \Aboxed{\det C &= 0}
        \end{align*}
        so \fbox{$C$ is not invertible.}
    \end{proof}
    \item Determine whether the following linear systems admit solution(s); if they do, write down the solution (or the formula for the general solution).
    \begin{enumerate}
        \item 
        \begin{equation*}
            \begin{pmatrix}
                1 & 2\\
                2 & -1\\
            \end{pmatrix}
            \begin{pmatrix}
                x^1\\
                x^2\\
            \end{pmatrix}
            =
            \begin{pmatrix}
                -1\\
                1\\
            \end{pmatrix}
        \end{equation*}
        \begin{proof}
            By inspection, $A$ is a dimension 2 matrix of rank 2, so it \fbox{admits a unique solution.} We now row-reducing the augmented matrix.
            \begin{equation*}
                \begin{pNiceArray}{cc|c}
                    1 & 2 & -1\\
                    2 & -1 & 1\\
                \end{pNiceArray}
                \cong
                \begin{pNiceArray}{cc|c}
                    1 & 0 & \frac{1}{5}\\
                    0 & 1 & -\frac{3}{5}\\
                \end{pNiceArray}
            \end{equation*}
            Therefore, the solution is
            \begin{equation*}
                \boxed{
                    x =
                    \begin{pNiceMatrix}
                        \frac{1}{5}\\
                        -\frac{3}{5}\\
                    \end{pNiceMatrix}
                }
            \end{equation*}
        \end{proof}
        \item 
        \begin{equation*}
            \begin{pmatrix}
                -1 & 2 & 1\\
                3 & -1 & 2\\
                2 & 1 & 3\\
            \end{pmatrix}
            \begin{pmatrix}
                x^1\\
                x^2\\
                x^3\\
            \end{pmatrix}
            =
            \begin{pmatrix}
                1\\
                2\\
                3\\
            \end{pmatrix}
        \end{equation*}
        \begin{proof}
            By inspection, $A$ is a dimension 3 matrix of rank 2 and the $b$ vector is in the column space of $A$, so it \fbox{admits a family of solutions.} We now row-reducing the augmented matrix.
            \begin{equation*}
                \begin{pNiceArray}{ccc|c}
                    -1 & 2 & 1 & 1\\
                    3 & -1 & 2 & 2\\
                    2 & 1 & 3 & 3\\
                \end{pNiceArray}
                \cong
                \begin{pNiceArray}{ccc|c}
                    1 & 0 & 1 & 1\\
                    0 & 1 & 1 & 1\\
                    0 & 0 & 0 & 0
                \end{pNiceArray}
            \end{equation*}
            Therefore, the family of solutions is given by
            \begin{equation*}
                \boxed{
                    x =
                    \begin{pmatrix}
                        1-x^3\\
                        1-x^3\\
                        x^3\\
                    \end{pmatrix}
                }
            \end{equation*}
            for $x^3\in\R$.
        \end{proof}
        \item 
        \begin{equation*}
            \begin{pmatrix}
                -1 & 2 & 1\\
                3 & -1 & 2\\
                2 & 1 & 3\\
            \end{pmatrix}
            \begin{pmatrix}
                x^1\\
                x^2\\
                x^3\\
            \end{pmatrix}
            =
            \begin{pmatrix}
                1\\
                0\\
                1\\
            \end{pmatrix}
        \end{equation*}
        \begin{proof}
            No promising solution immediately appears by inspection, so we row reduce and evaluate the results.
            \begin{equation*}
                \begin{pNiceArray}{ccc|c}
                    -1 & 2 & 1 & 1\\
                    3 & -1 & 2 & 0\\
                    2 & 1 & 3 & 1\\
                \end{pNiceArray}
                \cong
                \begin{pNiceArray}{ccc|c}
                    1 & 0 & 1 & \frac{1}{5}\\
                    0 & 1 & 1 & \frac{3}{5}\\
                    0 & 0 & 0 & 0
                \end{pNiceArray}
            \end{equation*}
            It follows that $A$ \fbox{admits a family of solutions.} In particular, these are given by
            \begin{equation*}
                \boxed{
                    x =
                    \begin{pNiceMatrix}
                        \frac{1}{5}-x^3\\
                        \frac{3}{5}-x^3\\
                        x^3\\
                    \end{pNiceMatrix}
                }
            \end{equation*}
            for $x^3\in\R$.
        \end{proof}
    \end{enumerate}
    \item Find the connecting matrix from the basis $
        \begin{pmatrix}
            p_1 & p_2 & p_3\\
        \end{pmatrix}
    $ to the new basis $
        \begin{pmatrix}
            q_1 & q_2 & q_3\\
        \end{pmatrix}
    $, where
    \begin{align*}
        \begin{pmatrix}
            p_1 & p_2 & p_3\\
        \end{pmatrix}
        &=
        \begin{pmatrix}
            1 & 0 & -1\\
            1 & 2 & 0\\
            0 & -1 & 2\\
        \end{pmatrix}&
        \begin{pmatrix}
            q_1 & q_2 & q_3\\
        \end{pmatrix}
        &=
        \begin{pmatrix}
            0 & 1 & 0\\
            1 & -1 & 1\\
            0 & 0 & 1\\
        \end{pmatrix}
    \end{align*}
    That is, represent $q_1,q_2,q_3$ as linear combinations of $p_1,p_2,p_3$.
    \begin{proof}
        % Let's denote the connecting matrix by $R$. If $R$ is the described connecting matrix, then $Rp_1=q_1$, $Rp_2=q_2$, and $Rp_3=q_3$ (note that we can shuffle which vector gets sent to which other vector; it will just be easiest notationally ).

        % Suppose we know $Px_1=q_1$, \dots. Then $PX=Q$. If $QX=P$.

        $P$ is the connecting matrix from the standard basis $(e_1,e_2,e_3)$ to $(p_1,p_2,p_3)$. Likewise, $Q$ is the connecting matrix from $(e_1,e_2,e_3)$ to $(q_1,q_2,q_3)$. It follows that if we want $A$ to be the connecting matrix from $(p_1,p_2,p_3)$ to $(q_1,q_2,q_3)$, then we can do the transformation stepwise, i.e., take a vector represented in $(p_1,p_2,p_3)$ to its representation in $(e_1,e_2,e_3)$ using $P^{-1}$ and then to its representation in $(q_1,q_2,q_3)$ using $Q$. Indeed, the desired connecting matrix is
        \begin{align*}
            A &= QP^{-1}\\
            \Aboxed{A &= \frac{1}{5} {
                \begin{pmatrix}
                    -2 & 2 & -1\\
                    5 & 0 & 5\\
                    -1 & 1 & 2\\
                \end{pmatrix}
            }}
        \end{align*}
        Direct computation can confirm that $Ap_i=q_i$ for $i=1,2,3$.\par
        With respect to representing $q_1,q_2,q_3$ as linear combinations of $p_1,p_2,p_3$, we can solve the equations $q_i=Px_i$ for $i=1,2,3$ via row reduction, as in previous responses. The final expressions obtained are
        \begin{empheq}[box=\fbox]{align*}
            q_1 &= \frac{1}{5}(p_1+2p_2+p_3)&
            q_2 &= \frac{1}{5}(3p_1-4p_2-2p_3)&
            q_3 &= \frac{1}{5}(3p_1+p_2+3p_3)
        \end{empheq}
        Note that if we combine the coefficients above into a matrix $X$ such that $PX=Q$, then $A=PXP^{-1}=QXQ^{-1}$.
    \end{proof}
    \item Let $\theta\in[0,2\pi)$. The rotation through angle $\theta$ in the plane is represented by the matrix
    \begin{equation*}
        R(\theta) =
        \begin{pmatrix}
            \cos\theta & -\sin\theta\\
            \sin\theta & \cos\theta\\
        \end{pmatrix}
    \end{equation*}
    Compute its determinant, characteristic polynomial, and eigenvalues. Compute its eigenvectors in $\C^2$. You need to use the Euler formula $\e[i\theta]=\cos\theta+i\sin\theta$. For two angles $\theta,\varphi$, compute the product $R(\theta)R(\varphi)$ and represent it in terms of $\theta+\varphi$. What is the geometric meaning of this equality?
    \begin{proof}
        The determinant of $R$ is
        \begin{align*}
            \det R &= \cos^2\theta+\sin^2\theta\\
            \Aboxed{\det R &= 1}
        \end{align*}
        The characteristic polynomial of $R$ is
        \begin{align*}
            \chi_R(z) &= \det(R-zI)\\
            &= (\cos\theta-z)^2+\sin^2\theta\\
            &= z^2-2z\cos\theta+\cos\theta^2+\sin\theta^2\\
            \Aboxed{\chi_R(z) &= z^2-2z\cos\theta+1}
        \end{align*}
        The eigenvalues of $R$ are
        \begin{align*}
            0 &= \chi_R(\lambda)\\
            &= (\cos\theta-\lambda)^2+\sin^2\theta\\
            -\sin^2\theta &= (\cos\theta-\lambda)^2\\
            \pm i\sin\theta &= \pm(\cos\theta-\lambda)\\
            \lambda &= \cos\theta\pm i\sin\theta\\
            \Aboxed{\lambda &= \e[\pm i\theta]}
        \end{align*}
        It follows by solving the systems of equations
        \begin{align*}
            x^1\cos\theta-x^2\sin\theta &= \e[i\theta]x^1&
                y^1\cos\theta-y^2\sin\theta &= \e[-i\theta]y^1\\
            x^1\sin\theta+x^2\cos\theta &= \e[i\theta]x^2&
                y^1\sin\theta+y^2\cos\theta &= \e[-i\theta]y^2
        \end{align*}
        that the eigenvectors are
        \begin{empheq}[box=\fbox]{align*}
            x &=
            \begin{pmatrix}
                1\\
                -i\\
            \end{pmatrix}&
            y &=
            \begin{pmatrix}
                1\\
                i\\
            \end{pmatrix}
        \end{empheq}
        The product $R(\theta)R(\varphi)$ may be computed as follows.
        \begin{align*}
            R(\theta)R(\phi) &=
            \begin{pmatrix}
                \cos\theta & -\sin\theta\\
                \sin\theta & \cos\theta\\
            \end{pmatrix}
            \begin{pmatrix}
                \cos\varphi & -\sin\varphi\\
                \sin\varphi & \cos\varphi\\
            \end{pmatrix}\\
            &=
            \begin{pmatrix}
                \cos\theta\cos\varphi-\sin\theta\sin\varphi & -\cos\theta\sin\varphi-\sin\theta\cos\varphi\\
                \sin\theta\cos\varphi+\cos\theta\sin\varphi & -\sin\theta\sin\varphi+\cos\theta\cos\varphi\\
            \end{pmatrix}\\
            &=
            \begin{pmatrix}
                \cos(\theta+\varphi) & -\sin(\theta+\varphi)\\
                \sin(\theta+\varphi) & \cos(\theta+\varphi)\\
            \end{pmatrix}\\
            \Aboxed{R(\theta)R(\varphi) &= R(\theta+\phi)}
        \end{align*}
        The geometric meaning is that rotating through an angle $\theta$ and then through an additional angle $\varphi$ is the same as rotating through an angle $\theta+\varphi$ all at once.
    \end{proof}
    \stepcounter{enumi}
    \item Find the algebraic and geometric multiplicities of the eigenvalues of the following matrices.
    \begin{align*}
        A &=
        \begin{pmatrix}
            1 & 1 & 2\\
            0 & 1 & 2\\
            0 & 0 & 3\\
        \end{pmatrix}&
        B &=
        \begin{pmatrix}
            1 & 0 & 2\\
            0 & 1 & 2\\
            0 & 0 & 3\\
        \end{pmatrix}
    \end{align*}
    \begin{proof}
        We tackle $A$ first. $A$ is an upper triangular matrix. Thus, $\chi_A(\lambda)=\det(A-\lambda I)$ can be read directly off of the diagonal:
        \begin{equation*}
            \chi_A(\lambda) = (1-\lambda)^2(3-\lambda)
        \end{equation*}
        Thus, the eigenvalues are $\lambda=1,3$ with respective algebraic multiplicities
        \begin{empheq}[box=\fbox]{align*}
            \alpha_1 &= 2&
            \alpha_3 &= 1
        \end{empheq}
        It follows immediately that
        \begin{equation*}
            \boxed{\gamma_3 = 1}
        \end{equation*}
        and from the observation that $A-1I$ has 2 linearly independent columns that this $3\times 3$ matrix has a $3-2=1$ dimensional null space, i.e.,
        \begin{equation*}
            \boxed{\gamma_1 = 1}
        \end{equation*}
        The procedure for $B$ is almost entirely symmetric. Once again, $B$ is upper triangular, so
        \begin{equation*}
            \chi_B(\lambda) = (1-\lambda)^2(3-\lambda)
        \end{equation*}
        implying that
        \begin{empheq}[box=\fbox]{align*}
            \alpha_1 &= 2&
            \alpha_3 &= 1
        \end{empheq}
        There is a difference with respect to the geometric multiplicities, however. We still have
        \begin{equation*}
            \boxed{\gamma_3 = 1}
        \end{equation*}
        but since $A-I$ now has only 1 linearly independent column, we have
        \begin{equation*}
            \boxed{\gamma_1 = 2}
        \end{equation*}
        this time.
    \end{proof}
    \item Compute the Jordan normal form of the following $2\times 2$ matrices.
    \begin{align*}
        A &=
        \begin{pmatrix}
            2 & 1\\
            1 & 2\\
        \end{pmatrix}&
        B &=
        \begin{pmatrix}
            0 & -1\\
            1 & -2\\
        \end{pmatrix}
    \end{align*}
    Notice that you not only need to find all the Jordan blocks, but also need to find the Jordan basis matrix $Q$ such that $Q^{-1}AQ$ is in Jordan normal form.
    \begin{proof}
        We tackle $A$ first.\par
        Calculate the characteristic polynomial to begin.
        \begin{align*}
            \chi_A(z) &= \det(A-zI)\\
            &= z^2-4z+3\\
            &= (1-z)(3-z)
        \end{align*}
        It follows that the eigenvalues are
        \begin{align*}
            \lambda_1 &= 1&
            \lambda_2 &= 3
        \end{align*}
        Since these eigenvalues are distinct, we can fully diagonalize this matrix. Indeed, we can determine by inspection that suitable corresponding eigenvectors are
        \begin{align*}
            v_1 &=
            \begin{pmatrix}
                -1\\
                1\\
            \end{pmatrix}&
            v_2 &=
            \begin{pmatrix}
                1\\
                1\\
            \end{pmatrix}
        \end{align*}
        Therefore,
        \begin{empheq}[box=\fbox]{align*}
            Q &=
            \begin{pmatrix}
                -1 & 1\\
                1 & 1\\
            \end{pmatrix}&
            Q^{-1}AQ &=
            \begin{pmatrix}
                1 & 0\\
                0 & 3\\
            \end{pmatrix}
        \end{empheq}
        The procedure for $B$ is very much analogous to the procedure for $A$.\par
        Characteristic polynomial:
        \begin{align*}
            \chi_B(z) &= \det(B-zI)\\
            &= z^2+2z+1\\
            &= (1+z)^2
        \end{align*}
        Eigenvalue:
        \begin{equation*}
            \lambda = -1
        \end{equation*}
        By inspection of $B+I$, we can pick one eigenvector of $B$:
        \begin{equation*}
            v =
            \begin{pmatrix}
                1\\
                1\\
            \end{pmatrix}
        \end{equation*}
        We now solve $(B+I)u=v$. By inspection, this yields
        \begin{equation*}
            u =
            \begin{pmatrix}
                1\\
                0\\
            \end{pmatrix}
        \end{equation*}
        Therefore,
        \begin{empheq}[box=\fbox]{align*}
            Q &=
            \begin{pmatrix}
                1 & 1\\
                1 & 0\\
            \end{pmatrix}&
            Q^{-1}BQ &=
            \begin{pmatrix}
                -1 & 1\\
                0 & -1\\
            \end{pmatrix}
        \end{empheq}
    \end{proof}
    \item Compute the Jordan normal form of the following $3\times 3$ matrices.
    \begin{align*}
        A &=
        \begin{pmatrix}
            4 & -5 & 2\\
            5 & -7 & 3\\
            6 & -9 & 4\\
        \end{pmatrix}&
        B &=
        \begin{pmatrix}
            2 & -1 & -1\\
            2 & -1 & -2\\
            -1 & 1 & 2\\
        \end{pmatrix}&
        C &=
        \begin{pmatrix}
            2 & 1 & 3\\
            0 & 2 & -1\\
            0 & 0 & 2\\
        \end{pmatrix}
    \end{align*}
    Notice that you not only need to find all the Jordan blocks, but also need to find the Jordan basis matrix $Q$ such that $Q^{-1}AQ$ is in Jordan normal form. \emph{Hint}: These three matrices represent three different possibilities of nondiagonalizable Jordan normal forms of a $3\times 3$ matrix: $A$ reduces to $(2\times 2)\oplus(1\times 1)$ Jordan blocks with different eigenvalues, $B$ reduces to $(2\times 2)\oplus(1\times 1)$ Jordan blocks with the same eigenvalue, and $C$ reduces to a $3\times 3$ Jordan block.
    \begin{proof}
        We tackle $A$ first.\par
        Calculate the characteristic polynomial to begin.
        \begin{align*}
            \chi_A(z) ={}& \det(A-z I)\\
            % \begin{split}
            %     ={}& (4-z)[(-7-z)(4-z)-(3)(-9)]\\
            %     &-(-5)[(5)(4-z)-(3)(6)]\\
            %     &+(2)[(5)(-9)-(-7-z)(6)]
            % \end{split}\\
            ={}& -z^3+z^2\\
            ={}& z^2(1-z)
        \end{align*}
        It follows that the eigenvalues are
        \begin{align*}
            \lambda_1 &= \lambda_2 = 0&
            \lambda_3 &= 1
        \end{align*}
        We can solve for an eigenvector $v_1$ corresponding to $\lambda_1=\lambda_2=0$ using the augmented matrix and row reduction as follows.
        \begin{equation*}
            \begin{pNiceArray}{ccc|c}
                4 & -5 & 2 & 0\\
                5 & -7 & 3 & 0\\
                6 & -9 & 4 & 0\\
            \end{pNiceArray}
            \cong
            \begin{pNiceArray}{ccc|c}
                1 & 0 & -\frac{1}{3} & 0\\
                0 & 1 & -\frac{2}{3} & 0\\
                0 & 0 & 0 & 0\\
            \end{pNiceArray}
        \end{equation*}
        Thus, if we choose $v_1^3=3$, then the desired eigenvector is
        \begin{equation*}
            v_1 =
            \begin{pmatrix}
                1\\
                2\\
                3\\
            \end{pmatrix}
        \end{equation*}
        Similarly, we can solve for an eigenvector $v_3$ corresponding to $\lambda_3=1$ using the following. Note that to solve $Ax=1x$, we row-reduce $(A-I)x=0$.
        \begin{equation*}
            \begin{pNiceArray}{ccc|c}
                3 & -5 & 2 & 0\\
                5 & -8 & 3 & 0\\
                6 & -9 & 3 & 0\\
            \end{pNiceArray}
            \cong
            \begin{pNiceArray}{ccc|c}
                1 & 0 & -1 & 0\\
                0 & 1 & -1 & 0\\
                0 & 0 & 0 & 0\\
            \end{pNiceArray}
        \end{equation*}
        This yields
        \begin{equation*}
            v_3 =
            \begin{pmatrix}
                1\\
                1\\
                1\\
            \end{pmatrix}
        \end{equation*}
        We now solve the equation $(A-0I)u=v_1$ to find a generalized eigenvector $u$ corresponding to $\lambda_1=\lambda_2=0$. This can also be done with an augmented matrix.
        \begin{equation*}
            \begin{pNiceArray}{ccc|c}
                4 & -5 & 2 & 1\\
                5 & -7 & 3 & 2\\
                6 & -9 & 4 & 3\\
            \end{pNiceArray}
            \cong
            \begin{pNiceArray}{ccc|c}
                1 & 0 & -\frac{1}{3} & 0\\
                0 & 1 & -\frac{2}{3} & 0\\
                0 & 0 & 0 & 0\\
            \end{pNiceArray}
        \end{equation*}
        This yields
        \begin{equation*}
            u =
            \begin{pmatrix}
                0\\
                1\\
                3\\
            \end{pmatrix}
        \end{equation*}
        Therefore,
        \begin{empheq}[box=\fbox]{align*}
            Q &=
            \begin{pmatrix}
                1 & 0 & 1\\
                2 & 1 & 1\\
                3 & 3 & 1\\
            \end{pmatrix}&
            Q^{-1}AQ &=
            \begin{pmatrix}
                0 & 1 & 0\\
                0 & 0 & 0\\
                0 & 0 & 1\\
            \end{pmatrix}
        \end{empheq}
        The procedure for $B$ is very much analogous to the procedure for $A$.\par
        Characteristic polynomial:
        \begin{align*}
            \chi_B(z) &= \det(B-zI)\\
            &= -z^3+3z^2-3z+1\\
            &= (1-z)^3
        \end{align*}
        Eigenvalue:
        \begin{equation*}
            \lambda = 1
        \end{equation*}
        By inspection of
        \begin{equation*}
            B-I =
            \begin{pmatrix}
                1 & -1 & -1\\
                2 & -2 & -2\\
                -1 & 1 & 1\\
            \end{pmatrix}
        \end{equation*}
        we can pick two eigenvectors of $B$ corresponding to $\lambda$, i.e., two elements of the null space of the above matrix. In this subcase of the $3\times 3$ case, we always pick the first of these to be an element of the column space of $B-I$, as well. Thus, choose
        \begin{align*}
            v_1 &=
            \begin{pmatrix}
                1\\
                2\\
                -1\\
            \end{pmatrix}&
            v_2 &=
            \begin{pmatrix}
                1\\
                1\\
                0\\
            \end{pmatrix}
        \end{align*}
        We now solve $(B-\lambda I)u=v_1$. By inspection, this yields
        \begin{equation*}
            u =
            \begin{pmatrix}
                1\\
                0\\
                0\\
            \end{pmatrix}
        \end{equation*}
        Therefore,
        \begin{empheq}[box=\fbox]{align*}
            Q &=
            \begin{pmatrix}
                1 & 1 & 1\\
                2 & 0 & 1\\
                -1 & 0 & 0\\
            \end{pmatrix}&
            Q^{-1}BQ &=
            \begin{pmatrix}
                1 & 1 & 0\\
                0 & 1 & 0\\
                0 & 0 & 1\\
            \end{pmatrix}
        \end{empheq}
        The procedure for $C$ is likewise quite analogous.\par
        The matrix is upper triangular, so the eigenvalues are on the diagonal. It follows that
        \begin{equation*}
            \lambda = 2
        \end{equation*}
        is the sole eigenvalue. We can solve $(C-2I)v=0$ for one eigenvector $v$ by inspection, yielding
        \begin{equation*}
            v =
            \begin{pmatrix}
                1\\
                0\\
                0\\
            \end{pmatrix}
        \end{equation*}
        We can also solve $(C-2I)u_1=v$ by inspection to get
        \begin{equation*}
            u_1 =
            \begin{pmatrix}
                0\\
                1\\
                0\\
            \end{pmatrix}
        \end{equation*}
        One more time, we can solve $(C-2I)u_2=u_1$ by inspection to get
        \begin{equation*}
            u_2 =
            \begin{pmatrix}
                0\\
                3\\
                -1\\
            \end{pmatrix}
        \end{equation*}
        Therefore,
        \begin{empheq}[box=\fbox]{align*}
            Q &=
            \begin{pmatrix}
                1 & 0 & 0\\
                0 & 1 & 3\\
                0 & 0 & -1\\
            \end{pmatrix}&
            Q^{-1}CQ &=
            \begin{pmatrix}
                2 & 1 & 0\\
                0 & 2 & 1\\
                0 & 0 & 2\\
            \end{pmatrix}
        \end{empheq}
    \end{proof}
\end{enumerate}


\subsection*{Bonus Problems}
\begin{enumerate}
    \item You may find the characteristic root method for the second-order equation $y''+ay'+b=0$ quite abrupt. This problem helps you see where it comes from. The origin of this method is in fact a comparison with the linear recursive relation
    \begin{equation*}
        y_{n+2}+ay_{n+1}+by_n = 0
    \end{equation*}
    where $a,b$ are given complex numbers.
    \begin{enumerate}
        \item The linear recursive relation $y_{n+1}+ay_n=0$ gives rise to a geometric sequence
        \begin{equation*}
            y_0,y_0(-a),y_0(-a)^2,\dots
        \end{equation*}
        We now want to try to reduce the second-order recursive relation $y_{n+2}+ay_{n+1}+by_n=0$ to a first-order relation. Thus, we look for complex numbers $\lambda,\mu$ such that
        \begin{equation*}
            (y_{n+2}-\lambda y_{n+1})-\mu(y_{n+1}-\lambda y_n) = 0
        \end{equation*}
        Then $\lambda,\mu$ should be the roots of the characteristic polynomial
        \begin{equation*}
            X^2+aX+b
        \end{equation*}
        Taking $\lambda,\mu$ as known quantities, find the general formula for $y_n$, regarding $y_0,y_1$ as known quantities. \emph{Hint}: The sequence of numbers $y_{n+1}-\lambda y_n$ is a geometric sequence with ratio $\mu$. You should also discuss $\mu\neq\lambda$ and $\mu=\lambda$ separately.
        \begin{proof}
            We start with the case $\mu\neq\lambda$ here. Define a sequence of numbers $\{x_n\}$ by
            \begin{equation*}
                x_n := y_{n+1}-\lambda y_n
            \end{equation*}
            We have from the above that $x_n$ is recursively defined by
            \begin{equation*}
                x_{n+1}-\mu x_n = 0
                \quad\Longleftrightarrow\quad
                x_n = x_0\mu^n
            \end{equation*}
            Returning the substitution, we have that
            \begin{equation*}
                y_{n+1}-\lambda y_n = (y_1-\lambda y_0)\mu^n
            \end{equation*}
            Rearranging, we have that
            \begin{equation*}
                y_{n+1} = \lambda y_n+(y_1-\lambda y_0)\mu^n
            \end{equation*}
            In other words, to generate each new term, we multiply through the previous term by $\lambda$ and add to it a new term. Let's investigate the effect of this procedure on the first few terms past $y_0,y_1$. These terms are
            \begin{align*}
                \begin{split}
                    y_2 ={}& \lambda y_1+(y_1-\lambda y_0)\mu^1\\
                    &= (\underbrace{\lambda+\mu}_{p_1(\lambda,\mu)})y_1-\lambda\mu(\underbrace{1}_{q_1(\lambda,\mu)})y_0
                \end{split}\\
                \begin{split}
                    y_3 ={}& \lambda(\lambda+\mu)y_1-\lambda\mu\cdot\lambda y_0+(y_1-\lambda y_0)\mu^2\\
                    &= [\lambda(\lambda+\mu)+\mu^2]y_1-\lambda\mu\cdot[\lambda+\mu]y_0\\
                    &= (\underbrace{\lambda^2+\lambda\mu+\mu^2}_{p_2(\lambda,\mu)})y_1-\lambda\mu(\underbrace{\lambda+\mu}_{q_2(\lambda,\mu)})y_0
                \end{split}\\
                \begin{split}
                    y_4 ={}& \lambda(\lambda^2+\lambda\mu+\mu^2)y_1-\lambda\mu\cdot\lambda(\lambda+\mu)y_0+(y_1-\lambda y_0)\mu^3\\
                    &= [\lambda(\lambda^2+\lambda\mu+\mu^2)+\mu^3]y_1-\lambda\mu\cdot[\lambda(\lambda+\mu)+\mu^2]y_0\\
                    &= (\underbrace{\lambda^3+\lambda^2\mu+\lambda\mu^2+\mu^3}_{p_3(\lambda,\mu)})y_1-\lambda\mu(\underbrace{\lambda^2+\lambda\mu+\mu^2}_{q_3(\lambda,\mu)})y_0
                \end{split}
            \end{align*}
            There are a number of interesting things to note here. First of all, see that the first line defining each term comes straight from the recursive relation (i.e., is $\lambda$ times the previous term plus an auxiliary term). The second line is always a rewrite that combines all of the $y_1$ terms and all of the $y_0$ terms. The third line (if it exists; the second line for $y_2$ serves this purpose as well) is always a final simplification in terms of polynomials of $\lambda$ and $\mu$. We have taken the liberty of naming each of these final polynomials for purposes that will soon be revealed.\par
            Notice that the polynomials appear to be defined by the recursive relations\footnote{We can rigorously prove this via induction. Also note the familiar theme of "multiply through by $\lambda$ and add a term."}
            \begin{align*}
                p_n &= \lambda p_{n-1}+\mu^n&
                q_n &= \lambda q_{n-1}+\mu^{n-1}
            \end{align*}
            Also note that we factor $-\lambda\mu$ out of the $y_0$ term every time so that we have
            \begin{equation*}
                q_n = p_{n-1}
            \end{equation*}
            These polynomials have a very well-defined form; specifically,
            \begin{equation*}
                q_{n+1} = p_n
                = \sum_{i=0}^n\lambda^{n-i}\mu^i
                = \frac{\lambda^{n+1}-\mu^{n+1}}{\lambda-\mu}
            \end{equation*}
            This allows us to explicitly define the following general formula for $y_n$ in the $\mu\neq\lambda$ case.
            \begin{equation*}
                \boxed{y_n = \frac{\lambda^n-\mu^n}{\lambda-\mu}y_1-\lambda\mu\frac{\lambda^{n-1}-\mu^{n-1}}{\lambda-\mu}y_0}
            \end{equation*}
            I believe that this is along the lines of what Shao wanted us to do.\par
            Note, however, that there exists an alternate, much more logical way to derive this formula using linear algebra. In particular, we have from the recursive relation and the trivial reflexive equality $y_1=y_1$ that
            \begin{align*}
                \begin{pmatrix}
                    y_2\\
                    y_1\\
                \end{pmatrix}
                &=
                \begin{pmatrix}
                    -ay_1-by_0\\
                    y_1\\
                \end{pmatrix}\\
                &=
                \begin{pmatrix}
                    -a & -b\\
                    1 & 0\\
                \end{pmatrix}
                \begin{pmatrix}
                    y_1\\
                    y_0\\
                \end{pmatrix}
            \end{align*}
            It follows that
            \begin{equation*}
                \begin{pmatrix}
                    y^{n+1}\\
                    y^n\\
                \end{pmatrix}
                =
                \begin{pmatrix}
                    -a & -b\\
                    1 & 0\\
                \end{pmatrix}^n
                \begin{pmatrix}
                    y_1\\
                    y_0\\
                \end{pmatrix}
            \end{equation*}
            Diagonalizing the matrix raised to the $n^\text{th}$ power above (which we may call $A$) requires finding the roots of the (of course similarly named) characteristic polynomial
            \begin{align*}
                0 &= \chi_A(z)\\
                &= (-a-z)(-z)+b\\
                &= z^2+az+b
            \end{align*}
            But we have from the problem statement that the roots of this polynomial are the same $\lambda,\mu$ we've been working with this whole time. Thus, we have a new interpretation of these roots: As \emph{eigenvalues}. And to each of these distinct eigenvalues corresponds a distinct eigenvector, which we may find by inspection as follows. Consider $\lambda$ WLOG. The eigenvector $v$ corresponding to $\lambda$ must satisfy
            \begin{equation*}
                \begin{pmatrix}
                    -a-\lambda & -b\\
                    1 & -\lambda\\
                \end{pmatrix}
                \begin{pmatrix}
                    v^1\\
                    v^2\\
                \end{pmatrix}
                =
                \begin{pmatrix}
                    0\\
                    0\\
                \end{pmatrix}
            \end{equation*}
            If we choose $v^1=\lambda$ and $v^2=1$, then we of course have
            \begin{equation*}
                (1)(v^1)+(-\lambda)(v^2) = \lambda-\lambda
                = 0
            \end{equation*}
            But we actually also have
            \begin{equation*}
                (-a-\lambda)(v^1)+(-b)(1) = -\lambda^2-a\lambda-b
                = -(\lambda^2+a\lambda+b)
                = -0
                = 0
            \end{equation*}
            since $\lambda$ is a root of the characteristic polynomial! Thus, the eigenvectors corresponding to $\lambda,\mu$ are
            \begin{equation*}
                \begin{pmatrix}
                    \lambda\\
                    1\\
                \end{pmatrix},
                \begin{pmatrix}
                    \mu\\
                    1\\
                \end{pmatrix}
            \end{equation*}
            respectively. Thus, with what we have so far plus an assist from Problem 2, we can diagonalize $A$ to
            \begin{equation*}
                \begin{pmatrix}
                    -a & -b\\
                    1 & 0\\
                \end{pmatrix}
                = \frac{1}{\lambda-\mu}
                \begin{pmatrix}
                    \lambda & \mu\\
                    1 & 1\\
                \end{pmatrix}
                \begin{pmatrix}
                    \lambda & 0\\
                    0 & \mu\\
                \end{pmatrix}
                \begin{pmatrix}
                    1 & -\mu\\
                    -1 & \lambda\\
                \end{pmatrix}
            \end{equation*}
            It follows that
            \begin{align*}
                \begin{pmatrix}
                    y^{n+1}\\
                    y^n\\
                \end{pmatrix}
                &=
                \begin{pmatrix}
                    -a & -b\\
                    1 & 0\\
                \end{pmatrix}^n
                \begin{pmatrix}
                    y_1\\
                    y_0\\
                \end{pmatrix}\\
                &= \frac{1}{\lambda-\mu}
                \begin{pmatrix}
                    \lambda & \mu\\
                    1 & 1\\
                \end{pmatrix}
                \begin{pmatrix}
                    \lambda & 0\\
                    0 & \mu\\
                \end{pmatrix}^n
                \begin{pmatrix}
                    1 & -\mu\\
                    -1 & \lambda\\
                \end{pmatrix}
                \begin{pmatrix}
                    y_1\\
                    y_0\\
                \end{pmatrix}\\
                &= \frac{1}{\lambda-\mu}
                \begin{pmatrix}
                    \lambda & \mu\\
                    1 & 1\\
                \end{pmatrix}
                \begin{pmatrix}
                    \lambda^n & 0\\
                    0 & \mu^n\\
                \end{pmatrix}
                \begin{pmatrix}
                    y_1-\mu y_0\\
                    -(y_1-\lambda y_0)\\
                \end{pmatrix}\\
                &= \frac{1}{\lambda-\mu}
                \begin{pmatrix}
                    \lambda & \mu\\
                    1 & 1\\
                \end{pmatrix}
                \begin{pmatrix}
                    \lambda^n(y_1-\mu y_0)\\
                    -\mu^n(y_1-\lambda y_0)\\
                \end{pmatrix}\\
                &= \frac{1}{\lambda-\mu}
                \begin{pmatrix}
                    \lambda^{n+1}(y_1-\mu y_0)-\mu^{n+1}(y_1-\lambda y_0)\\
                    \lambda^n(y_1-\mu y_0)-\mu^n(y_1-\lambda y_0)\\
                \end{pmatrix}
            \end{align*}
            From the above, we can read that
            \begin{align*}
                y_n &= \frac{1}{\lambda-\mu}[\lambda^n(y_1-\mu y_0)-\mu^n(y_1-\lambda y_0)]\\
                &= \frac{\lambda^n-\mu^n}{\lambda-\mu}y_1-\frac{\lambda^n\mu-\lambda\mu^n}{\lambda-\mu}y_0\\
                &= \frac{\lambda^n-\mu^n}{\lambda-\mu}y_1-\lambda\mu\frac{\lambda^{n-1}-\mu^{n-1}}{\lambda-\mu}y_0
            \end{align*}
            in agreement with the original answer.\par\smallskip
            We now treat the case where $\mu=\lambda$. We will do this using linear algebra, alone, since this is a far more rigorous and overall superior method. That being said, a purely sequence-based method would also work here. Let's begin.\par
            The setup proceeds as before. However, this time we only have one distinct eigenvalue. Moreover, this eigenvalue only corresponds to one eigenvector since
            \begin{equation*}
                \begin{pmatrix}
                    -a-\mu & -b\\
                    1 & -\mu\\
                \end{pmatrix}
            \end{equation*}
            is a rank 1 matrix, not a rank 0 matrix. Thus, we must delve into JNF and generalized eigenvectors. In particular, we can still choose
            \begin{equation*}
                \begin{pmatrix}
                    \mu\\
                    1\\
                \end{pmatrix}
            \end{equation*}
            as an eigenvector, but we must solve
            \begin{equation*}
                \begin{pmatrix}
                    -a-\mu & -b\\
                    1 & -\mu\\
                \end{pmatrix}
                \begin{pmatrix}
                    u^1\\
                    u^2\\
                \end{pmatrix}
                =
                \begin{pmatrix}
                    \mu\\
                    1\\
                \end{pmatrix}
            \end{equation*}
            to obtain the following additional \emph{generalized} eigenvector.
            \begin{equation*}
                \begin{pmatrix}
                    -1\\
                    -2/\mu\\
                \end{pmatrix}
            \end{equation*}
            Therefore, with what we have so far, we can diagonalize $A$ to
            \begin{equation*}
                \begin{pmatrix}
                    -a & -b\\
                    1 & 0\\
                \end{pmatrix}
                =
                \begin{pmatrix}
                    \mu & -1\\
                    1 & -2/\mu\\
                \end{pmatrix}
                \begin{pmatrix}
                    \mu & 1\\
                    0 & \mu\\
                \end{pmatrix}
                \begin{pmatrix}
                    2/\mu & -1\\
                    1 & -\mu\\
                \end{pmatrix}
            \end{equation*}
            It follows that
            \begin{align*}
                \begin{pmatrix}
                    y^{n+1}\\
                    y^n\\
                \end{pmatrix}
                &=
                \begin{pmatrix}
                    \mu & -1\\
                    1 & -2/\mu\\
                \end{pmatrix}
                \begin{pmatrix}
                    \mu & 1\\
                    0 & \mu\\
                \end{pmatrix}^n
                \begin{pmatrix}
                    2/\mu & -1\\
                    1 & -\mu\\
                \end{pmatrix}
                \begin{pmatrix}
                    y_1\\
                    y_0\\
                \end{pmatrix}\\
                &=
                \begin{pmatrix}
                    \mu & -1\\
                    1 & -2/\mu\\
                \end{pmatrix}
                \begin{pmatrix}
                    \mu^n & n\mu^{n-1}\\
                    0 & \mu^n\\
                \end{pmatrix}
                \begin{pmatrix}
                    2y_1/\mu-y_0\\
                    y_1-\mu y_0\\
                \end{pmatrix}\\
                &=
                \begin{pmatrix}
                    \mu & -1\\
                    1 & -2/\mu\\
                \end{pmatrix}
                \begin{pmatrix}
                    (n+2)\mu^{n-1}y_1-(n+1)\mu^ny_0\\
                    \mu^ny_1-\mu^{n+1}y_0\\
                \end{pmatrix}\\
                &=
                \begin{pmatrix}
                    (n+1)\mu^ny_1-n\mu^{n+1}y_0\\
                    n\mu^{n-1}y_1-(n-1)\mu^ny_0\\
                \end{pmatrix}
            \end{align*}
            From the above, we can read off the following general formula for $y_n$ in the $\mu=\lambda$ case.
            \begin{equation*}
                \boxed{y_n = n\mu^{n-1}y_1-(n-1)\mu^ny_0}
            \end{equation*}
        \end{proof}
        \item Use the method of part (1) to find the general formula for the linear discursive relation
        \begin{equation*}
            y_{n+2}-2y_{n+1}+y_n = 0
        \end{equation*}
        Use the same method to find the general formula for the Fibonacci sequence
        \begin{equation*}
            F_{n+2} = F_{n+1}+F_n
        \end{equation*}
        \begin{proof}
            The first part is plug and chug; the Fibonacci sequence I covered in LinAlgNotes, Chapter 6.\par
            Last note: Somebody please explain to me how any of this makes the ODE-solving method make any more sense. I mean they're clearly related, but there's no Duhamel formula for sequences that I'm aware of!
        \end{proof}
    \end{enumerate}
    \item In this exercise, we aim to prove an important theorem in linear algebra:
    \begin{equation*}
        \textit{Complex Hermitian matrices are always diagonalizable.}
    \end{equation*}
    Here the term "Hermitian" means that the matrix equals its conjugate transpose. In terms of entries, this means that in general, $a_{ij}=\bar{a}_{ji}$. For example,
    \begin{equation*}
        \begin{pmatrix}
            2 & 1 & -i\\
            1 & 3 & -2i\\
            i & 2i & 1\\
        \end{pmatrix}
    \end{equation*}
    is Hermitian.
    \begin{enumerate}
        \item Let $\inp{\cdot}{\cdot}$ be the standard Hermitian inner product, that is, for $x,y\in\C^n$,
        \begin{equation*}
            \inp{x}{y} = \sum_{j=1}^nx^j\bar{y}^j
        \end{equation*}
        Show that for any $n\times n$ real matrix,
        \begin{equation*}
            \inp{Ax}{y} = \inp{x}{A^*y}
        \end{equation*}
        for any $x,y\in\C^n$, where $A^*$ denotes the conjugate transpose of $A$. For example,
        \begin{equation*}
            A =
            \begin{pmatrix}
                1 & 1 & 2i\\
                0 & 3+i & 3\\
                2 & 0 & 1\\
            \end{pmatrix}
            \quad\Longleftrightarrow\quad
            A^* =
            \begin{pmatrix}
                1 & 0 & 2\\
                1 & 3-i & 0\\
                -2i & 3 & 1\\
            \end{pmatrix}
        \end{equation*}
        \begin{proof}
            I almost surely have this all written up somewhere in LinAlgNotes, LADRNotes, or MATH20700Notes.
        \end{proof}
        \item Suppose now that $A$ is Hermitian. Use part (1) to show that any eigenvalue of $A$ must be a real number. Show further that if $x,y$ are eigenvectors corresponding to different eigenvalues, then $\inp{x}{y}=0$, that is, $x$ is orthogonal to $y$.
        \item Prove that every Hermitian matrix $A$ is diagonalizable. \emph{Hint}: Take any eigenvector $v_1$ of $A$. Decompose $\C^n$ into the direct sum of $\spn(v_1)$ and its orthogonal complement. Show that the orthogonal complement is an invariant subspace for $A$.
    \end{enumerate}
\end{enumerate}




\end{document}