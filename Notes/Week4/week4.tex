\documentclass[../notes.tex]{subfiles}

\pagestyle{main}
\renewcommand{\chaptermark}[1]{\markboth{\chaptername\ \thechapter\ (#1)}{}}
\setcounter{chapter}{3}

\begin{document}




\chapter{Linear Systems}
\section{Autonomous Linear Systems}
\begin{itemize}
    \item \marginnote{10/17:}Today: General theory for autonomous linear systems.
    \item Review session Wednesday (no new material).
    \item First midterm Friday.
    \begin{itemize}
        \item Test problems will be slight variations of homework problems or examples given in class.
    \end{itemize}
    \item \textbf{Linear autonomous system}: A system of $n$ linear equations written in the following form. \emph{Denoted by} $\bm{y'=Ay}$. \emph{Given by}
    \begin{align*}
        \begin{pmatrix}
            y^1\\
            \vdots\\
            y^n\\
        \end{pmatrix}'
        &= 
        \begin{pmatrix}
            a_{11} & \cdots & a_{1n}\\
            \vdots & \ddots & \vdots\\
            a_{n1} & \cdots & a_{nn}\\
        \end{pmatrix}
        \begin{pmatrix}
            y^1\\
            \vdots\\
            y^n\\
        \end{pmatrix}&
        y(0) &= 0
    \end{align*}
    \begin{itemize}
        \item Note that the $a_{ij}$'s are complex or real.
    \end{itemize}
    \item The explicit solution is given by $y(t)=\e[tA]y_0$.
    \begin{itemize}
        \item Recall that $\dv*{t}(\e[tA])=A\e[tA]$, as we can show via the power series expansion.
    \end{itemize}
    \item \textbf{Picard iteration}: We take
    \begingroup
    \allowdisplaybreaks
    \begin{align*}
        y'(t) &= Ay(t)\\
        \int_0^ty'(\tau)\dd{\tau} &= \int_0^tAy(\tau)\dd\tau\\
        y(t) &= y_0+\int_0^tAy(\tau_1)\dd\tau_1\\
        &= y_0+\int_0^tA\left[ y_0+\int_0^{\tau_1}Ay(\tau_2)\dd\tau_2 \right]\dd\tau_1\\
        &= y_0+tAy_0+\int_0^t\int_0^{\tau_1}A^2y(\tau_2)\dd\tau_2\dd\tau_1\\
        &= y_0+tAy_0+\int_0^t\int_0^{\tau_1}A^2\left[ y_0+\int_0^{\tau_2}Ay(\tau_3)\dd\tau_3 \right]\dd\tau_2\dd\tau_1\\
        &= y_0+tAy_0+\frac{t^2A^2}{2}+\int_0^t\int_0^{\tau_1}\int_0^{\tau_2}A^3y(\tau_3)\dd\tau_3\dd\tau_2\dd\tau_1\\
        &= \sum_{k=0}^m\frac{t^kA^k}{k!}y_0+A^{m+1}\underbrace{\int_0^t\cdots\int_0^{\tau_m}}_{m+1}y(\tau_{m+1})\dd\tau_{m+1}\cdots\dd\tau_1
    \end{align*}
    \endgroup
    \begin{itemize}
        \item We get from the second to the third line by substituting $y(t)$, as defined into the second line, into where it appears in the integral.
        \item This is one form of the Picard iteration. Another that's more consistent with other techniques we'll use later is presented in the reading. The above one substitutes in the first equation each time; the other one substitutes in the new equation each time.
    \end{itemize}
    \item We want to show that the integral converges to zero.
    \begin{itemize}
        \item The magnitude of the remainder is less than or equal to
        \begin{equation*}
            \norm{A}^{m+1}\left( \sup_{\tau\in[0,t]}|y(\tau)| \right)\frac{t^{m+1}}{(m+1)!}
        \end{equation*}
        \begin{itemize}
            \item Justification of this term: Look at the rightmost term in the last line of the Picard iteration above. Imagine taking the norm of it. Splitting the "scalar" integral from the matrix allows us to take a matrix norm, and the property $\norm{AB}\leq\norm{A}\norm{B}$ tells us that $\norm{A^{m+1}}\leq\norm{A}^{m+1}$. Then with respect to the integral, if we evaluate it, we will get the next polynomial term in the sequence --- $t^{m+1}/(m+1)!$ --- times at most the maximum value of $y$ at every infinitesimal.
        \end{itemize}
        \item We can visualize lower-dimensional integrals as the volume of the corresponding unit \textbf{simplex}.
        \begin{itemize}
            \item For example, in $\R^2$,
            \begin{equation*}
                \int_0^1\int_0^{\tau_1}1\dd\tau_2\dd{\tau_1}
            \end{equation*}
            can be visualized as the area of the unit triangle. This rationalizes why it evaluates to $1/2$, the area of said triangle.
            \item In $\R^3$,
            \begin{equation*}
                \int_0^1\int_0^{\tau_1}\int_0^{\tau_2}1\dd\tau_3\dd\tau_2\dd{\tau_1}
            \end{equation*}
            can be visualized as the area of the unit simplex. This rationalizes why it evaluates to $1/3!=1/6$, the volume of said simplex.
        \end{itemize}
        \item Since $(m+1)!\to\infty$ faster than any other term, the whole thing goes to zero.
        \item Thus, since the remainder goes to zero as we add more terms, we eventually reach the limit
        \begin{align*}
            y(t) &= \sum_{k=0}^\infty\frac{t^k}{k!}A^ky_0\\
            &= \e[tA]y_0
        \end{align*}
    \end{itemize}
    \item \textbf{Simplex}: A higher-dimensional generalization of a triangle.
    \item We now consider the following inhomogeneous equation. An appropriate integrating factor still helps.
    \begin{align*}
        y' &= Ay+f(t)\\
        y'-Ay &= f(t)\\
        \e[-tA]y'-A\e[-tA]y &= \e[-tA]f(t)\\
        \dv{t}(\e[-tA]y(t)) &= \e[-tA]f(t)\\
        \e[-tA]y(t)-y_0 &= \int_0^t\e[-\tau A]f(\tau)\dd\tau\\
        y(t) &= \e[tA]y_0+\int_0^t\e[(t-\tau)A]f(\tau)\dd\tau
    \end{align*}
    \begin{itemize}
        \item We also call this the Duhamel formula.
    \end{itemize}
    \item Note that if your time scale starts from $t_0$, then
    \begin{equation*}
        y(t) = \e[(t-t_0)A]y(t_0)+\int_{t_0}^t\e[(t-\tau)A]f(\tau)\dd\tau
    \end{equation*}
    \item The utility of JNF: If we want to understand $\e[tA]y_0$, we convert $A=QBQ^{-1}$, allowing us to evaluate $\e[tA]$.
    \begin{itemize}
        \item Shao reviews some facts of JNF from previous lectures.
    \end{itemize}
    \item From last lecture, we have that
    \begin{equation*}
        \e[tA]y_0 = Q\e[tB]Q^{-1}y_0
    \end{equation*}
    \item Example: Let
    \begin{equation*}
        A =
        \begin{pmatrix}
            -2 & 2 & 1\\
            -7 & 4 & 2\\
            5 & 0 & 0\\
        \end{pmatrix}
    \end{equation*}
    \begin{itemize}
        \item This is the same matrix from a previous lecture. As before, we have that
        \begin{align*}
            Q &=
            \begin{pmatrix}
                0 & 1 & 0\\
                -1 & -1 & 3\\
                2 & 5 & -5\\
            \end{pmatrix}&
            B &=
            \begin{pmatrix}
                1 & 0 & 0\\
                0 & 1 & 1\\
                0 & 0 & 1\\
            \end{pmatrix}
        \end{align*}
        \begin{itemize}
            \item Recall that the left two vectors are normal eigenvectors (the leftmost one corresponds to $\lambda_1=0$ and the middle one corresponds to $\lambda_2=1$) and the rightmost one is a generalized eigenvector.
        \end{itemize}
        \item We can compute that
        \begin{equation*}
            \e[tB] =
            \begin{pmatrix}
                \e[0t] & 0 & 0\\
                0 & \e[1t] & 1t\e[1t]\\
                0 & 0 & \e[1t]\\
            \end{pmatrix}
            =
            \begin{pmatrix}
                1 & 0 & 0\\
                0 & \e[t] & t\e[t]\\
                0 & 0 & \e[t]\\
            \end{pmatrix}
        \end{equation*}
        \item It follows that
        \begin{align*}
            \e[tA]y_0 &= Q
            \begin{pmatrix}
                1 & 0 & 0\\
                0 & \e[t] & t\e[t]\\
                0 & 0 & \e[t]\\
            \end{pmatrix}
            Q^{-1}y_0\\
            &=
            \begin{pmatrix}
                -3t\e[t]+\e[t] & 2t\e[t] & t\e[t]\\
                3t\e[t]-10\e[t]+10 & -2t\e[t]+6\e[t]-5 & -t\e[t]+3\e[t]-3\\
                -15t\e[t]+20\e[t]-20 & 10t\e[t]-10\e[t]+10 & 5t\e[t]-5\e[t]+6\\
            \end{pmatrix}
            \begin{pmatrix}
                y_0^1\\
                y_0^2\\
                y_0^3\\
            \end{pmatrix}
        \end{align*}
    \end{itemize}
    \item \textbf{Stable} (eigenvalue): An eigenvalue $\lambda_j=\sigma_j+i\beta_j$ for which $\sigma_j<0$.
    \item \textbf{Unstable} (eigenvalue): An eigenvalue $\lambda_j=\sigma_j+i\beta_j$ for which $\sigma_j>0$.
    \item \textbf{Stable} (subspace of the system): A generalized eigenspace corresponding a stable eigenvalue.
    \item \textbf{Unstable} (subspace of the system): A generalized eigenspace corresponding an unstable eigenvalue.
    \begin{itemize}
        \item If $\lambda_j$ is unstable, then the corresponding entries in $\e[tB_j]$ are exponentially growing functions.
        \item If $\lambda_j$ is stable, then the corresponding entries in $\e[tB_j]$ are exponentially decreasing functions.
        \item If $\sigma_j=0$, then the "stability" depends on the geometric multiplicity??
        \item Along the stable subspaces, your points will be attracted to zero.
        \item Along the unstable subspaces, your points will be repelled from zero.
        \item If $\sigma_h=0$, then we have rotation around a point, oscillation about zero, or oscillation whose magnitude grows to infinity. We do not talk about its stability.
        \begin{itemize}
            \item We do not include the eigenvector corresponding to $\lambda_1=0$ in the above basis of the stable subspace because the solution oscillates about $y_1$??
        \end{itemize}
        \item The stable subspace of our example is
        \begin{equation*}
            \spn\left\{
                \begin{pmatrix}
                    1\\
                    -1\\
                    5\\
                \end{pmatrix},
                \begin{pmatrix}
                    0\\
                    3\\
                    -5\\
                \end{pmatrix}
            \right\}
        \end{equation*}
    \end{itemize}
    \item Recall that $B_j$ acts on $K_j$.
    \begin{itemize}
        \item ... in picture??
        \item Recall that $\C^n=K_1\oplus\cdots\oplus K_m$.
        \item $P_j$ is not an \emph{orthogonal} projection, but it is a projection of $y_0$ onto $K_j$. It's also a polynomial??
    \end{itemize}
    \item Consider the order $n$ linear differential equation
    \begin{equation*}
        x^{(n)}+a_{n-1}x^{(n-1)}+\cdots+a_1x'+a_0x = 0
    \end{equation*}
    \begin{itemize}
        \item Then we can make a system out of it:
        \begin{equation*}
            \begin{pmatrix}
                y^1\\
                \vdots\\
                y^n\\
            \end{pmatrix}'
            = \underbrace{
                \begin{pmatrix}
                    0 & 1 &  & \\
                     & \ddots & \ddots & \\
                     &  & 0 & 1\\
                    -a_0 & -a_1 & \cdots & -a_{n-1}\\
                \end{pmatrix}
            }_{F[p]}
            \begin{pmatrix}
                y^1\\
                \vdots\\
                y^n\\
            \end{pmatrix}
        \end{equation*}
        \begin{itemize}
            \item Recall how to do the transformation from Lecture 1.
        \end{itemize}
        \item $F[p]$ is the \textbf{Frobenius matrix}.
        \item The transpose of this matrix is a very special matrix called the \textbf{companion matrix} $C[p]=F[p]^T$.
        \item Claim: Let $p(z)=z^n+a_{n-1}z^{n-1}+\cdots+a_1z+a_0$. Then $\chi_{C[p]}=\chi_{F[p]}=p(z)$.
        \begin{proof}
            % We have that
            % \begin{align*}
            %     \chi_{C[p]}(z) &= \det(zI-C[p])\\
            %     &= z(z^{n-1}+a_{n-1}(z^{n-2}+a_{n-2}(z^{n-3}+\cdots)))\\
            %     &= p(z)
            % \end{align*}
            % as desired.


            Do the Laplace expansion with respect to the last column of $A-zI$ (companion) or last row (Frobenius).
        \end{proof}
        \item Roots of $p(z)$ are the eigenvalues of $F[p]$ and $C[p]$.
        \item Claim: $C[p]$ has \textbf{minimal polynomial} $p(z)$.
        \begin{proof}
            We have that $C[p]e_i=e_{i+1}$ for $i=1,\dots,n-1$ and
            \begin{equation*}
                C[p]e_n = -a_0e_1-\cdots-a_{n-1}e_n
            \end{equation*}
            which implies that if $r(z)/\deg r<n$ nullifies $C[p]$, then necessarily $r(z)=p(z)$ since $(z-\lambda_j)^{<\alpha_j}$??
        \end{proof}
        \item Claim: $C[p],F[p]$ have the same Jordan normal form.
        \begin{itemize}
            \item More generally, transpose matrices are similar so they have the same JNF.
        \end{itemize}
    \end{itemize}
    \item \textbf{Monic polynomial}: A polynomial whose highest-degree coefficient equals 1.
    \item \textbf{Minimal polynomial} (of $A$): The unique monic polynomial $p$ of smallest degree such that $p(A)=0$.
    \item Theorem: In the Jordan normal form $F[p]$, each $\lambda_j$ corresponds to only one Jordan block.
    \begin{itemize}
        \item Thus,
        \begin{equation*}
            F[p] \sim
            \begin{pmatrix}
                J_{\alpha_1}(\lambda_1) &  & \\
                 & \ddots & \\
                 &  & J_{\alpha_m}(\lambda_m)\\
            \end{pmatrix}
        \end{equation*}
        The implication is that
        \begin{equation*}
            J_d(\lambda) \neq
            \begin{pmatrix}
                \lambda &  & \\
                 & \lambda & 1\\
                 &  & \lambda\\
            \end{pmatrix}
        \end{equation*}
        ever??
    \end{itemize}
    \item Corollary: The solution $y(t)$ is of the form
    \begin{equation*}
        (\cdots)+a_1\e[t\lambda_j]+\cdots+c_{\alpha_j-1}t^{\alpha_j-1}\e[t\lambda_j]+\cdots
    \end{equation*}
    \item Example: Solving a second-order ODE.
    \begin{equation*}
        x''+ax'+bx = 0
        \quad\Longleftrightarrow\quad
        \begin{pmatrix}
            y^1\\
            y^2\\
        \end{pmatrix}'
        =
        \begin{pmatrix}
            0 & 1\\
            -b & -a\\
        \end{pmatrix}
        \begin{pmatrix}
            y^1\\
            y^2\\
        \end{pmatrix}
    \end{equation*}
    \begin{itemize}
        \item The characteristic polynomial of the equation (and this matrix) is $z^2+az+b=0$.
        \item If $\lambda_1\neq\lambda_2$, then $x(t)=A\e[t\lambda_1]+B\e[t\lambda_2]$. If $\lambda_1=\lambda_2=\lambda$, then $x(t)=A\e[t\lambda]+Bt\e[t\lambda]$.
    \end{itemize}
\end{itemize}



\section{Midterm 1 Review}
\begin{itemize}
    \item \marginnote{10/19:}Notes on Friday's exam.
    \begin{itemize}
        \item Three problems. All will be calculations for specific equations. They will all be standard examples that appeared in the lectures or homeworks.
        \item The materials that you can bring to the exam are the notes on JNF (printed). You will be dealing with the JNF of $2\times 2$ or $3\times 3$ matrices.
    \end{itemize}
    \item Review session today, no new content.
    \item Remind Shao to post teaching notes from more recent weeks.
    \item \textbf{Ordinary differential equation}: An equation that involves an unknown function together with its derivatives. \emph{Given by}
    \begin{equation*}
        F(t,y,y',y'',\dots,y^{(n)}) = 0
    \end{equation*}
    \item \textbf{Order} (of an ODE): The highest order derivative present in the ODE.
    \item Two types of ODE problems: IVPs and BVPs.
    \begin{itemize}
        \item IVPs arise in dynamical systems.
        \item BVPs arise in variational problems in physics.
    \end{itemize}
    \item We are primarily interested in ODEs which can be explicitly solved for $y\in C^1(\R^n)$ (resp. $C^1(\C^n)$).
    \item Two types of equations:
    \begin{itemize}
        \item A higher-order scalar equation.
        \item The more general form of vector-valued systems of the form $y'=f(t,y)$.
    \end{itemize}
    \item In order to determine $y$, the initial value $y(t_0)=y_0$ is needed.
    \begin{itemize}
        \item If a vector-valued system, you need $y_0^1,\dots,y_0^n$ (all components).
        \item If a scalar system, you need $y(t_0),y'(t_0),\dots,y^{(n-1)}(t_0)$.
    \end{itemize}
    \item The idea of well-posedness is not yet well-defined in the course; we will cover it after the midterm.
    \item \textbf{Well-posed} (IVP): For every initial value, there is only one unique solution, and for a small change in the initial value, there is only a small change in the solution (continuous dependence on initial values).
    \item The theorem that we've been relying on but haven't proven yet: \textbf{Cauchy-Lipschitz} / \textbf{Picard-Lindel\"{o}f theorem}.
    \item \textbf{Cauchy-Lipschitz theorem}: If $f(t,y)$ is Lipschitz continuous with respect to $y$, then the IVP is locally well-posed. \emph{Also known as} \textbf{Picard-Lindel\"{o}f theorem}.
    \begin{itemize}
        \item The term \textbf{locally well-posed} has not been rigorously defined either.
    \end{itemize}
    \item Given any ODE, it is usually very easy to verify the Lipschitz condition for the RHS.
    \item Example of an IVP that is not locally well-posed.
    \begin{itemize}
        \item $y=\sqrt{y}$, $y(0)=0$.
        \item Note that if we start at any $t_0>0$, then this IVP \emph{is} locally well-posed.
    \end{itemize}
    \item No Cauchy-Lipschitz in the first midterm; just calculations. We will need the precise statement in the second midterm, though.
    \item We are not going to talk about solutions that require power series because that inevitably involves complex analysis.
    \item Explicitly solvable equations: Equations of separable form, i.e., the IVP $y'(t)=f(y)g(t)$, $y(t_0)=y_0$.
    \item From C-L theorem: If $f(y)$ is continuously differentiable in some neighborhood of $y_0$, then the solution is unique.
    \item If $f(y_0)=0$, then $y(t)=y_0$.
    \begin{itemize}
        \item Because then $y'(t)=f(y_0)g(t)=0$, so $y$ is a constant function.
    \end{itemize}
    \item If $f(y)\neq 0$ in some neighborhood of $y_0$, then the solution should satisfy the implicit equation
    \begin{equation*}
        \int_{y_0}^y\frac{\dd{w}}{f(w)} = \int_{t_0}^tg(\tau)\dd\tau
    \end{equation*}
    \begin{itemize}
        \item We use the chain rule to make separation of variables rigorous: We can differentiate the LHS above wrt. $t$ and get $y'(t)/f(y(t))$.
        \item Relating the $f(y_0)=0$ and $f(y)\neq 0$ cases and not making them overlap: We start integrating from the nonzero value.
    \end{itemize}
    \item Examples: $y'(t)=p(t)y(t)$ is homogeneous linear. It follows that
    \begin{equation*}
        y(t) = \exp[\int_{t_0}^tp(\tau)\dd\tau]y_0
    \end{equation*}
    \item If $p(t)=r\neq 0$, then the solution is exponential growth or decay:
    \begin{equation*}
        y(t) = y_0\e[r(t-t_0)]
    \end{equation*}
    \item Logistic growth:
    \begin{equation*}
        y'(t) = ry\left( 1-\frac{y}{M} \right)
        \quad\Longleftrightarrow\quad
        y(t) = \frac{My_0\e[rt]}{M+y_0(\e[rt]-1)}
    \end{equation*}
    \begin{itemize}
        \item Shao gives the related implicit integral equation and logarithmic equation as well.
    \end{itemize}
    \item There exist equations which cannot be solved by separation of variables. One case is equations of the form
    \begin{equation*}
        g(x,y)\dv{y}{x}+f(x,y) = 0
    \end{equation*}
    where $\partial_xg(x,y)=\partial_yf(x,y)$.
    \begin{itemize}
        \item In this case, there exists $F(x,y)$ such that $\partial_xF=f$, $\partial_yF=g$, and $F(x,y)=C$ is the relation satisfied by the solution.
        \item These are \textbf{exact form} equations.
    \end{itemize}
    \item Not all equations satisfy this relation. However, it is often possible (though potentially quite hard) to find an \textbf{integrating factor} by which you can multiply your equation to put it in exact form.
    \item Special case where it is easy to find the integrating factor: Consider the inhomogeneous linear equation $y'(t)=p(t)y(t)+f(t)$. Then the integrating factor is
    \begin{equation*}
        \mu = \exp[-\int_{t_0}^tp(\tau)\dd\tau]
    \end{equation*}
    \item Multiplying through, we get
    \begin{align*}
        \exp[-\int_{t_0}^tp(\tau)\dd\tau]f(t) &= \exp[-\int_{t_0}^tp(\tau)\dd\tau]y'(t)-\exp[-\int_{t_0}^tp(\tau)\dd\tau]p(t)y(t)\\
        &= \dv{t}\left\{ \exp[-\int_{t_0}^tp(\tau)\dd\tau]y(t) \right\}\\
        y(t) &= \exp[\int_{t_0}^tp(\tau)\dd\tau]y_0+\exp[\int_{t_0}^tp(\tau)\dd\tau]\cdot\int_{t_0}^t\exp[-\int_{t_0}^\tau p(\tau')\dd\tau']f(t)\dd\tau
    \end{align*}
    \item The above formula is complicated, though, so it is probably better to remember the method than to memorize the above.
    \item When $p(t)=a$ for all $t$, $y'(t)=ay+f(t)$. The solution is given by the \textbf{Duhamel formula}.
    \item \textbf{Duhamel formula}: The following equation, which solves ODEs of the form $y'(t)=ay+f(t)$. \emph{Given by}
    \begin{equation*}
        y(t) = \e[a(t-t_0)]y_0+\int_{t_0}^t\e[a(t-\tau)]f(\tau)\dd\tau
    \end{equation*}
    \begin{itemize}
        \item We should understand the derivation, but we can apply the Duhamel formula on PSets and exams without further justification.
    \end{itemize}
    \item Other things (??) are related to this form by some smart transformation.
    \item Final example of explicitly solvable ODEs: Linear autonomous systems.
    \item \textbf{Linear autonomous system}: A system of equations of the form $y'=Ay$ where $A$ is a constant $n\times n$ matrix and $y$ takes its value in $\R^n$ (resp. $\C^n$).
    \item The homogeneous solution is
    \begin{equation*}
        y(t) = \e[tA]y_0
    \end{equation*}
    where $\e[tA] = 1+\frac{tA}{1!}+\frac{t^2A^2}{2!}+\cdots$.
    \item In the inhomogeneous case $y'=Ay+f(t)$, our solution is
    \begin{equation*}
        y(t) = \e[tA]y_0+\int_0^t\e[(t-\tau)A]f(\tau)\dd\tau
    \end{equation*}
    \item We don't want to compute $\e[tA]$ using an infinite power series. Thus, we introduce similarity.
    \item Let $Q$ be the connecting matrix from the standard basis to the new basis. Then the matrix of $Q$ is the set of new basis vectors $q_1,q_2,q_3$, i.e., $
        Q =
        \begin{pmatrix}
            q_1 & q_2 & q_3\\
        \end{pmatrix}
    $. Then $B=Q^{-1}AQ$ or $A=QBQ^{-1}$.
    \item We want $B$ to be in the most convenient basis possible. Thus, we take the basis to be the Jordan basis.
    \item We fortunately have $\e[tA]=Q\e[tB]Q^{-1}$.
    \item Consider $\chi_A(z)=\det(zI_n-A)$ where $n=2,3$. If $\chi_A$ has distinct roots, then the eigenvalues of $A$ are distinct. At this point, we can find an eigenvector corresponding to each eigenvalue and diagonalize our matrix.
    \item Alternatively, if $\chi_A$ has multiple roots\dots
    \begin{itemize}
        \item $2\times 2$ case, $A$ is not diagonal. Then there is only one eigenvector $v_\lambda$. In this case, solve $(A-\lambda)u=v_\lambda$. Here, we say that the algebraic multiplicity is 2 and the geometric multiplicity is 1. Then
        \begin{align*}
            Q &=
            \begin{pmatrix}
                v_\lambda & u\\
            \end{pmatrix}&
            B &=
            \begin{pmatrix}
                \lambda & 1\\
                0 & \lambda\\
            \end{pmatrix}&
            \e[tA] &= Q
            \begin{pmatrix}
                \e[t\lambda] & t\e[t\lambda]\\
                0 & \e[t\lambda]\\
            \end{pmatrix}
            Q^{-1}
        \end{align*}
        \item $3\times 3$ case: If we have $\lambda$ of $\alpha_\lambda=2$ and $\mu$ of $\alpha_\mu=1$, or if we have $\lambda$ with $\alpha_\lambda=3$. First case: Check geometric multiplicity of $\lambda$, i.e., how many linearly independent $v$ give $(A-\lambda I)v=0$. If there is one, solve $(A-\lambda I)u=v_\lambda$. If there are more than one, $A$ is diagonalizable. Second case: Check geometric multiplicity of $\lambda$. Divide into two subcases. If $\gamma_\lambda=1$, then we need to solve $(A-\lambda I)u_1=v_\lambda$ and $(A-\lambda I)u_2=u_1$, and we get
        \begin{align*}
            Q &=
            \begin{pmatrix}
                v_\lambda & u_1 & u_2\\
            \end{pmatrix}&
            B &=
            \begin{pmatrix}
                \lambda & 1 & 0\\
                0 & \lambda & 1\\
                0 & 0 & \lambda\\
            \end{pmatrix}
        \end{align*}
        If $\gamma_\lambda=2$, then cleverly choose $v_1$ such that $v_1$ is in the column space of $A-\lambda I$. This will allow us to solve $(A-\lambda I)u=v_1$. Then
        \begin{align*}
            Q &=
            \begin{pmatrix}
                v_1 & u & v_2\\
            \end{pmatrix}&
            B &=
            \begin{pmatrix}
                \lambda & 1 & 0\\
                0 & \lambda & 0\\
                0 & 0 & \lambda\\
            \end{pmatrix}
        \end{align*}
    \end{itemize}
    \item For our linear autonomous system $y'=Ay$, $\lambda$ is an eigenvector of $A$. Write $\lambda=\sigma+i\beta$. If $\lambda>0$, then $\lambda$ is \textbf{unstable} and the corresponding generalized eigenspace is said to be an \textbf{unstable eigenspace}.
    \item For example, if the JNF is
    \begin{equation*}
        A =
        \begin{pNiceArray}{cc|c}
            1 & 1 & \\
             & 1 & \\
            \hline
             &  & -2\\
        \end{pNiceArray}
    \end{equation*}
    then the eigenspace corresponding to the upper block is said to be unstable, and the other one is said to be stable.
    \item Consider the vector $\e[tA]v$. The entries consist of linear combinations of functions of the form $t^k\e[t\lambda]$. If the real part is greater than zero, the solution grows exponentially fast in the $t$ direction (notice how $t\to\infty$ implies $t^k\e[t\lambda]\to\infty$). Otherwise, the solution decays exponentially fast (notice how $t\to\infty$ implies $t^k\e[t\lambda]\to 0$).
\end{itemize}



\section{Chapter 3: Linear Equations}
\emph{From \textcite{bib:Teschl}.}
\subsection*{Section 3.1: The Matrix Exponential}
\begin{itemize}
    \item \marginnote{12/6:}Note: This section (in the book's chronology) follows several others that we will only study later in the course. Thus, when terms are bolded but not defined here, they are likely callbacks to prior sections in the book. You can look up their definitions in the portion of these notes treating those sections.
    \item Herein, we will study the system
    \begin{equation*}
        \dot{x}(t) = Ax(t)
        ,\quad
        x(0) = x_0
    \end{equation*}
    where $A$ is an $n\times n$ matrix.
    \item \textcite{bib:Teschl} reviews how we take the matrix product, the definition of the scalar product, and the definition of the norm.
    \item \textcite{bib:Teschl} denotes the identity matrix by $\mathbb{I}$.
    \item Deriving the solution to the above ODE.
    \begin{itemize}
        \item We use the \textbf{Picard iteration} to derive a Taylor series that converges to the solution to the ODE by the \textbf{Picard-Lindel\"{o}f theorem}.
        \item Start with $x_0(t)=x_0$ as our first approximation. The first few terms are
        \begin{align*}
            x_0(t) &= x_0\\
            x_1(t) &= x_0+\int_0^tAx_0(s)\dd{s}
                = x_0+Ax_0\int_0^t\dd{s}
                = x_0+tAx_0\\
            x_2(t) &= x_0+\int_0^tAx_1(s)\dd{s}
                = x_0+Ax_0\int_0^t\dd{s}+A^2x_0\int_0^tx\dd{s}\\
                &= x_0+tAx_0+\frac{t^2}{2}A^2x_0
        \end{align*}
        \item This motivates an induction proof (omitted here and in \textcite{bib:Teschl}), resulting in the formula
        \begin{equation*}
            x_m(t) = \sum_{j=0}^m\frac{t^j}{j!}A^jx_0
        \end{equation*}
        \item As mentioned above, the Picard-Lindel\"{o}f theorem implies that the Taylor series converges and thus
        \begin{equation*}
            x(t) = \lim_{m\to\infty}x_m(t)
            = \sum_{j=0}^\infty\frac{t^j}{j!}A^jx_0
        \end{equation*}
        \item But this series converges to a variant of the \textbf{matrix exponential}, yielding
        \begin{equation*}
            x(t) = \exp(tA)x_0
        \end{equation*}
        \item Therefore, "to understand our original problem, we have to understand the matrix exponential!" \parencite[60]{bib:Teschl}.
    \end{itemize}
    \item \textbf{Matrix exponential} (of $A$): The following matrix. \emph{Denoted by} $\bm{\textbf{exp}(A)}$. \emph{Given by}
    \begin{equation*}
        \exp(A) = \sum_{j=0}^\infty\frac{1}{j!}A^j
    \end{equation*}
    \item Going forward, we will work in $\C^n$ since $\C$ is algebraically closed (which will be important later on for JCF).
    \item For use later, we introduce a suitable norm for matrices and give a direct proof for the convergence of the above series in this norm. In particular\dots
    \item \textbf{Matrix norm} (of $A$): The following function from the space of $n\times n$ matrices to the nonnegative real numbers, where $A$ is a complex matrix acting on $\C^n$. \emph{Denoted by} $\bm{\Vert A\Vert}$. \emph{Given by}
    \begin{equation*}
        \norm{A} = \sup_{x:|x|=1}|Ax|
    \end{equation*}
    \item $\bm{\pmb{\C}^{n\times n}}$: The vector space of $n\times n$ matrices over $\C$.
    \item Under the matrix norm, $\C^{n\times n}$ becomes a \textbf{Banach space}.
    \item An interesting formula.
    \begin{equation*}
        \max_{1\leq 1,j\leq n}|A_{i,j}| \leq \norm{A}
        \leq n\max_{1\leq i,j\leq n}|A_{i,j}|
    \end{equation*}
    \begin{itemize}
        \item Implication: A sequence of matrices converges in the matrix norm iff all matrix entries converge.
    \end{itemize}
    \item Since $\norm{A^j}\leq\norm{A}^j$, convergence of the series defining $\exp(A)$ follows from convergence of
    \begin{equation*}
        \sum_{j=0}^\infty\frac{\norm{A}^j}{j!} = \exp(\norm{A})
    \end{equation*}
    \item \textbf{Commutator} (of $A,B$): The following matrix, where $A,B$ are matrices. \emph{Denoted by} $\bm{[A,B]}$. \emph{Given by}
    \begin{equation*}
        [A,B] = AB-BA
    \end{equation*}
    \begin{itemize}
        \item The commutator vanishes iff $A,B$ commute.
    \end{itemize}
    \item Lemma 3.1: Suppose $A,B$ commute, i.e., $[A,B]=0$. Then
    \begin{equation*}
        \exp(A+B) = \exp(A)\exp(B)
    \end{equation*}
    \item Suppose we perform a linear change of coordinates $y=U^{-1}x$. Then the matrix exponential in the new coordinates is given by
    \begin{equation*}
        U^{-1}\exp(A)U = \exp(U^{-1}AU)
    \end{equation*}
    \begin{itemize}
        \item This follows from the definition of the matrix exponential, the fact that $U^{-1}A^jU=(U^{-1}AU)^j$ for arbitrary natural number powers $j$, and the fact that the matrix product is continuous with respect to the matrix norm (i.e., if $A_j\to A$ and $B_j\to B$, then $A_jB_j\to AB$).
        \item Thus, to compute $\exp(A)$, we'd prefer a coordinate transform which renders $A$ as simple as possible.
    \end{itemize}
    \item Theorem 3.2 (Jordan canonical form): Let $A$ be a complex $n\times n$ matrix. Then there exists a linear change of coordinates $U$ such that $A$ transforms into a block matrix
    \begin{equation*}
        U^{-1}AU =
        \begin{pmatrix}
            J_1 &  & \\
             & \ddots & \\
             &  & J_m\\
        \end{pmatrix}
    \end{equation*}
    with each block of the form
    \begin{equation*}
        J = \alpha\mathbb{I}+N
        =
        \begin{pmatrix}
            \alpha & 1 &  &  & \\
             & \alpha & 1 &  & \\
             &  & \alpha & \ddots & \\
             &  &  & \ddots & 1\\
             &  &  &  & \alpha\\
        \end{pmatrix}
    \end{equation*}
    Here, $N$ is a matrix with ones in the first diagonal above the main diagonal and zeroes elsewhere.
    \begin{itemize}
        \item The numbers $\alpha$ are the eigenvalues of $A$. The new basis vectors $u_j$ (the columns of $U$) consist of generalized eigenvectors of $A$.
        \item The general details of finding the JCF are quite cumbersome and deferred to Section 3.8. Typically, we use computers to do this nowadays.
    \end{itemize}
    \item We can now compute the matrix exponential.
    \begin{itemize}
        \item First, we break into Jordan blocks.
        \begin{equation*}
            \exp(U^{-1}AU) =
            \begin{pmatrix}
                \exp(J_1) &  & \\
                 & \ddots & \\
                 &  & \exp(J_m)\\
            \end{pmatrix}
        \end{equation*}
        \item Since $\alpha I$ commutes with $N$, we infer from Lemma 3.1 that we can compute the matrix exponential of a Jordan block as follows.
        \begin{equation*}
            \exp(J) = \exp(\alpha\mathbb{I})\exp(N)
            = \e[\alpha]\sum_{j=0}^{k-1}\frac{1}{j!}N^j
            = \e[\alpha]
            \begin{pmatrix}
                1 & 1 & \frac{1}{2!} & \cdots & \frac{1}{(k-1)!}\\
                 & 1 & 1 & \ddots & \vdots\\
                 &  & 1 & \ddots & \frac{1}{2!}\\
                 &  &  & \ddots & 1\\
                 &  &  &  & 1\\
            \end{pmatrix}
        \end{equation*}
        \begin{itemize}
            \item We assume that $J$ (and hence $\mathbb{I},N$) is/are $k\times k$.
            \item In the last step, we make use of the fact that $N^j$ is a matrix with ones in the $j^\text{th}$ diagonal above the main diagonal, and thus that $N^j$ vanishes from when $j$ reaches the size of $N$. Indeed, we could still sum all the way up to $\infty$; we'd just only be adding on zeroes matrices after the $k-1$ term.
        \end{itemize}
    \end{itemize}
    \item In two dimensions, the exponential of
    \begin{equation*}
        A =
        \begin{pmatrix}
            a & b\\
            c & d\\
        \end{pmatrix}
    \end{equation*}
    is given by
    \begin{equation*}
        \exp(A) = \e[\delta]\left(
            \cosh(\Delta)\mathbb{I}+\frac{\sinh(\Delta)}{\Delta}
            \begin{pmatrix}
                \gamma & b\\
                c & -\gamma\\
            \end{pmatrix}
        \right)
    \end{equation*}
    where
    \begin{align*}
        \delta &= \frac{a+d}{2}&
        \gamma &= \frac{a-d}{2}&
        \Delta &= \sqrt{\gamma^2+bc}
    \end{align*}
    \begin{itemize}
        \item Special cases:
        \begin{itemize}
            \item $\Delta=0$. In this case, define
            \begin{equation*}
                \frac{\sinh(\Delta)}{\Delta} = 1
            \end{equation*}
            \item $\Delta$ is purely imaginary. In this case, we have
            \begin{align*}
                \cosh(i\Delta) &= \cos\Delta&
                \frac{\sinh(i\Delta)}{i\Delta} &= \frac{\sin(\Delta)}{\Delta}
            \end{align*}
        \end{itemize}
        \item Derivation: Given as in HW3 Q3.
    \end{itemize}
    \item If $A$ is in JCF, we can easily see that
    \begin{equation*}
        \det(\exp(A)) = \exp(\trace(A))
    \end{equation*}
    \begin{itemize}
        \item Since the determinant and trace are invariant under coordinate change, this formula holds for arbitrary matrices, too.
    \end{itemize}
    \item Lemma 3.3: A vector $u$ is an eigenvector of $A$ corresponding to the eigenvalue $\alpha$ iff $u$ is an eigenvector of $\exp(A)$ corresponding to the eigenvalue $\e[\alpha]$.\par
    Moreover, the Jordan structure of $A$ and $\exp(A)$ are the same except for the fact that the eigenvalues of $A$ which differ by a multiple of $2\pi i$ (as well as the corresponding Jordan blocks) are mapped to the same eigenvalue of $\exp(A)$. In particular, the geometric and algebraic multiplicity of $\e[\alpha]$ is the sum of the geometric and algebraic multiplicities of the eigenvalues which differ from $\alpha$ by a multiple of $2\pi i$.
    \begin{proof}
        Given.
    \end{proof}
    \item \textcite{bib:Teschl} covers a method not described in class in which we can get an alternate \textbf{real Jordan canonical form}.
    \begin{itemize}
        \item This method may explain the complex eigenvectors final step for that subset of planar autonomous systems.
    \end{itemize}
\end{itemize}



\subsection*{Section 3.8: Appendix -- Jordan Canonical Form}
\begin{itemize}
    \item \textcite{bib:Teschl} has quite a bit to say on JCF!
\end{itemize}




\end{document}