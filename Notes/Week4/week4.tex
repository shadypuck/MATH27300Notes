\documentclass[../notes.tex]{subfiles}

\pagestyle{main}
\renewcommand{\chaptermark}[1]{\markboth{\chaptername\ \thechapter\ (#1)}{}}
\setcounter{chapter}{3}

\begin{document}




\chapter{Linear Systems}
\section{Autonomous Linear Systems}
\begin{itemize}
    \item \marginnote{10/17:}Today: General theory for autonomous linear systems.
    \item Review session Wednesday (no new material).
    \item First midterm Friday.
    \begin{itemize}
        \item Test problems will be slight variations of homework problems or examples given in class.
    \end{itemize}
    \item \textbf{Linear autonomous system}: A system of $n$ linear equations written in the following form. \emph{Denoted by} $\bm{y'=Ay}$. \emph{Given by}
    \begin{align*}
        \begin{pmatrix}
            y^1\\
            \vdots\\
            y^n\\
        \end{pmatrix}'
        &= 
        \begin{pmatrix}
            a_{11} & \cdots & a_{1n}\\
            \vdots & \ddots & \vdots\\
            a_{n1} & \cdots & a_{nn}\\
        \end{pmatrix}
        \begin{pmatrix}
            y^1\\
            \vdots\\
            y^n\\
        \end{pmatrix}&
        y(0) &= 0
    \end{align*}
    \begin{itemize}
        \item Note that the $a_{ij}$'s are complex or real.
    \end{itemize}
    \item The explicit solution is given by $y(t)=\e[tA]y_0$.
    \begin{itemize}
        \item Recall that $\dv*{t}(\e[tA])=A\e[tA]$, as we can show via the power series expansion.
    \end{itemize}
    \item \textbf{Picard iteration}: We take
    \begingroup
    \allowdisplaybreaks
    \begin{align*}
        y'(t) &= Ay(t)\\
        \int_0^ty'(\tau)\dd{\tau} &= \int_0^tAy(\tau)\dd\tau\\
        y(t) &= y_0+\int_0^tAy(\tau_1)\dd\tau_1\\
        &= y_0+\int_0^tA\left[ y_0+\int_0^{\tau_1}Ay(\tau_2)\dd\tau_2 \right]\dd\tau_1\\
        &= y_0+tAy_0+\int_0^t\int_0^{\tau_1}A^2y(\tau_2)\dd\tau_2\dd\tau_1\\
        &= y_0+tAy_0+\int_0^t\int_0^{\tau_1}A^2\left[ y_0+\int_0^{\tau_2}Ay(\tau_3)\dd\tau_3 \right]\dd\tau_2\dd\tau_1\\
        &= y_0+tAy_0+\frac{t^2A^2}{2}+\int_0^t\int_0^{\tau_1}\int_0^{\tau_2}A^3y(\tau_3)\dd\tau_3\dd\tau_2\dd\tau_1\\
        &= \sum_{k=0}^m\frac{t^kA^k}{k!}y_0+A^{m+1}\underbrace{\int_0^t\cdots\int_0^{\tau_m}}_{m+1}y(\tau_{m+1})\dd\tau_{m+1}\cdots\dd\tau_1
    \end{align*}
    \endgroup
    \begin{itemize}
        \item We get from the second to the third line by substituting $y(t)$, as defined into the second line, into where it appears in the integral.
    \end{itemize}
    \item We want to show that the integral converges to zero.
    \begin{itemize}
        \item The magnitude of the remainder is less than or equal to
        \begin{equation*}
            \norm{A}^{m+1}\left( \sup_{\tau\in[0,t]}|y(\tau)| \right)\frac{t^{m+1}}{(m+1)!}
        \end{equation*}
        \begin{itemize}
            \item Justification of this term: Look at the rightmost term in the last line of the Picard iteration above. Imagine taking the norm of it. Splitting the "scalar" integral from the matrix allows us to take a matrix norm, and the property $\norm{AB}\leq\norm{A}\norm{B}$ tells us that $\norm{A^{m+1}}\leq\norm{A}^{m+1}$. Then with respect to the integral, if we evaluate it, we will get the next polynomial term in the sequence --- $t^{m+1}/(m+1)!$ --- times at most the maximum value of $y$ at every infinitesimal.
        \end{itemize}
        \item We can visualize lower-dimensional integrals as the volume of the corresponding unit \textbf{simplex}.
        \begin{itemize}
            \item For example, in $\R^2$,
            \begin{equation*}
                \int_0^1\int_0^{\tau_1}1\dd\tau_2\dd{\tau_1}
            \end{equation*}
            can be visualized as the area of the unit triangle. This rationalizes why it evaluates to $1/2$, the area of said triangle.
            \item In $\R^3$,
            \begin{equation*}
                \int_0^1\int_0^{\tau_1}\int_0^{\tau_2}1\dd\tau_3\dd\tau_2\dd{\tau_1}
            \end{equation*}
            can be visualized as the area of the unit simplex. This rationalizes why it evaluates to $1/3!=1/6$, the volume of said simplex.
        \end{itemize}
        \item Since $(m+1)!\to\infty$ faster than any other term, the whole thing goes to zero.
        \item Thus, since the remainder goes to zero as we add more terms, we eventually reach the limit
        \begin{align*}
            y(t) &= \sum_{k=0}^\infty\frac{t^k}{k!}A^ky_0\\
            &= \e[tA]y_0
        \end{align*}
    \end{itemize}
    \item \textbf{Simplex}: A higher-dimensional generalization of a triangle.
    \item We now consider the following inhomogeneous equation. An appropriate integrating factor still helps.
    \begin{align*}
        y' &= Ay+f(t)\\
        y'-Ay &= f(t)\\
        \e[-tA]y'-A\e[-tA]y &= \e[-tA]f(t)\\
        \dv{t}(\e[-tA]y(t)) &= \e[-tA]f(t)\\
        \e[-tA]y(t)-y_0 &= \int_0^t\e[-\tau A]f(\tau)\dd\tau\\
        y(t) &= \e[tA]y_0+\int_0^t\e[(t-\tau)A]f(\tau)\dd\tau
    \end{align*}
    \begin{itemize}
        \item We also call this the Duhamel formula.
    \end{itemize}
    \item Note that if your time scale starts from $t_0$, then
    \begin{equation*}
        y(t) = \e[(t-t_0)A]y(t_0)+\int_{t_0}^t\e[(t-\tau)A]f(\tau)\dd\tau
    \end{equation*}
    \item The utility of JNF: If we want to understand $\e[tA]y_0$, we convert $A=QBQ^{-1}$, allowing us to evaluate $\e[tA]$.
    \begin{itemize}
        \item Shao reviews some facts of JNF from previous lectures.
    \end{itemize}
    \item From last lecture, we have that
    \begin{equation*}
        \e[tA]y_0 = Q\e[tB]Q^{-1}y_0
    \end{equation*}
    \item Example: Let
    \begin{equation*}
        A =
        \begin{pmatrix}
            -2 & 2 & 1\\
            -7 & 4 & 2\\
            5 & 0 & 0\\
        \end{pmatrix}
    \end{equation*}
    \begin{itemize}
        \item This is the same matrix from a previous lecture. As before, we have that
        \begin{align*}
            Q &=
            \begin{pmatrix}
                0 & 1 & 0\\
                -1 & -1 & 3\\
                2 & 5 & -5\\
            \end{pmatrix}&
            B &=
            \begin{pmatrix}
                1 & 0 & 0\\
                0 & 1 & 1\\
                0 & 0 & 1\\
            \end{pmatrix}
        \end{align*}
        \begin{itemize}
            \item Recall that the left two vectors are normal eigenvectors (the leftmost one corresponds to $\lambda_1=0$ and the middle one corresponds to $\lambda_2=1$) and the rightmost one is a generalized eigenvector.
        \end{itemize}
        \item We can compute that
        \begin{equation*}
            \e[tB] =
            \begin{pmatrix}
                \e[0t] & 0 & 0\\
                0 & \e[1t] & 1t\e[1t]\\
                0 & 0 & \e[1t]\\
            \end{pmatrix}
            =
            \begin{pmatrix}
                1 & 0 & 0\\
                0 & \e[t] & t\e[t]\\
                0 & 0 & \e[t]\\
            \end{pmatrix}
        \end{equation*}
        \item It follows that
        \begin{align*}
            \e[tA]y_0 &= Q
            \begin{pmatrix}
                1 & 0 & 0\\
                0 & \e[t] & t\e[t]\\
                0 & 0 & \e[t]\\
            \end{pmatrix}
            Q^{-1}y_0\\
            &=
            \begin{pmatrix}
                -3t\e[t]+\e[t] & 2t\e[t] & t\e[t]\\
                3t\e[t]-10\e[t]+10 & -2t\e[t]+6\e[t]-5 & -t\e[t]+3\e[t]-3\\
                -15t\e[t]+20\e[t]-20 & 10t\e[t]-10\e[t]+10 & 5t\e[t]-5\e[t]+6\\
            \end{pmatrix}
            \begin{pmatrix}
                y_0^1\\
                y_0^2\\
                y_0^3\\
            \end{pmatrix}
        \end{align*}
    \end{itemize}
    \item \textbf{Stable} (eigenvalue): An eigenvalue $\lambda_j=\sigma_j+i\beta_j$ for which $\sigma_j<0$.
    \item \textbf{Unstable} (eigenvalue): An eigenvalue $\lambda_j=\sigma_j+i\beta_j$ for which $\sigma_j>0$.
    \item \textbf{Stable} (subspace of the system): A generalized eigenspace corresponding a stable eigenvalue.
    \item \textbf{Unstable} (subspace of the system): A generalized eigenspace corresponding an unstable eigenvalue.
    \begin{itemize}
        \item If $\lambda_j$ is unstable, then the corresponding entries in $\e[tB_j]$ are exponentially growing functions.
        \item If $\lambda_j$ is stable, then the corresponding entries in $\e[tB_j]$ are exponentially decreasing functions.
        \item If $\sigma_j=0$, then the "stability" depends on the geometric multiplicity??
        \item Along the stable subspaces, your points will be attracted to zero.
        \item Along the unstable subspaces, your points will be repelled from zero.
        \item If $\sigma_h=0$, then we have rotation around a point, oscillation about zero, or oscillation whose magnitude grows to infinity. We do not talk about its stability.
        \begin{itemize}
            \item We do not include the eigenvector corresponding to $\lambda_1=0$ in the above basis of the stable subspace because the solution oscillates about $y_1$??
        \end{itemize}
        \item The stable subspace of our example is
        \begin{equation*}
            \spn\left\{
                \begin{pmatrix}
                    1\\
                    -1\\
                    5\\
                \end{pmatrix},
                \begin{pmatrix}
                    0\\
                    3\\
                    -5\\
                \end{pmatrix}
            \right\}
        \end{equation*}
    \end{itemize}
    \item Recall that $B_j$ acts on $K_j$.
    \begin{itemize}
        \item ... in picture??
        \item Recall that $\C^n=K_1\oplus\cdots\oplus K_m$.
        \item $P_j$ is not an \emph{orthogonal} projection, but it is a projection of $y_0$ onto $K_j$. It's also a polynomial??
    \end{itemize}
    \item Consider the order $n$ linear differential equation
    \begin{equation*}
        x^{(n)}+a_{n-1}x^{(n-1)}+\cdots+a_1x'+a_0x = 0
    \end{equation*}
    \begin{itemize}
        \item Then we can make a system out of it:
        \begin{equation*}
            \begin{pmatrix}
                y^1\\
                \vdots\\
                y^n\\
            \end{pmatrix}'
            = \underbrace{
                \begin{pmatrix}
                    0 & 1 &  & \\
                     & \ddots & \ddots & \\
                     &  & 0 & 1\\
                    -a_0 & -a_1 & \cdots & -a_{n-1}\\
                \end{pmatrix}
            }_{F[p]}
            \begin{pmatrix}
                y^1\\
                \vdots\\
                y^n\\
            \end{pmatrix}
        \end{equation*}
        \begin{itemize}
            \item Recall how to do the transformation from Lecture 1.
        \end{itemize}
        \item $F[p]$ is the \textbf{Frobenius matrix}.
        \item The transpose of this matrix is a very special matrix called the \textbf{companion matrix} $C[p]=F[p]^T$.
        \item Claim: Let $p(z)=z^n+a_{n-1}z^{n-1}+\cdots+a_1z+a_0$. Then $\chi_{C[p]}=\chi_{F[p]}=p(z)$.
        \begin{proof}
            % We have that
            % \begin{align*}
            %     \chi_{C[p]}(z) &= \det(zI-C[p])\\
            %     &= z(z^{n-1}+a_{n-1}(z^{n-2}+a_{n-2}(z^{n-3}+\cdots)))\\
            %     &= p(z)
            % \end{align*}
            % as desired.


            Do the Laplace expansion with respect to the last column of $A-zI$ (companion) or last row (Frobenius).
        \end{proof}
        \item Roots of $p(z)$ are the eigenvalues of $F[p]$ and $C[p]$.
        \item Claim: $C[p]$ has \textbf{minimal polynomial} $p(z)$.
        \begin{proof}
            We have that $C[p]e_i=e_{i+1}$ for $i=1,\dots,n-1$ and
            \begin{equation*}
                C[p]e_n = -a_0e_1-\cdots-a_{n-1}e_n
            \end{equation*}
            which implies that if $r(z)/\deg r<n$ nullifies $C[p]$, then necessarily $r(z)=p(z)$ since $(z-\lambda_j)^{<\alpha_j}$??
        \end{proof}
        \item Claim: $C[p],F[p]$ have the same Jordan normal form.
        \begin{itemize}
            \item More generally, transpose matrices are similar so they have the same JNF.
        \end{itemize}
    \end{itemize}
    \item \textbf{Monic polynomial}: A polynomial whose highest-degree coefficient equals 1.
    \item \textbf{Minimal polynomial} (of $A$): The unique monic polynomial $p$ of smallest degree such that $p(A)=0$.
    \item Theorem: In the Jordan normal form $F[p]$, each $\lambda_j$ corresponds to only one Jordan block.
    \begin{itemize}
        \item Thus,
        \begin{equation*}
            F[p] \sim
            \begin{pmatrix}
                J_{\alpha_1}(\lambda_1) &  & \\
                 & \ddots & \\
                 &  & J_{\alpha_m}(\lambda_m)\\
            \end{pmatrix}
        \end{equation*}
        The implication is that
        \begin{equation*}
            J_d(\lambda) \neq
            \begin{pmatrix}
                \lambda &  & \\
                 & \lambda & 1\\
                 &  & \lambda\\
            \end{pmatrix}
        \end{equation*}
        ever??
    \end{itemize}
    \item Corollary: The solution $y(t)$ is of the form
    \begin{equation*}
        (\cdots)+a_1\e[t\lambda_j]+\cdots+c_{\alpha_j-1}t^{\alpha_j-1}\e[t\lambda_j]+\cdots
    \end{equation*}
    \item Example: Solving a second-order ODE.
    \begin{equation*}
        x''+ax'+bx = 0
        \quad\Longleftrightarrow\quad
        \begin{pmatrix}
            y^1\\
            y^2\\
        \end{pmatrix}'
        =
        \begin{pmatrix}
            0 & 1\\
            -b & -a\\
        \end{pmatrix}
        \begin{pmatrix}
            y^1\\
            y^2\\
        \end{pmatrix}
    \end{equation*}
    \begin{itemize}
        \item The characteristic polynomial of the equation (and this matrix) is $z^2+az+b=0$.
        \item If $\lambda_1\neq\lambda_2$, then $x(t)=A\e[t\lambda_1]+B\e[t\lambda_2]$. If $\lambda_1=\lambda_2=\lambda$, then $x(t)=A\e[t\lambda]+Bt\e[t\lambda]$.
    \end{itemize}
\end{itemize}



\section{Midterm 1 Review}
\begin{itemize}
    \item \marginnote{10/19:}Notes on Friday's exam.
    \begin{itemize}
        \item Three problems. All will be calculations for specific equations. They will all be standard examples that appeared in the lectures or homeworks.
        \item The materials that you can bring to the exam are the notes on JNF (printed). You will be dealing with the JNF of $2\times 2$ or $3\times 3$ matrices.
    \end{itemize}
    \item Review session today, no new content.
    \item Remind Shao to post teaching notes from more recent weeks.
    \item \textbf{Ordinary differential equation}: An equation that involves an unknown function together with its derivatives. \emph{Given by}
    \begin{equation*}
        F(t,y,y',y'',\dots,y^{(n)}) = 0
    \end{equation*}
    \item \textbf{Order} (of an ODE): The highest order derivative present in the ODE.
    \item Two types of ODE problems: IVPs and BVPs.
    \begin{itemize}
        \item IVPs arise in dynamical systems.
        \item BVPs arise in variational problems in physics.
    \end{itemize}
    \item We are primarily interested in ODEs which can be explicitly solved for $y\in C^1(\R^n)$ (resp. $C^1(\C^n)$).
    \item Two types of equations:
    \begin{itemize}
        \item A higher-order scalar equation.
        \item The more general form of vector-valued systems of the form $y'=f(t,y)$.
    \end{itemize}
    \item In order to determine $y$, the initial value $y(t_0)=y_0$ is needed.
    \begin{itemize}
        \item If a vector-valued system, you need $y_0^1,\dots,y_0^n$ (all components).
        \item If a scalar system, you need $y(t_0),y'(t_0),\dots,y^{(n-1)}(t_0)$.
    \end{itemize}
    \item The idea of well-posedness is not yet well-defined in the course; we will cover it after the midterm.
    \item \textbf{Well-posed} (IVP): For every initial value, there is only one unique solution, and for a small change in the initial value, there is only a small change in the solution (continuous dependence on initial values).
    \item The theorem that we've been relying on but haven't proven yet: \textbf{Cauchy-Lipschitz} / \textbf{Picard-Lindel\"{o}f theorem}.
    \item \textbf{Cauchy-Lipschitz theorem}: If $f(t,y)$ is Lipschitz continuous with respect to $y$, then the IVP is locally well-posed. \emph{Also known as} \textbf{Picard-Lindel\"{o}f theorem}.
    \begin{itemize}
        \item The term \textbf{locally well-posed} has not been rigorously defined either.
    \end{itemize}
    \item Given any ODE, it is usually very easy to verify the Lipschitz condition for the RHS.
    \item Example of an IVP that is not locally well-posed.
    \begin{itemize}
        \item $y=\sqrt{y}$, $y(0)=0$.
        \item Note that if we start at any $t_0>0$, then this IVP \emph{is} locally well-posed.
    \end{itemize}
    \item No Cauchy-Lipschitz in the first midterm; just calculations. We will need the precise statement in the second midterm, though.
    \item We are not going to talk about solutions that require power series because that inevitably involves complex analysis.
    \item Explicitly solvable equations: Equations of separable form, i.e., the IVP $y'(t)=f(y)g(t)$, $y(t_0)=y_0$.
    \item From C-L theorem: If $f(y)$ is continuously differentiable in some neighborhood of $y_0$, then the solution is unique.
    \item If $f(y_0)=0$, then $y(t)=y_0$.
    \begin{itemize}
        \item Because then $y'(t)=f(y_0)g(t)=0$, so $y$ is a constant function.
    \end{itemize}
    \item If $f(y)\neq 0$ in some neighborhood of $y_0$, then the solution should satisfy the implicit equation
    \begin{equation*}
        \int_{y_0}^y\frac{\dd{w}}{f(w)} = \int_{t_0}^tg(\tau)\dd\tau
    \end{equation*}
    \begin{itemize}
        \item We use the chain rule to make separation of variables rigorous: We can differentiate the LHS above wrt. $t$ and get $y'(t)/f(y(t))$.
        \item Relating the $f(y_0)=0$ and $f(y)\neq 0$ cases and not making them overlap: We start integrating from the nonzero value.
    \end{itemize}
    \item Examples: $y'(t)=p(t)y(t)$ is homogeneous linear. It follows that
    \begin{equation*}
        y(t) = \exp[\int_{t_0}^tp(\tau)\dd\tau]y_0
    \end{equation*}
    \item If $p(t)=r\neq 0$, then the solution is exponential growth or decay:
    \begin{equation*}
        y(t) = y_0\e[r(t-t_0)]
    \end{equation*}
    \item Logistic growth:
    \begin{equation*}
        y'(t) = ry\left( 1-\frac{y}{M} \right)
        \quad\Longleftrightarrow\quad
        y(t) = \frac{My_0\e[rt]}{M+y_0(\e[rt]-1)}
    \end{equation*}
    \begin{itemize}
        \item Shao gives the related implicit integral equation and logarithmic equation as well.
    \end{itemize}
    \item There exist equations which cannot be solved by separation of variables. One case is equations of the form
    \begin{equation*}
        g(x,y)\dv{y}{x}+f(x,y) = 0
    \end{equation*}
    where $\partial_xg(x,y)=\partial_yf(x,y)$.
    \begin{itemize}
        \item In this case, there exists $F(x,y)$ such that $\partial_xF=f$, $\partial_yF=g$, and $F(x,y)=C$ is the relation satisfied by the solution.
        \item These are \textbf{exact form} equations.
    \end{itemize}
    \item Not all equations satisfy this relation. However, it is often possible (though potentially quite hard) to find an \textbf{integrating factor} by which you can multiply your equation to put it in exact form.
    \item Special case where it is easy to find the integrating factor: Consider the inhomogeneous linear equation $y'(t)=p(t)y(t)+f(t)$. Then the integrating factor is
    \begin{equation*}
        \mu = \exp[-\int_{t_0}^tp(\tau)\dd\tau]
    \end{equation*}
    \item Multiplying through, we get
    \begin{align*}
        \exp[-\int_{t_0}^tp(\tau)\dd\tau]f(t) &= \exp[-\int_{t_0}^tp(\tau)\dd\tau]y'(t)-\exp[-\int_{t_0}^tp(\tau)\dd\tau]p(t)y(t)\\
        &= \dv{t}\left\{ \exp[-\int_{t_0}^tp(\tau)\dd\tau]y(t) \right\}\\
        y(t) &= \exp[\int_{t_0}^tp(\tau)\dd\tau]y_0+\exp[\int_{t_0}^tp(\tau)\dd\tau]\cdot\int_{t_0}^t\exp[-\int_{t_0}^\tau p(\tau')\dd\tau']f(t)\dd\tau
    \end{align*}
    \item The above formula is complicated, though, so it is probably better to remember the method than to memorize the above.
    \item When $p(t)=a$ for all $t$, $y'(t)=ay+f(t)$. The solution is given by the \textbf{Duhamel formula}.
    \item \textbf{Duhamel formula}: The following equation, which solves ODEs of the form $y'(t)=ay+f(t)$. \emph{Given by}
    \begin{equation*}
        y(t) = \e[a(t-t_0)]y_0+\int_{t_0}^t\e[a(t-\tau)]f(\tau)\dd\tau
    \end{equation*}
    \begin{itemize}
        \item We should understand the derivation, but we can apply the Duhamel formula on PSets and exams without further justification.
    \end{itemize}
    \item Other things (??) are related to this form by some smart transformation.
    \item Final example of explicitly solvable ODEs: Linear autonomous systems.
    \item \textbf{Linear autonomous system}: A system of equations of the form $y'=Ay$ where $A$ is a constant $n\times n$ matrix and $y$ takes its value in $\R^n$ (resp. $\C^n$).
    \item The homogeneous solution is
    \begin{equation*}
        y(t) = \e[tA]y_0
    \end{equation*}
    where $\e[tA] = 1+\frac{tA}{1!}+\frac{t^2A^2}{2!}+\cdots$.
    \item In the inhomogeneous case $y'=Ay+f(t)$, our solution is
    \begin{equation*}
        y(t) = \e[tA]y_0+\int_0^t\e[(t-\tau)A]f(\tau)\dd\tau
    \end{equation*}
    \item We don't want to compute $\e[tA]$ using an infinite power series. Thus, we introduce similarity.
    \item Let $Q$ be the connecting matrix from the standard basis to the new basis. Then the matrix of $Q$ is the set of new basis vectors $q_1,q_2,q_3$, i.e., $
        Q =
        \begin{pmatrix}
            q_1 & q_2 & q_3\\
        \end{pmatrix}
    $. Then $B=Q^{-1}AQ$ or $A=QBQ^{-1}$.
    \item We want $B$ to be in the most convenient basis possible. Thus, we take the basis to be the Jordan basis.
    \item We fortunately have $\e[tA]=Q\e[tB]Q^{-1}$.
    \item Consider $\chi_A(z)=\det(zI_n-A)$ where $n=2,3$. If $\chi_A$ has distinct roots, then the eigenvalues of $A$ are distinct. At this point, we can find an eigenvector corresponding to each eigenvalue and diagonalize our matrix.
    \item Alternatively, if $\chi_A$ has multiple roots\dots
    \begin{itemize}
        \item $2\times 2$ case, $A$ is not diagonal. Then there is only one eigenvector $v_\lambda$. In this case, solve $(A-\lambda)u=v_\lambda$. Here, we say that the algebraic multiplicity is 2 and the geometric multiplicity is 1. Then
        \begin{align*}
            Q &=
            \begin{pmatrix}
                v_\lambda & u\\
            \end{pmatrix}&
            B &=
            \begin{pmatrix}
                \lambda & 1\\
                0 & \lambda\\
            \end{pmatrix}&
            \e[tA] &= Q
            \begin{pmatrix}
                \e[t\lambda] & t\e[t\lambda]\\
                0 & \e[t\lambda]\\
            \end{pmatrix}
            Q^{-1}
        \end{align*}
        \item $3\times 3$ case: If we have $\lambda$ of $\alpha_\lambda=2$ and $\mu$ of $\alpha_\mu=1$, or if we have $\lambda$ with $\alpha_\lambda=3$. First case: Check geometric multiplicity of $\lambda$, i.e., how many linearly independent $v$ give $(A-\lambda I)v=0$. If there is one, solve $(A-\lambda I)u=v_\lambda$. If there are more than one, $A$ is diagonalizable. Second case: Check geometric multiplicity of $\lambda$. Divide into two subcases. If $\gamma_\lambda=1$, then we need to solve $(A-\lambda I)u_1=v_\lambda$ and $(A-\lambda I)u_2=u_1$, and we get
        \begin{align*}
            Q &=
            \begin{pmatrix}
                v_\lambda & u_1 & u_2\\
            \end{pmatrix}&
            B &=
            \begin{pmatrix}
                \lambda & 1 & 0\\
                0 & \lambda & 1\\
                0 & 0 & \lambda\\
            \end{pmatrix}
        \end{align*}
        If $\gamma_\lambda=2$, then cleverly choose $v_1$ such that $v_1$ is in the column space of $A-\lambda I$. This will allow us to solve $(A-\lambda I)u=v_1$. Then
        \begin{align*}
            Q &=
            \begin{pmatrix}
                v_1 & u & v_2\\
            \end{pmatrix}&
            B &=
            \begin{pmatrix}
                \lambda & 1 & 0\\
                0 & \lambda & 0\\
                0 & 0 & \lambda\\
            \end{pmatrix}
        \end{align*}
    \end{itemize}
    \item For our linear autonomous system $y'=Ay$, $\lambda$ is an eigenvector of $A$. Write $\lambda=\sigma+i\beta$. If $\lambda>0$, then $\lambda$ is \textbf{unstable} and the corresponding generalized eigenspace is said to be an \textbf{unstable eigenspace}.
    \item For example, if the JNF is
    \begin{equation*}
        A =
        \begin{pNiceArray}{cc|c}
            1 & 1 & \\
             & 1 & \\
            \hline
             &  & -2\\
        \end{pNiceArray}
    \end{equation*}
    then the eigenspace corresponding to the upper block is said to be unstable, and the other one is said to be stable.
    \item Consider the vector $\e[tA]v$. The entries consist of linear combinations of functions of the form $t^k\e[t\lambda]$. If the real part is greater than zero, the solution grows exponentially fast in the $t$ direction (notice how $t\to\infty$ implies $t^k\e[t\lambda]\to\infty$). Otherwise, the solution decays exponentially fast (notice how $t\to\infty$ implies $t^k\e[t\lambda]\to 0$).
\end{itemize}




\end{document}