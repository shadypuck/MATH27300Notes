\documentclass[../notes.tex]{subfiles}

\pagestyle{main}
\renewcommand{\chaptermark}[1]{\markboth{\chaptername\ \thechapter\ (#1)}{}}
\setcounter{chapter}{2}

\begin{document}




\chapter{Linear Algebra Review}
\section{Elements of Linear Algebra}
\begin{itemize}
    \item \marginnote{10/10:}Today: Review of linear algebra.
    \item Start with a \textbf{vector space} over $\R$ or $\C$ or, more generally, any field $K$.
    \item \textbf{Vector space} (over $K$): A set equipped with addition and scalar multiplication such that the following axioms are satisfied.
    \begin{enumerate}
        \item Commutativity and associativity of addition.
        \item Additive identity and inverse.
        \item Compatibility of scalar multiplication and addition (distributive laws).
        \item The additive identity times any vector is zero.
    \end{enumerate}
    \item In $\R^n,\C^n$, addition is component-wise and scalar multiplication is scaling of the element.
    \item For a homogeneous equation
    \begin{equation*}
        y' = A(t)y
        =
        \begin{pmatrix}
            a_{11}(t)y^1+a_{12}(t)y^2+\cdots\\
            \vdots\\
        \end{pmatrix}
        \begin{pmatrix}
            y^1\\
            y^2\\
            \vdots\\
            y^n
        \end{pmatrix}
    \end{equation*}
    \begin{itemize}
        \item If $y_1,y_2$ are solutions, any linear combination of them is a solution. This is called the \textbf{solution space} of the equation.
    \end{itemize}
    \item \textbf{Linearly independent} (set of vectors): A set of vectors $x_1,\dots,x_m\in V$ such that the only coefficients $\lambda_1,\dots,\lambda_m$ such that
    \begin{equation*}
        \lambda_1x_1+\cdots+\lambda_mx_m = 0
    \end{equation*}
    is $\lambda_1=\cdots=\lambda_m=0$.
    \begin{itemize}
        \item $\lambda_m\neq 0$ implies
        \begin{equation*}
            x_m = -\frac{1}{\lambda_m}(\lambda_1x_1+\cdots+\lambda_{m-1}x_{m-1})
        \end{equation*}
    \end{itemize}
    \item \textbf{Maximal linear independence group}: A subset $X\subset V$ such that for any $y\in V$, $\{y\}\cup X$ is not linearly independent. \emph{Also known as} \textbf{basis}.
    \item Theorem: Any basis in $V$ has the same cardinality.
    \item \textbf{Dimension} (of $V$): The cardinality given by the above theorem. \emph{Denoted by} $\bm{\dim V}$.
    \item We usually denoted a basis as an ordered $n$-tuple since the order often matters (for orientation?).
    \item Notational onventions.
    \begin{itemize}
        \item For $\R^n,\C^n$, we will always use column vectors.
        \item $x_1,x_2,\dots$ denotes vectors.
        \item $x^1,x^2,\dots$ denotes the components of a column vector.
        \item A vector component squared may be denoted $(x^1)^2$.
    \end{itemize}
    \item \textbf{Standard basis} (for $\R^n$): The set of $n$ vectors of length $n$ which have a 1 as one entry and a zero in all the others and are all distinct.
    \item \textbf{Linear transformation} (of $V$ to $V$): A mapping $\phi:V\to V$ satisfying
    \begin{equation*}
        \phi(\lambda x+\mu y) = \lambda\phi(x)+\mu\phi(y)
    \end{equation*}
    \item A mapping is completely determined by its action on the basis vectors:
    \begin{equation*}
        \phi\left( \sum_{k=1}^nx^ke_k \right) = \sum_{k=1}^nx^k\phi(e_k)
    \end{equation*}
    \item \textbf{Matrix} (of a linear transformation wrt. the standard basis): The $n\times n$ array
    \begin{equation*}
        \begin{pmatrix}
            \phi(e_1) & \cdots & \phi(e_n)\\
        \end{pmatrix}
    \end{equation*}
    \item If $\phi,\psi:V\to V$ are linear, $\phi\circ\psi$ is also linear.
    \begin{itemize}
        \item Composition of linear transformations corresponds to matrix multiplication.
    \end{itemize}
    \item Matrix multiplication: If
    \begin{equation*}
        B =
        \begin{pmatrix}
            b_1 & \cdots & b_n\\
        \end{pmatrix}
    \end{equation*}
    then
    \begin{equation*}
        AB =
        \begin{pmatrix}
            Ab_1 & \cdots & Ab_n\\
        \end{pmatrix}
    \end{equation*}
    where
    \begin{equation*}
        Ax =
        \begin{pmatrix}
            a_{11}x^1+\cdots+a_{1n}x^n\\
            \vdots\\
            a_{n1}x^1+\cdots+a_{nn}x^n\\
        \end{pmatrix}
    \end{equation*}
    \item We can talk about matrix inverses: If it exists, it is unique, and
    \begin{equation*}
        AA^{-1} = A^{-1}A = I_n
    \end{equation*}
    \item Matrix multiplication is not commutative in general. Shao gives a counterexample.
    \item $A$ is invertible iff the columns of $A$ are a basis for $\R^n$ (resp. $\C^n$).
    \item \textbf{Determinant} (of $A$): Not defined.
    \item Properties of the determinant.
    \begin{itemize}
        \item Multilinear.
        \begin{equation*}
            \det
            \begin{pmatrix}
                a_1 & \cdots & \lambda a_k+\mu\tilde{a}_k & \cdots & a_n\\
            \end{pmatrix}
            = \lambda\det
            \begin{pmatrix}
                a_1 & \cdots & a_k & \cdots & a_n\\
            \end{pmatrix}
            +\mu\det
            \begin{pmatrix}
                a_1 & \cdots & \tilde{a}_k & \cdots & a_n\\
            \end{pmatrix}
        \end{equation*}
        \item Skew-symmetric.
        \begin{equation*}
            \det
            \begin{pmatrix}
                a_1 & \cdots & a_i & \cdots & a_j & \cdots & a_n\\
            \end{pmatrix}
            = -\det
            \begin{pmatrix}
                a_1 & \cdots & a_j & \cdots & a_i & \cdots & a_n\\
            \end{pmatrix}
        \end{equation*}
    \end{itemize}
    \item Theorem: The determinant is uniquely characterized by these three (??) axioms.
    \item $\det I_n=1$.
    \item Shao goes over computing the determinant via minors.
    \item Special cases:
    \begin{itemize}
        \item If the matrix is upper- or lower-triangular, the determinant is equal to the product of the diagonal entries.
        \item If the matrix is blocked upper- or lower-triangular, e.g.,
        \begin{equation*}
            A =
            \begin{pmatrix}
                A_1 & *\\
                0 & A_2\\
            \end{pmatrix}
        \end{equation*}
        then $\det A=\det A_1\cdot\det A_2$.
    \end{itemize}
    \item $\det(AB)=\det(A)\det(B)$.
    \item $\det A\neq 0$ iff $A$ is invertible.
    \item Direct formula to compute the inverse.
    \begin{equation*}
        A^{-1} = \frac{1}{\det A}
        \begin{pmatrix}
            a_{\ell k}(-1)^{k+\ell}\det A_{k\ell}
        \end{pmatrix}
    \end{equation*}
    \begin{itemize}
        \item Tedious for higher-dimensional cases, but quite sufficient for $n=2,3$.
    \end{itemize}
    \item Let $A$ be $n\times n$, and let $Ax=b$.
    \begin{itemize}
        \item If $A$ is invertible, then $x=A^{-1}b$.
        \item If $A$ is not invertible and $b\in\spn(a_1,\dots,a_n)$, then $x=x_h+x_p$ where $Ax_h=0$ and $Ax_p=b$.
    \end{itemize}
    \item \textbf{Kernel} (of $A$): The set of all vectors $y\in\R^n$ (resp. $\C^n$) such that $Ay=0$.
    \item \textbf{Range} (of $A$): The set of all linear combinations of $a_1,\dots,a_n$.
    \item Suppose $\phi:\R^n\to\R^n$ has matrix $A$ under $(e_1,\dots,e_n)$. Let $(q_1,\dots,q_n)$ be another basis.
    \begin{itemize}
        \item There exists a matrix $Q$ such that $q_k=Qe_k$. $Q$ is called the \textbf{connecting matrix} between $(e_1,\dots,e_n)$ and $(q_1,\dots,q_n)$.
        \item Claim: Let $x\in\R^n$ have representation $x=(x^1,\dots,x^n)$ under the standard basis. Then under the $Q$ basis, $x$ has representation $x'=Q^{-1}(x^1,\dots,x^n)$. Similarly, $x=Qx'$.
        \item Claim: $\phi$ has matrix $B=Q^{-1}AQ$ with respect to the $Q$ basis.
    \end{itemize}
    \item Matrix similarity: $A\sim B$ iff there exists $Q$ invertible such that $B=Q^{-1}AQ$.
    \begin{itemize}
        \item Implies that $A$ and $B$ describe the same matrix under different bases.
        \item Matrix product under the old and new bases are related.
        \begin{equation*}
            Q^{-1}ABQ = (Q^{-1}AQ)(Q^{-1}BQ)
        \end{equation*}
        \item Similarity preserves the determinant:
        \begin{equation*}
            \det(Q^{-1}AQ) = \det(Q^{-1})\det(A)\det(Q)
            = \det(A)\det(Q^{-1})\det(Q)
            = \det(A)
        \end{equation*}
    \end{itemize}
\end{itemize}



\section{Diagonalization and Jordan Normal Form}
\begin{itemize}
    \item \marginnote{10/12:}Similar matrices and Jordan Normal Form (JNF).
    \item Suppose $A:\C^n\to\C^n$ is linear. We can express $A$ in a different basis with the help of the connecting matrix $Q$.
    \item In this lecture, we seek to find the most convenient basis in which to discuss our linear transformation.
    \item Today we will work in $\C^n$ (but all results hold for $\R^n$, too).
    \item \textbf{Invariant subspace} (of $A$): A subspace $K\subset\C^n$ such that $A(K)=K$.
    \item Suppose you have $m$ invariant subspaces $K_1,\dots,K_m\subset\C^n$ whose pairwise intersection is $\{0\}$.
    \item \textbf{Direct sum} (of $K_1,\dots,K_m$): The collection of all vectors which can be represented as sums from each of the subspaces. \emph{Denoted by} $\bm{K_1\oplus\cdots\oplus K_m}$. \emph{Given by}
    \begin{equation*}
        K_1\oplus\cdots\oplus K_m = \left\{ x\in\C^n\mid x=\sum_{j=1}^mx_j,\ x_j\in K_j \right\}
    \end{equation*}
    \item Suppose $K_1,K_2\in\C^n$ are invariant subspaces of $A$ of dimension $n_1,n_2$, respectively, such that $K_1\oplus K_2=\C^n$. Then choosing a basis for $K_1$ and $K_2$, the matrix $A$ takes the form
    \begin{equation*}
        \begin{pmatrix}
            B_1 & 0\\
            0 & B_2\\
        \end{pmatrix}
    \end{equation*}
    where $B_1$ is an $n_1\times n_1$ block and $B_2$ is an $n_2\times n_2$ block.
    \item \textbf{Eigenvalue} (of $A$): A complex number $\lambda\in\C$ such that $A-\lambda I$ is not invertible. \emph{Denoted by} $\bm{\lambda}$.
    \begin{itemize}
        \item Equivalently, $\det(A-\lambda I)=0$.
    \end{itemize}
    \item \textbf{Characteristic polynomial}: The polynomial in $z$ defined as follows. \emph{Denoted by} $\bm{\chi_A(z)}$. \emph{Given by}
    \begin{equation*}
        \chi_A(z) = \det(A-zI)
    \end{equation*}
    \begin{itemize}
        \item Similar matrices have the same characteristic polynomials.
    \end{itemize}
    \item \textbf{Spectrum} (of $A$): The set of all eigenvalues of $A$.
    \item \textbf{Eigenvector} (of $A$): A vector $v\in\C^n$ corresponding to an eigenvalue $\lambda$ via
    \begin{equation*}
        Av = \lambda v
    \end{equation*}
    \item Claim: The set of all eigenvectors corresponding to $\lambda$ form an invariant subspace.
    \begin{proof}
        \begin{equation*}
            A(v_1+v_2) = \lambda v_1+\lambda v_2
            = \lambda(v_1+v_2)
        \end{equation*}
    \end{proof}
    \item \textbf{Eigenspace} (of $\lambda$): The vector subspace of $\C^n$ equal to the span of the eigenvectors of $\lambda$. \emph{Denoted by} $\bm{V_\lambda}$.
    \item \textbf{Algebraic multiplicity} (of $\lambda$): The degree of the $(z-\lambda)$ term in the factorization of the characteristic polynomial. \emph{Denoted by} $\bm{\alpha_\lambda}$.
    \item \textbf{Geometric multiplicity} (of $\lambda$): The dimension of the eigenspace of $\lambda$. \emph{Denoted by} $\bm{\gamma_\lambda}$.
    \item $\gamma_\lambda\leq\alpha_\lambda$.
    \item If $\alpha_\lambda=\gamma_\lambda$ for each $\lambda$, then each eigenspace $V_\lambda$ has a basis such that $\oplus_\lambda V_\lambda=\C^n$.
    \begin{itemize}
        \item Under this basis, the matrix of $A$ is diagonal with all $\lambda$'s (along the diagonal) repeated according to their algebraic multiplicity.
    \end{itemize}
    \item \textbf{Superdiagonal}: The set of entries in a matrix which are directly above a diagonal entry.
    \item \textbf{Jordan block}: A $d\times d$ matrix corresponding to an eigenvalue $\lambda$ that has $\lambda$ as every diagonal entry, 1 as every superdiagonal entry, and zeroes everywhere else. \emph{Denoted by} $\bm{J_d(\lambda)}$.
    \begin{itemize}
        \item The geometric multiplicity $\gamma_j$ is the number of Jordan blocks with eigenvalue $\lambda_j$. Of course, when $\gamma_j=\alpha_j$ (in particular, if $\alpha_j=1$), there is no Jordan block corresponding to $\lambda_j$ at all.
    \end{itemize}
    \item We have that
    \begin{align*}
        J_d(\lambda)
        \begin{pmatrix}
            1\\
            0\\
            \vdots\\
            0\\
        \end{pmatrix}
        &=
        \begin{pmatrix}
            \lambda\\
            0\\
            \vdots\\
            0\\
        \end{pmatrix}
        = \lambda
        \begin{pmatrix}
            1\\
            0\\
            \vdots\\
            0\\
        \end{pmatrix}
        = \lambda e_1\\
        J_d(\lambda)
        \begin{pmatrix}
            0\\
            1\\
            0\\
            \vdots\\
            0\\
        \end{pmatrix}
        &=
        \begin{pmatrix}
            1\\
            \lambda\\
            0\\
            \vdots\\
            0\\
        \end{pmatrix}
        = e_1+\lambda e_2\\
        &\vdots\\
        J_d(\lambda)e_{d-1} &= e_{d-2}+\lambda e_{d-1}
    \end{align*}
    \item For any linear transformation, we can find a basis such that the matrix is the diagonalized Jordon blocks.
    \item Theorem: Let $A$ be an $n\times n$ complex matrix. Then there is a \textbf{Jordan basis} $Q$ under which
    \begin{equation*}
        Q^{-1}AQ =
        \begin{pmatrix}
            J_{h_1}(\lambda_1) &  & \\
             & J_{h_{d_1}}(\lambda_1) & \\
             &  & \ddots\\
        \end{pmatrix}
    \end{equation*}
    \begin{itemize}
        \item We have that $h_1+\cdots+h_{d_1}=\alpha_1$??
        \item The proof will not be tested --- it is very hard. Shao will sketch it, though.
        \item The proof is constructive: It will tell you how to convert a matrix into the Jordan normal form.
    \end{itemize}
    \item Proof procedure:
    \begin{enumerate}
        \item Determine the eigenvalues as well as their algebraic and geometric multiplicities.
        \begin{enumerate}
            \item Compute $\chi_A(z)$.
            \item Find $\lambda_1,\dots,\lambda_m$ (factor $\chi_A(z)$).
            \item Find $\alpha_1,\dots,\alpha_m$ (combine like terms in the factorization of $\chi_A(z)$).
            \item Find $\gamma_1,\dots,\gamma_m$ ($\gamma_i=n-\rank(A-\lambda_iI)$).
        \end{enumerate}
        \item Find the \textbf{generalized eigenspaces} of each $\lambda_i$. This will allow us to block-diagonalize $A$.
        \begin{enumerate}
            \item For each $\lambda_i$, compute the $\ker(A-\lambda_iI)\subset\ker(A-\lambda_iI)^2\subset\ker(A-\lambda_iI)^3\subset\cdots$.
            \item The sequence will stop at some $d_i\in\N$. In particular, it will stop when $\dim\ker(A-\lambda I)^{d_j}=\alpha_i$.
            \begin{itemize}
                \item Claim: $K_i\cap K_j=\{0\}$. Let $j_i=\dim K_i$. Take the direct sum of all $K_i$. Then $j_1+\cdots+j_m=n$.
            \end{itemize}
            \item Since each $K_i$ is an invariant subspace of $A$, we know that there is a matrix of the linear transformation corresponding to $A$ of the form
            \begin{equation*}
                \begin{pmatrix}
                    B_1 &  & 0\\
                     & \ddots & \\
                    0 &  & B_m\\
                \end{pmatrix}
            \end{equation*}
            We now just need to choose the \emph{best} basis of each $K_i$, i.e., the one that makes each $B_i$ into a (direct sum of) Jordan block(s).
        \end{enumerate}
        \item Find the best basis for each $K_i$.
        \begin{enumerate}
            \item Recall that each $\lambda_i$ corresponds to $\gamma=\gamma_i$ linearly independent eigenvectors, which we will denote $v_{i,1},\dots,v_{i,\gamma}$. We will block-diagonalize $B_i$ into $\gamma$ Jordan blocks, each of which corresponds to a $v_{i,j}$ as follows.\par
            Every Jordan block is of the form
            \begin{equation*}
                \begin{pmatrix}
                    \lambda_i & 1 &  & 0\\
                     & \lambda_i & \ddots & \\
                     &  & \ddots & 1\\
                    0 &  &  & \lambda_i\\
                \end{pmatrix}
            \end{equation*}
            Let the above block be of dimension $k_{i,j}=d$. It follows that this block will be responsible for linearly transforming $d$ vectors in the Jordan basis. Let $v_{i,j,1}=v_{i,j}$ be the first of these $d$ vectors. Then the submatrix of $v_{i,j,1}$ in the Jordan basis corresponding to this Jordan block is
            \begin{equation*}
                \begin{pmatrix}
                    1\\
                    0\\
                    \vdots\\
                    0\\
                \end{pmatrix}
            \end{equation*}
            which should make sense since we want $Av_{i,j}=\lambda_iv_{i,j}$ and under this definition,
            \begin{equation*}
                \begin{pmatrix}
                    \lambda_i & 1 &  & 0\\
                     & \lambda_i & \ddots & \\
                     &  & \ddots & 1\\
                    0 &  &  & \lambda_i\\
                \end{pmatrix}
                \begin{pmatrix}
                    1\\
                    0\\
                    \vdots\\
                    0\\
                \end{pmatrix}
                = \lambda_i
                \begin{pmatrix}
                    1\\
                    0\\
                    \vdots\\
                    0\\
                \end{pmatrix}
            \end{equation*}
            Now let $v_{i,j,2}$ be the second of the $d$ vectors. Naturally, its submatrix in the Jordan basis should be
            \begin{equation*}
                \begin{pmatrix}
                    0\\
                    1\\
                    \vdots\\
                    0\\
                \end{pmatrix}
            \end{equation*}
            But this implies that
            \begin{align*}
                \begin{pmatrix}
                    \lambda_i & 1 &  & 0\\
                     & \lambda_i & \ddots & \\
                     &  & \ddots & 1\\
                    0 &  &  & \lambda_i\\
                \end{pmatrix}
                \begin{pmatrix}
                    0\\
                    1\\
                    \vdots\\
                    0\\
                \end{pmatrix}
                &=
                \begin{pmatrix}
                    1\\
                    \lambda_i\\
                    \vdots\\
                    0\\
                \end{pmatrix}\\
                &=
                \begin{pmatrix}
                    1\\
                    0\\
                    \vdots\\
                    0\\
                \end{pmatrix}
                +\lambda_i
                \begin{pmatrix}
                    0\\
                    1\\
                    \vdots\\
                    0\\
                \end{pmatrix}\\
                Av_{i,j,2} &= v_{i,j,1}+\lambda_iv_{i,j,2}\\
                (A-\lambda I)v_{i,j,2} &= v_{i,j,1}
            \end{align*}
            Naturally, this process will generalize to show that $(A-\lambda_iI)v_{i,j,k}=v_{i,j,k-1}$, i.e., we can recursively determine the $v_{i,j,1},\dots,v_{i,j,k_{i,j}}$.
            \item Thus, using the above process, we will find $k_{i,j}$ elements of the Jordan basis for each $v_{i,j}$. The full, ordered set of these vectors constitutes the Jordan basis.
            \item Note that each of these vectors is naturally an element of the generalized eigenspace $K_i$ since for each $k=1,\dots,k_{i,j}$, the formula $(A-\lambda_iI)v_{i,j,k}=v_{i,j,k-1}$ implies that
            \begin{equation*}
                (A-\lambda_iI)^kv_{i,j,k} = 0
            \end{equation*}
            Also note that each $k_{i,j}\leq d_i$ and $k_{i,1}+\cdots+k_{i,\gamma}=d_i$.
        \end{enumerate}
    \end{enumerate}
    \item \textbf{Generalized eigenspace} (of $\lambda$): The kernel of $(A-\lambda I)^{d_\lambda}$. \emph{Denoted by} $\bm{K_\lambda}$. \emph{Given by}
    \begin{equation*}
        K_\lambda = \ker(A-\lambda I)^{d_\lambda}
    \end{equation*}
    \item $\bm{d_\lambda}$: The power of $A-\lambda I$ for which the kernel stabilizes.
    \item $\bm{j_\lambda}$: The dimension of the generalized eigenspace of $\lambda$. \emph{Given by}
    \begin{equation*}
        j_\lambda = \dim K_\lambda
    \end{equation*}
    \item The JNF computation can be really heavy; we'll only ever compute $2\times 2$ or $3\times 3$ versions.
    \item Example:
    \begin{itemize}
        \item Consider
        \begin{equation*}
            A =
            \begin{pmatrix}
                -2 & 2 & 1\\
                -7 & 4 & 2\\
                5 & 0 & 0\\
            \end{pmatrix}
        \end{equation*}
        \item Then
        \begin{equation*}
            \chi_A(z) = z(z-1)^2
        \end{equation*}
        \item (1) It follows that
        \begin{align*}
            \lambda_1 &= 0&
            \lambda_2 &= 1
        \end{align*}
        \item (2) We have that
        \begin{align*}
            \ker(A-0I) &= \spn\left\{
                \begin{pmatrix}
                    0\\
                    -1\\
                    2\\
                \end{pmatrix}
            \right\}&
            \ker(A-1I) &= \spn\left\{
                \begin{pmatrix}
                    1\\
                    -1\\
                    5\\
                \end{pmatrix}
            \right\}
        \end{align*}
        \begin{itemize}
            \item We call the left vector above $q_1$ and the right vector above $q_2$.
        \end{itemize}
        \item Thus,
        \begin{equation*}
            A \sim
            \begin{pNiceArray}{c|cc}
                0 &  & \\
                \hline
                 & 1 & x\\
                 &  & 1\\
            \end{pNiceArray}
        \end{equation*}
        \item We find that
        \begin{equation*}
            (A-1I)^2 =
            \begin{pmatrix}
                0 & 0 & 0\\
                10 & -5 & -3\\
                -20 & 10 & 6\\
            \end{pmatrix}
        \end{equation*}
        so
        \begin{equation*}
            \ker(A-I)^2 = \spn\left\{
                \begin{pmatrix}
                    1\\
                    2\\
                    0\\
                \end{pmatrix},
                \begin{pmatrix}
                    3\\
                    0\\
                    10\\
                \end{pmatrix}
            \right\}
        \end{equation*}
        \item Clearly,
        \begin{equation*}
            \ker(A-I) \subsetneq \ker(A-I)^2
        \end{equation*}
        so we can stop here because the dimension of the kernel has reached the algebraic multiplicity.
        \item Since $q_2\in K_1$, $q_3$ solves the equation $(A-I)q_3=q_2$.
        \item We know that
        \begin{align*}
            \begin{pmatrix}
                \lambda & 1\\
                0 & \lambda\\
            \end{pmatrix}
            e_1 &= \lambda e_1&
            \begin{pmatrix}
                \lambda & 1\\
                0 & \lambda\\
            \end{pmatrix}
            e_2 &= e_1+\lambda e_2
        \end{align*}
        \item It follows that
        \begin{equation*}
            q_3 =
            \begin{pmatrix}
                0\\
                3\\
                -5\\
            \end{pmatrix}
        \end{equation*}
        and hence
        \begin{equation*}
            Q =
            \begin{pmatrix}
                q_1 & q_2 & q_3\\
            \end{pmatrix}
            =
            \begin{pmatrix}
                0 & 1 & 0\\
                -1 & -1 & 3\\
                2 & 5 & -5\\
            \end{pmatrix}
        \end{equation*}
        and
        \begin{equation*}
            Q^{-1}AQ =
            \begin{pmatrix}
                0 & 0 & 0\\
                0 & 1 & 1\\
                0 & 0 & 1\\
            \end{pmatrix}
        \end{equation*}
    \end{itemize}
    \item Simple cases.
    \item The $2\times 2$ case.
    \begin{itemize}
        \item $A\in\mathcal{M}^2(\C)$ can only have nontrivial Jordan form if it has a single eigenvalue $\lambda$ with $\alpha_\lambda=2$ and $\gamma_\lambda=1$. If both equal 2, then $A=\lambda I_2$. If it has two eigenvalues, then it is regularly diagonalizable.
        \item In this particular case, calculate $\lambda$ from $\chi_Z(z)=(z-\lambda)^2$, find one eigenvector $v$, and find the other generalized eigenvector $u$; $u$ will satisfy $(A-\lambda I)u=v$. The connecting matrix will be $Q=(v|u)$\footnote{Order matters! We need the eigenvector, specifically, to get scaled by $\lambda$ only.} and the JNF is
        \begin{equation*}
            Q^{-1}AQ =
            \begin{pmatrix}
                \lambda & 1\\
                0 & \lambda\\
            \end{pmatrix}
        \end{equation*}
    \end{itemize}
    \item The $3\times 3$ case.
    \begin{itemize}
        \item We divide into three nontrivial cases: $\chi_A(z)=(z-\lambda)^3$ with $\gamma_\lambda=2$, $\chi_A(z)=(z-\lambda)^3$ with $\gamma_\lambda=1$, and $\chi_A(z)=(z-\lambda)^2(z-\mu)$ with $\gamma_\lambda=1$.
        \item In the first case, we have two eigenvectors $v_1,v_2$. We can find the third (generalized) eigenvector by solving $(A-\lambda I)u=v_1$ and $(A-\lambda I)u=v_2$ (only one of these will have a solution). WLOG let the first equation have a solution. Then $Q=(v_1|u|v_2)$ and the JNF is
        \begin{equation*}
            Q^{-1}AQ =
            \begin{pmatrix}
                \lambda & 1 & 0\\
                0 & \lambda & 0\\
                0 & 0 & \lambda\\
            \end{pmatrix}
        \end{equation*}
        \item In the second case, we have one eigenvector $v$. We can find the second and third generalized eigenvectors by solving $(A-\lambda I)u_1=v$ and $(A-\lambda I)u_2=u_1$. Then $Q=(v|u_1|u_2)$ and
        \begin{equation*}
            Q^{-1}AQ =
            \begin{pmatrix}
                \lambda & 1 & 0\\
                0 & \lambda & 1\\
                0 & 0 & \lambda\\
            \end{pmatrix}
        \end{equation*}
        \item In the third case, we have two eigenvectors $v_\lambda,v_\mu$. We can find the third (generalized) eigenvector by solving $(A-\lambda I)u=v_\lambda$. Then $Q=(v_\lambda|u|v_\mu)$ and
        \begin{equation*}
            Q^{-1}AQ =
            \begin{pmatrix}
                \lambda & 1 & 0\\
                0 & \lambda & 0\\
                0 & 0 & \mu\\
            \end{pmatrix}
        \end{equation*}
    \end{itemize}
\end{itemize}




\section{Matrix Calculus}
\begin{itemize}
    \item \marginnote{10/14:}Today: Matrix calculus.
    \item We introduced the Jordan normal form because it is an easy form on which to do matrix calculus.
    \item \textbf{Matrix norm}: A function for $n\times n$ complex matrices such that
    \begin{enumerate}
        \item $\norm{A}\geq 0$, $\norm{A}=0$ iff $A=0$.
        \item $\norm{A+B}\leq\norm{A}+\norm{B}$.
        \item $\norm{\lambda A}=|\lambda|\norm{A}$.
        \item $\norm{AB}\leq\norm{A}\norm{B}$.
    \end{enumerate}
    \emph{Denoted by} $\bm{||\cdot||}$.
    \begin{itemize}
        \item The first three axioms above are the normal norm axioms; the last one is unique to matrix norms.
    \end{itemize}
    \item \textbf{Operator norm}: The norm defined by
    \begin{equation*}
        \norm{Ax} = \sup_{|x|=1}|Ax|
    \end{equation*}
    \item \textbf{??}: The norm defined by
    \begin{equation*}
        \norm{A} = \sum_{i,j=1}^n|a_{i,j}|
    \end{equation*}
    \item Theorem: Any two matrix norms are equivalent.
    \item \textbf{Convergent} (sequence of matrices): A sequence of matrices $A_n$ for which there exists $A$ such that $\norm{A_n-A}\to 0$ as $n\to\infty$. \emph{Denoted by} $\bm{A_n\to A}$.
    \begin{itemize}
        \item Note that $\norm{A_n-A}\to 0$ iff the entries of $A_n$ converge to the entries of $A$.
    \end{itemize}
    \item Suppose $A(t)=(a_{ij}(t))_{i,j=1}^n$ is a matrix function. Then
    \begin{align*}
        A'(t) &= (a_{ij}'(t))_{i,j=1}^n&
        \int_{t_0}^tA(t)\dd{t} &= \left( \int_{t_0}^ta_{ij}(\tau)\dd\tau \right)_{i,j=1}^n
    \end{align*}
    \item The product rule holds:
    \begin{equation*}
        \dv{t}[A(t)B(t)] = A'(t)B(t)+A(t)B'(t)
    \end{equation*}
    \item However, matrix multiplication is not commutative. This can get us into trouble in the following situation: We might think that
    \begin{equation*}
        \dv{t}[A(t)^2] = 2A'(t)A(t)
    \end{equation*}
    but, in fact,
    \begin{equation*}
        \dv{t}[A(t)^2] = A'(t)A(t)+A(t)A'(t)
    \end{equation*}
    \begin{itemize}
        \item For example, let
        \begin{equation*}
            A(t) =
            \begin{pmatrix}
                0 & 1\\
                t & 0\\
            \end{pmatrix}
        \end{equation*}
        \item Then
        \begin{equation*}
            A'(t) =
            \begin{pmatrix}
                0 & 0\\
                1 & 0\\
            \end{pmatrix}
        \end{equation*}
        \item It follows that
        \begin{equation*}
            \dv{t}[A'(t)^2] = \underbrace{
                \begin{pmatrix}
                    0 & 0\\
                    0 & 1\\
                \end{pmatrix}
            }_{A'(t)A(t)}+\underbrace{
                \begin{pmatrix}
                    1 & 0\\
                    0 & 0\\
                \end{pmatrix}
            }_{A(t)A'(t)}
        \end{equation*}
        \item Notice that $A'(t)A(t)\neq A(t)A'(t)$.
    \end{itemize}
    \item Suppose we have a matrix $A$ and we want to compute $A^{100}$.
    \item If $A$ is diagonalizable, then $A^n=Q^{-1}\Lambda^nQ$.
    \item What if $A$ is not diagonalizable?
    \begin{itemize}
        \item Then we convert to $A$ to Jordan normal form $A=Q^{-1}BQ$. Thus, we just need to compute the powers of the Jordan blocks.
        \item Suppose
        \begin{equation*}
            J_d(\lambda) =
            \begin{pmatrix}
                \lambda & 1 &  & 0\\
                 & \lambda & \ddots & \\
                 &  & \ddots & 1\\
                0 &  &  & \lambda\\
            \end{pmatrix}
        \end{equation*}
        \begin{itemize}
            \item In a given Jordan block, all entries above the diagonal are 1.
        \end{itemize}
        \item Decompose
        \begin{equation*}
            J_d(\lambda) = \lambda I_d+N_d
        \end{equation*}
        \item Note that $N_d$ is nilpotent --- every successive power to which you raise it shifts the 1s up one row until it becomes the zero matrix.
        \item In computing $[J_d(\lambda)]^m$, invoke the binomial expansion. When $m<d$ invoke the full expansion. When $m\geq d$, neglect all zero terms (terms with $N_d^i$ for $i\geq m$).
        \begin{equation*}
            [J_d(\lambda)]^m = \binom{m}{0}\lambda^mI_d+\binom{m}{1}\lambda^{m-1}N_d+\cdots+\binom{m}{m}N_d^m
        \end{equation*}
        \item Example: When $d=3$, then
        \begin{equation*}
            \begin{pmatrix}
                \lambda & 1 & 0\\
                 & \lambda & 1\\
                 &  & \lambda\\
            \end{pmatrix}^3
            =
            \begin{pmatrix}
                \lambda^m & m\lambda^{m-1} & m(m-1)\lambda^{m-2}\\
                 & \lambda^m & m\lambda^{m-1}\\
                 &  & \lambda^m\\
            \end{pmatrix}
        \end{equation*}
    \end{itemize}
    \item We will only compute JNF for $2\times 2$ and $3\times 3$; Shao reviews these cases from last class.
    \item We now have a formula to compute the powers of matrices with ease, so we can move onto more complicated functions of matrices now.
    \item Consider the power series
    \begin{equation*}
        f(z) = c_0+c_1z+c_2z^2+\cdots
    \end{equation*}
    \begin{itemize}
        \item The $c_i$ are complex coefficients.
    \end{itemize}
    \item \textbf{Analytic} (function): A function whose Taylor series (locally) converges and converges to the function in question.
    \item We can consider an analytic function of matrices:
    \begin{equation*}
        f(z) = c_0I+c_1A+c_2A^2+\cdots
    \end{equation*}
    \item \textbf{Radius of convergence}: The number $R$ such that the series converges for $\norm{A}<R$.
    \item \textbf{von Neumann series}: The series $I+A+A^2+\cdots$ converging to $(I_n-A)^{-1}$ for any $\norm{A}<1$.
    \begin{itemize}
        \item Example: We can check that the von Neumann series for $N_d$ converges.
    \end{itemize}
    \item Suppose $A=Q^{-1}BQ$. Then
    \begin{align*}
        f(A) &= f(Q^{-1}BQ)\\
        &= c_0I+c_1(Q^{-1}BQ)+c_2(Q^{-1}BQ)^2+\cdots\\
        &= Q^{-1}(c_0I+c_1B+c_2B^2+\cdots)Q\\
        &= Q^{-1}f(B)Q
    \end{align*}
    \begin{itemize}
        \item Going even further,
        \begin{equation*}
            B =
            \begin{pmatrix}
                B_1 & 0\\
                0 & B_2\\
            \end{pmatrix}
            \quad\Longrightarrow\quad
            f(B) =
            \begin{pmatrix}
                f(B_1) & 0\\
                0 & f(B_2)\\
            \end{pmatrix}
        \end{equation*}
        \item In particular, if $A$ is diagonalizable, then
        \begin{equation*}
            f(A) = Q^{-1}
            \begin{pmatrix}
                f(\lambda_1) &  & \\
                 & \ddots & \\
                 &  & f(\lambda_n)\\
            \end{pmatrix}
            Q
        \end{equation*}
    \end{itemize}
    \item Suppose $A$ is not diagonalizable, and $f$ is some analytic function.
    \begin{itemize}
        \item Then in the viscinity of $a$, $f$ can be approximated by the Taylor series
        \begin{equation*}
            f(z) = f(a)+f'(a)(z-a)+\frac{1}{2!}f^{(2)}(a)(z-a)^2+\cdots
        \end{equation*}
        \item Similarly, we can approximate $f[J_d(\lambda)]$ in the viscinity of $\lambda I_d$ with the Taylor series
        \begin{align*}
            f[J_d(\lambda)] &= f(\lambda I_d+N_d)\\
            &= f(\lambda I_d)+f'(\lambda I_d)[(\lambda I_d+N_d)-\lambda I_d]+\frac{1}{2!}f^{(2)}(\lambda I_d)[(\lambda I_d+N_d)-\lambda I_d]^2+\cdots\\
            &= f(\lambda)I_d+f'(\lambda)N_d+\frac{1}{2!}f^{(2)}(\lambda)N_d^2+\cdots\\
            &=
            \begin{pNiceMatrix}
                f(\lambda) & f'(\lambda) & \cdots & \frac{f^{(d-1)}(\lambda)}{(d-1)!}\\
                 & f(\lambda) & \ddots & \vdots\\
                 &  & \ddots & f'(\lambda)\\
                 &  &  & f(\lambda)\\
            \end{pNiceMatrix}
        \end{align*}
    \end{itemize}
    \item \textbf{Matrix exponential} (of $A$): The matrix with identical dimensions to $A$ defined by the following power series. \emph{Denoted by} $\textbf{e}^{\bm{A}}$. \emph{Given by}
    \begin{equation*}
        \e[A] = I_n+A+\frac{1}{2!}A^2+\cdots
    \end{equation*}
    \begin{itemize}
        \item This power series is convergent for matrices with $\norm{A}<1$ since $\norm{A^m}\leq\norm{A}^m\to 0$.
        \item Usual rules that you might expect the matrix exponential to obey based on the notation are obeyed.
        \begin{align*}
            \e[(t+\tau)A] &= \e[tA]\e[\tau A]&
            \e[A+B] &= \e[A]\e[B]
        \end{align*}
    \end{itemize}
    \item An explicit formula for the $\e[tA]$.
    \begin{itemize}
        \item We know that $tA=tQBQ^{-1}$, where we may take $B$ be in JNF.
        \item Consider $\e[tJ_3(\lambda)]$, for example.
        \item Then from the above, we have that
        \begin{equation*}
            \e[tJ_3(\lambda)] =
            \begin{pNiceMatrix}
                \e[t\lambda] & t\e[t\lambda] & \frac{t^2}{2}\e[t\lambda]\\
                 & \e[t\lambda] & t\e[t\lambda]\\
                 &  & \e[t\lambda]\\
            \end{pNiceMatrix}
        \end{equation*}
    \end{itemize}
    \item Next time: First order linear systems with constant coefficients; will make use of $\e[tA]$.
    \item Next Wednesday: Review; next Friday: Midterm.
\end{itemize}




\end{document}