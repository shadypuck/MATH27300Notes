\documentclass[../notes.tex]{subfiles}

\pagestyle{main}
\renewcommand{\chaptermark}[1]{\markboth{\chaptername\ \thechapter\ (#1)}{}}
\setcounter{chapter}{5}

\begin{document}




\chapter{Qualitative Theory of ODEs}
\section{More Cauchy-Lipschitz and Intro to Continuous Dependence}
\begin{itemize}
    \item \marginnote{10/31:}Last time, we built up a proof to the Cauchy-Lipschitz theorem intuitively.
    \begin{itemize}
        \item We begin today with a direct proof that is very similar, but slightly different.
    \end{itemize}
    \item Theorem (Cauchy-Lipschitz theorem): Let $f(t,z)$ be defined on an open subset $\Omega\subset\R\times\R^n$, let $(t_0,y_0)\in\Omega$, let $|f|$ be bounded on $\Omega$, and let $f$ be Lipschitz continuous in $z$ and continuous wrt. $t$ in some neighborhood of $(t_0,y_0)$. Then the IVP $y'(t)=f(t,y(t))$, $y(t_0)=y_0$ has a unique solution on $[t_0,t_0+T]$ for some $T>0$ such that $y(t)$ does not escape $\Omega$.
    \begin{proof}
        Let $f(t,z)$ be defined for $(t,z)\in[t_0,t_0+a]\times\bar{B}(y_0,b)\subset\Omega$. Let $|f(t,z)|\leq M$. Let $|f(t,z_1)-f(t,z_2)|\leq L|z_1-z_2|$ for all $z_1,z_2\in\bar{B}(y_0,b)$.\par
        Define $\{y_n\}$ recursively, starting from $y_0(t)=y_0$, by
        \begin{equation*}
            y_{k+1}(t) = y_0+\int_{t_0}^tf(\tau,y_k(\tau))\dd\tau
        \end{equation*}
        Since $f$ is continuous with respect to $t$, it is integrable with respect to $t$, so the above sequence is well-defined on $[t_0,t_0+T]$. Choose $T=\min(a,b/M,1/2L)$. Then
        \begin{equation*}
            \norm{y_k-y_0} \leq T\cdot M
            \leq \frac{b}{M}\cdot M
            = b
        \end{equation*}
        so no $y_k$ escapes $\bar{B}(y_0,b)$. Additionally,
        \begin{align*}
            \norm{y_{k+1}-y_k} &\leq \int_{t_0}^t\norm{f(\tau,y_k(\tau))-f(\tau,y_{k-1}(\tau))}\dd\tau\\
            &\leq TL\norm{y_k-y_{k-1}}\\
            &\leq \frac{1}{2}\norm{y_k-y_{k-1}}\\
            &\leq \left( \frac{1}{2} \right)^k\norm{y_1-y_0}
        \end{align*}
        Thus, the difference between successive terms in the sequence is controlled by a geometric progression, so $\{y_n\}$ is a Cauchy sequence in the function space. It follows that $\{y_k\}$ is uniformly convergent to some continuous $y:[t_0,t_0+T]\to\R^n$.
    \end{proof}
    % \item Consider the IVP $y'=f(t,y)$, $y(t_0)=y_0$. Let $f(t,z)$ be defined for $(t,z)\in[t_0,t_0+a]\times\bar{B}(y_0,b)$, let $|f(t,z)|\leq M$, and let $f$ be $L$ - Lipschitz wrt. $z$, i.e., $|f(t,z_1)-f(t,z_2)|\leq L|z_1-z_2|$ for all $z_1,z_2\in\bar{B}(y_0,b)$.
    % \item Form an iterative sequence $\{y_n\}$ starting from $y_0(t)=y_0$, recursively defined by
    % \begin{equation*}
    %     y_{k+1}(t) = y_0+\int_{t_0}^tf(\tau,y_k(\tau))\dd\tau
    % \end{equation*}
    % \begin{itemize}
    %     \item This sequence is well defined on $[t_0,t_0+T]$.
    %     \item Choose $T=\min(a,b/M,1/2L)$.
    % \end{itemize}
    % \item We have that
    % \begin{equation*}
    %     \norm{y_{k+1}-y_0} \leq T\cdot M \leq b
    % \end{equation*}
    % \begin{itemize}
    %     \item This implies that $\{y_k(t)\}$ does not escape $\bar{B}(y_0,b)$.
    % \end{itemize}
    % \item We have
    % \begin{equation*}
    %     y_{k+1}(t)-y_k(t) = \int_{t_0}^t[f(\tau,y_k(\tau))-f(\tau,y_{k-1}(\tau))]\dd\tau
    % \end{equation*}
    % \begin{itemize}
    %     \item Here we can apply the Lipschitz condition.
    % \end{itemize}
    % \item Taking $\sup_{t\in[t_0,t_0+T]}$, we get
    % \begin{equation*}
    %     \norm{y_{k+1}-y_k} \leq T\sup_{\tau\in[t_0,t_0+T]}|f(\tau,y_k(\tau))-f(\tau,y_{k-1}(\tau))| \leq TL\norm{y_k-y_{k-1}}
    % \end{equation*}
    % where we have applied the Lipschitz condition in the second step.
    % \item By our choice of $T$, $TL\leq 1/2$.
    % \begin{itemize}
    %     \item Thus, the difference between successive terms in the sequence is controlled by a geometric progression.
    %     \item As a result, $\{y_k\}$ is a Cauchy sequence in the function space. In other words, $y_k$ is uniformly convergent to some continuous $y(t)$.
    %     \item For uniformly convergent functions, we can always exchange the limit and the integral.
    % \end{itemize}
    \item This completes the proof. Although it's more concrete than the contraction mapping one, they are virtually the same: In both cases, we obtain an approximate sequence controlled by a geometric progression.
    \item Examples of the Picard iteration:
    \begin{enumerate}
        \item Consider an linear autonomous systems $y'=Ay$, $A$ an $n\times n$ matrix, and $y(0)=y_0$.
        \begin{itemize}
            \item We know that the solution is $y(t)=\e[tA]y_0$. However, we can derive this using the Picard iteration.
            \item Indeed, via this procedure, let's determine the first couple of Picard iterates.
            \begin{align*}
                y_0(t) &= y_0&
                    y_1(t) &= y_0+\int_0^tAy_0(\tau)\dd\tau&
                        y_2(t) &= y_0+\int_0^tAy_1(\tau)\dd\tau\\
                &&
                    &= y_0+tAy_0&
                        &= y_0+tAy_0+\frac{1}{2}t^2A^2y_0
            \end{align*}
            \item It follows inductively that
            \begin{equation*}
                y_k(t) = \sum_{j=0}^k\frac{t^jA^j}{j!}y_0
            \end{equation*}
            \item Since the term above is exactly the power series definition of $\e[tA]$, we have that $y_k(t)\to\e[tA]y_0$ with local uniformity in $t$, as desired.
        \end{itemize}
        \item Consider the ODE $y'=y^2$, $y(0)=1$.
        \begin{itemize}
            \item We know that the solution is $y(t)=1/(1-t)$. We will now also derive this via the Picard iteration.
            \item Choose $b=1$, so that
            \begin{equation*}
                \bar{B}(y_0,b) = \{y\mid |y-y(0)|\leq 1\}
                = \{y\mid |y-1|\leq 1\}
                = [0,2]
            \end{equation*}
            \item On this interval, $f(t,y)=y^2$ has maximum slope $L=4$. Thus, we should take $T\leq 1/2L=1/8$.
            \item It follows that $|y_1^2-y_2^2|\leq 4|y_1-y_2|$ for all $y_1,y_2\in\bar{B}(y_0,b)$.
            \item Calculate the first few Picard iterates.
            \begin{gather*}
                y_1(t) = 1+\int_0^t(y_0(\tau))^2\dd\tau
                    = 1+t\\
                y_2(t) = 1+\int_0^t(1+\tau)^2\dd\tau
                    = 1+t+t^2+\frac{t^3}{3}\\
                y_3(t) = 1+\int_0^t\left( 1+\tau+\tau^2+\frac{\tau^3}{3} \right)^2\dd\tau
                    = 1+t+t^2+t^3+\frac{2t^4}{3}+\frac{t^5}{3}+\frac{t^6}{9}+\frac{t^7}{63}
            \end{gather*}
            \item It follows by induction that
            \begin{align*}
                |y_k(t)-(1+t+\cdots+t^k)| &\leq t^{k+1}\\
                \left| y_k(t)-\frac{1-t^{k+1}}{1-t} \right| &\leq t^{k+1}
            \end{align*}
            It follows that $|t|<1/8$.
            \item For $|t|<1/8$, $y(t)=1/(1-t)$. Blows up as $t\to 1$.
            \item Some more details on the bounding of the error term are presented in the lecture notes document.
        \end{itemize}
    \end{enumerate}
    \item Lemma (Gr\"{o}nwall's inequality): Let $\varphi(t)$ be a real function defined for $t\in[t_0,t_0+T]$ such that
    \begin{equation*}
        \varphi(t) \leq f(t)+a\int_{t_0}^t\varphi(\tau)\dd\tau
    \end{equation*}
    Then
    \begin{equation*}
        \varphi(t) \leq f(t)+a\int_{t_0}^t\e[a(t-\tau)]f(\tau)\dd\tau
    \end{equation*}
    \begin{proof}
        Multiply both sides by $\e[-at]$:
        \begin{align*}
            \e[-at]\varphi(t)-a\e[-at]\int_{t_0}^t\varphi(\tau)\dd\tau &\leq \e[-at]f(t)\\
            \dv{t}(\e[-at]\int_{t_0}^t\varphi(\tau)\dd\tau) &\leq \e[-at]f(t)\\
            \e[-at]\int_{t_0}^t\varphi(\tau)\dd\tau &\leq \int_{t_0}^t\e[-a\tau]f(\tau)\dd\tau\\
            \int_{t_0}^t\varphi(\tau)\dd\tau &\leq \int_{t_0}^t\e[a(t-\tau)]f(\tau)\dd\tau
        \end{align*}
        Substituting back into the original equality yields the result at this point.
    \end{proof}
    \item Note that there is no sign condition on $f(t)$ or $a$.
    \item Gr\"{o}nwall's inequality is very important and we should remember it.
    \item It is also exactly what we need to prove continuous dependence.
    \item Theorem: Let $f(t,z),g(t,z)$ be defined on $\Omega\subset\R_t^1\times\R_z^n$, an open and bounded a region containing $(t_0,y_0)$ and $(t_0,w_0)$. Let the functions be $L$ - Lipschitz wrt. $z$. Consider two initial value problems $y'=f(t,y)$, $y(t_0)=y_0$ and $w'=g(t,w)$, $w(t_0)=w_0$. If $|f(t,z)-g(t,z)|<M$, then for $t\in[t_0,t_0+T]$,
    \begin{equation*}
        |y(t)-w(t)| \leq \e[LT]|y_0-w_0|+\frac{M}{L}(\e[LT]-1)
    \end{equation*}
    \begin{proof}
        We have that
        \begin{align*}
            |y(t)-w(t)| &= \left| \left[ y_0+\int_{t_0}^tf(\tau,y(\tau))\dd\tau \right]-\left[ w_0+\int_{t_0}^tg(\tau,y(\tau))\dd\tau \right] \right|\\
            &= \left| [y_0-w_0]+\int_{t_0}^t[f(\tau,y(\tau))-g(\tau,y(\tau))]\dd\tau \right|\\
            &\leq |y_0-w_0|+\left| \int_{t_0}^t[f(\tau,y(\tau))-g(\tau,w(\tau))]\dd\tau \right|\\
            &\leq |y_0-w_0|+\int_{t_0}^t|f(\tau,y(\tau))-g(\tau,w(\tau))|\dd\tau
        \end{align*}
        where we get from the second to the third line using the triangle inequality, and the third to the fourth line using Theorem 13.26 of Honors Calculus IBL. We also know that
        \begin{align*}
            |f(\tau,y(\tau))-g(\tau,w(\tau))| &\leq |f(\tau,y(\tau))-f(\tau,w(\tau))|+|f(\tau,w(\tau))-g(\tau,w(\tau))|\\
            &\leq L|y(\tau)-w(\tau)|+M
        \end{align*}
        Combining what we've obtained, we have
        \begin{align*}
            \underbrace{|y(t)-w(t)|}_{\psi(t)} &\leq \underbrace{|y_0-w_0|+M(t-t_0)}_{f(t)}+\underbrace{\vphantom{|}L}_a\int_{t_0}^t\underbrace{|y(\tau)-w(\tau)|}_{\psi(t)}\dd\tau\\
            &\leq MT+|y_0-w_0|+L\int_{t_0}^t\e[L(t-\tau)][|y_0-w_0|+M(t-\tau)]\dd\tau\tag*{Gr\"{o}nwall}\\
            &\leq \e[LT]|y_0-w_0|+\frac{M}{L}(\e[TL]-1)
        \end{align*}
        as desired.
    \end{proof}
    \item Note: Getting from directly from Gr\"{o}nwall's inequality in the second line above to the last line above is quite messy. A consequence of Gr\"{o}nwall's inequality explored in the book makes this much easier. \emph{Prove Equation 2.38 via Problem 2.12.}
    \item Implication: The IVP is not just solvable itself, but is solvable wrt. perturbation of the initial conditions and RHS within a small, finite interval in time.
    \item Suppose $y'=0$, $y(0)=1$ and $w'=\varepsilon w$, $w(0)=1$. Then $y(t)=1$ and $w(t)=\e[\varepsilon t]$ and solutions are only close when $t$ is small.
    \begin{itemize}
        \item $t\leq 1/\varepsilon$??
    \end{itemize}
    \item This is important in physics. In most physical scenarios, the RHS is $C^1$. This is called determinism.
\end{itemize}



\section{Differentiability With Respect To Parameters}
\begin{itemize}
    \item \marginnote{11/2:}Review: Implicit Function Theorem.
    \begin{itemize}
        \item Gives you a sufficient condition for which an implicit relation defines a function.
        \item Does not give you the function, but tells you that it must exist and that it is unique.
    \end{itemize}
    \item Theorem (Implicit Function Theorem): Let $F:\R^n\times\R^m\to\R^m$ be $C^k$ in some neighborhood of $(x_0,y_0)\in\R^n\times\R^m$ a point satisfying $F(x_0,y_0)=0$. If the truncated Jacobian matrix ${\pdv{F}{y}}(x_0,y_0)$, which is $m\times m$, is invertible, then there is a neighborhood $U$ of $x_0$ such that there is a unique function $f:U\to\R^m$ with $y_0=f(x_0)$ and $F(x,f(x))=0$ and
    \begin{equation*}
        f'(x) = -\left( {\pdv{F}{y}}(x,y) \right)^{-1}\cdot{\pdv{F}{x}}(x,f(x))
    \end{equation*}
    \begin{itemize}
        \item The proof is based on the Banach fixed point theorem (this may be false?? I think Shao is confusing the proof of this theorem with the proof of the Inverse Function Theorem).
        \item The motivation for the last equality (the line above) is that if $F(x,f(x))=0$, then by the chain rule for partial derivatives,
        \begin{align*}
            0 &= \dv{x}(F(x,f(x)))\\
            &= {\pdv{F}{x}}(x,f(x))\cdot\dv{x}{x}+\left[ {\pdv{F}{y}}(x,y) \right]\cdot\dv{f}{x}\\
            &= {\pdv{F}{x}}(x,f(x))+\left[ {\pdv{F}{y}}(x,y) \right]\cdot f'(x)\\
            f'(x) &= -\left( {\pdv{F}{y}}(x,y) \right)^{-1}\cdot{\pdv{F}{x}}(x,f(x))
        \end{align*}
        \item Recall that we know that the matrix bracketed in line 2 is invertible by hypothesis.
        \item Additionally, since $\pdv*{F}{x}=A$ is $n\times m$ and $\pdv*{F}{y}=B$ is $m\times m$, $f'=-A^{-1}B$ is $n\times m$, as it should be for a function $f:\R^n\to\R^m$.
    \end{itemize}
    \item Consider the IVP
    \begin{equation*}
        y' = f(t,y;\mu)
        ,\quad
        y(t_0) = x(\mu)
    \end{equation*}
    \begin{itemize}
        \item This ODE and its initial condition both depend on a parameter $\mu\in B(0,r)\subset\R^m$ (usually we take $m=1$ so $\mu$ is just real).
        \item We denote the solution by $y(t;\mu)$.
        \item Suppose $|x(\mu)|<C$ for $\mu\in B(0,r)$ and $x(\mu)\in C^1$. Suppose the RHS $f(t,z;\mu)$ of the ODE is defined on $[t_0,t_0+a]\times\bar{B}(x(0),b+C)\times B(0,r)$, is $C^1$ in all variables, is bounded by $M$ on its domain, and is $L$-Lipschitz in $z$.
    \end{itemize}
    \item By Cauchy-Lipschitz, for small
    \begin{equation*}
        T \leq \min\left( a,\frac{b}{M},\frac{1}{2L} \right)
    \end{equation*}
    and $\mu\in B(0,r)$ ($r$ small), the solution \emph{exists} on $[t_0,t_0+T]$ and its value does not escape $\bar{B}(x(0),b+C)$.
    \begin{itemize}
        \item We now aim to show that the solution is \emph{differentiable} wrt. $\mu$ on this interval.
    \end{itemize}
    \item If $y(t;\mu)$ satisfies $y'(t;\mu)=f(t,y(t;\mu);\mu)$ and if the Jacobian matrix $J=\pdv*{y}{\mu}$ exists, then $J$ satisfies the \textbf{first variation equation}.
    \item \textbf{First variation equation}: The following linear differential equation. \emph{Given by}
    \begin{equation*}
        \dv{t}~\underbrace{{\pdv{y}{\mu}}(t;\mu)}_{J(t;\mu)} = \underbrace{{\pdv{f}{z}}(t,y(t;\mu);\mu)}_{A(t;\mu)}\cdot\underbrace{{\pdv{y}{\mu}}(t;\mu)}_{J(t;\mu)}+\pdv{f}{\mu}~(t,y(t;\mu);\mu)
        ,\quad
        \pdv{y}{\mu}~(t_0,\mu) = \pdv{x}{\mu}~(\mu)
    \end{equation*}
    \item The first variation equation has a unique solution, but we do not yet know that $y(t;\mu)$ is even differentiable with respect to $\mu$. We presently verify this claim.
    \item Theorem\footnote{See the proof from the book, transcribed below.}: $y(t;\mu)$ is $C^1$ in $\mu$ and ${\pdv*{y}{\mu}}(t;\mu)$ satisfies the first variation equation.
    \begin{proof}
        Let $\Theta(t;\mu)=y(t;\mu+h)-y(t;\mu)-J(t;\mu)h$ for $h$ small. Aim, show that $\Theta(t;\mu)=o(h)$ as $h\to 0$.\par
        We compute
        \begin{align*}
            \dv{t}\Theta(t;\mu) &= y'(t;\mu+h)-y'(t;\mu)-J'(t;\mu)h\\
            &= \underbrace{f(t,y(t;\mu+h);\mu+h)-f(t,y(t;\mu);\mu)}_I-\underbrace{{\pdv{f}{z}}(t,y(t;\mu);\mu)J(t;\mu)+{\pdv{f}{\mu}}(t,y(t;\mu);\mu)}_{II}
        \end{align*}
        $I$ denotes the first term; $II$ denotes the second term.\par
        We have that
        \begin{align*}
            I &= {\pdv{f}{z}}(t,y(t;\mu);\mu)[y(t;\mu+h)-y(t;\mu)]+{\pdv{f}{\mu}}(t,y(t;\mu);\mu)h+\underbrace{R(t;\mu,h)}_{o(h)}
        \end{align*}
        \emph{color coding}
        \begin{align*}
            I-II &= \underbrace{\text{green}-\text{blue}}_{\Theta(t;\mu)}+R(t;\mu,h)\\
            &= \dv{t}\Theta(t;\mu) = \Theta(t;\mu)+\underbrace{R(t;\mu,h)}_{o(h)}\\
            \Theta(t_0;\mu) &= o(h)\\
            |\Theta(t;\mu)| &\leq C\int_{t_0}^t|R(\tau;\mu,h)|\dd\tau\tag*{Gr\"{o}nwall}\\
            &= o(h)
        \end{align*}
        circle terms cancel.
    \end{proof}
    \item Example: First order derivatives must satisfy the first variational equation
    \begin{equation*}
        \dv{t}~{\pdv{y}{\mu}}(t;\mu) = {\pdv{f}{z}}(t,y(t;\mu);\mu)\cdot{\pdv{y}{\mu}}(t;\mu)
    \end{equation*}
    and the second order derivative must satisfy the second variational equation
    \begin{equation*}
        \dv{t}~\pdv[2]{y}{\mu} = {\pdv[2]{f}{z}}\left( \pdv{y}{\mu}\pdv[2]{y}{\mu} \right)+\pdv{f}{z}{\mu}\pdv{y}{\mu}+{\pdv{f}{\mu}{z}}(-)\pdv{y}{\mu}+{\pdv[2]{f}{\mu}}(-)
    \end{equation*}
    \item Corollary: If $f(t,z;\mu)$ is $C^k$ in $(t,z,\mu)$, $y(t_0)=x(\mu)$ is $C^k$, then $y(t;\mu)$ is $C^k$ in $\mu$.
    \item The Taylor expansion
    \begin{equation*}
        y(t;\mu) = y(t;0)+y_1\mu+y_2\mu^2+\cdots+y_k\mu^k+O(\mu^{k+1})
    \end{equation*}
    of $y(t;\mu)$ about 0 gives an approximation of said function up to order $k$ in $\mu$.
    \begin{itemize}
        \item Misc notes: but you can cut off the expansion at $k$?? $y(t;0)$ being solvable implies inductively that the rest are solvable??
        \item We can take this Taylor expansion because we assume that $y$ is continuously differentiable $k$ times with respect to $\mu$.
        \item The coefficients $y_j$ are given as follows.
        \begin{equation*}
            y_j = \frac{1}{j!}\pdv[j]{y}{\mu}~(t;0)
        \end{equation*}
    \end{itemize}
    \item Application of the Taylor expansion: It can be substituted into the ODE as follows.
    \begin{align*}
        \dot{y} &= f(t,y;\mu)\\
        \dv{t}(y(t;\mu)) &= f(t,y(t;\mu);\mu)\\
        \dv{t}(y(t;0))+\dv{y_1}{t}\mu+\cdots+\dv{y_k}{t}\mu^k+O(\mu^{k+1}) &= f(t,y(t;0)+y_1\mu+\cdots+y_k\mu^k+O(\mu^{k+1});\mu)
    \end{align*}
    \begin{itemize}
        \item Then you can match coefficients of the various $\mu$ terms on the LHS and RHS and solve for $y_0,\dots,y_k$.
        \item When to use this method: Sometimes, you can view equations that aren't explicitly solvable as perturbations of an easily solvable system.
    \end{itemize}
    \item Simple example (more complex ones next lecture):
    \begin{equation*}
        \dv{y}{t} = \mu y
        ,\quad
        y(0) = 1
    \end{equation*}
    \begin{itemize}
        % \item We have $\dv*{y_0}{t}=0$ and $y_0(0)=1$ so $y_0(t)=1$.
        % \item First:
        % \begin{equation*}
        %     y_0'(t)+\mu y_1'(t)+O(\mu^2) = \mu(y_0(t)+\mu y_1(t)+O(\mu^2))
        % \end{equation*}
        % so $y_1'(t)=\mu y_0(t)$, $y_1(0)=0$. Implies that $y_1(t)=t\mu$.
        % \item Second:
        % \begin{equation*}
        %     y_0'+\mu y_1'+\mu^2y_2'+O(\mu^3) = \mu(y_0+\mu y_1+\mu^2y_2+O(\mu^3))
        % \end{equation*}
        % so $y_2'(t)=y_1$, $y_2(0)=0$. Implies that $y_2(t)=\frac{1}{2}t^2$.
        % \item Note that $y(t)=1+t\mu+\frac{1}{2}t^2\mu^2+O(\mu^3)$ does indeed give the first three terms in the Taylor series of $\e[\mu t]$.
        \item First off, we know that there is an explicit solution ($y(t)=\e[\mu t]$). Thus, we will be able to check our final answer.
        \item Suppose $y\in C^2$ with respect to $\mu$. Then
        \begin{equation*}
            y(t;\mu) = y_0+y_1\mu+y_2\mu^2+O(\mu^3)
        \end{equation*}
        \item It follows by substituting into the above differential equation that
        \begin{align*}
            \dv{y}{t} &= \mu y\\
            \dv{t}(y_0+y_1\mu+y_2\mu^2) &= \mu(y_0+y_1\mu+y_2\mu^2)\\
            \dv{y_0}{t}+\dv{y_1}{t}\mu+\dv{y_2}{t}\mu^2 &= 0+y_0\mu+y_1\mu^2+y_2\mu^3
        \end{align*}
        \item By comparing coefficients, this yields the sequentially solvable differential equations
        \begin{align*}
            \dv{y_0}{t} &= 0&
            \dv{y_1}{t} &= y_0&
            \dv{y_2}{t} &= y_1
        \end{align*}
        where we apply the initial condition $y_0(0)=1$ to solve the left ODE above.
        \item Solving, we get
        \begin{align*}
            y_0(t) &= 1&
            y_1(t) &= t&
            y_2(t) &= \frac{t^2}{2}
        \end{align*}
        \begin{itemize}
            \item Where do the other initial conditions (all zero) come from??
        \end{itemize}
        \item Therefore, our approximate solution is
        \begin{equation*}
            y(t) = 1+t\mu+\frac{1}{2}t^2\mu^2+O(\mu^3)
        \end{equation*}
        which does indeed give the first three terms in the Taylor series expansion of the solution $\e[\mu t]$.
    \end{itemize}
    \item The perturbative solution fails in large time intervals --- polynomials inevitably grow slower than exponential functions.
    \item Next time: Several examples applying what we've learned today.
    \item This week's homework: Some basic Lipschitz definitions and also computations with the perturbative series.
\end{itemize}



\section{Variational Examples}
\begin{itemize}
    \item \marginnote{11/4:}We begin today with a more direct and less involved proof of the variation of parameters theorem.
    \begin{proof}
        Let $y'(t;\mu)=f(t,y(t;\mu);\mu)$ with $y(t_0;\mu)=x(\mu)$. Assume Lipschitz continuity and $C^1$-ness of the ODE and the initial condition on $\mu$. Then differentiation with respect to $\mu$ must satisfy the first variational equation. In particular, let $J(t;\mu)$ be the solution of
        \begin{equation*}
            J'(t;\mu) = \underbrace{{\pdv{f}{z}}(t,y(t;\mu);\mu)}_{A(t;\mu)}J(t,\mu)+\underbrace{{\pdv{f}{\mu}}(t,y(t;\mu);\mu)}_{F(t;\mu)}
            ,\quad
            J(t_0;\mu) = \pdv{x}{\mu}
        \end{equation*}
        Consider the Picard iteration sequence defined by
        \begin{align*}
            y_{n+1}(t;\mu) = \underbrace{f(t;y_n(t;\mu);\mu)}_{A_n(t;\mu)}
            ,\quad
            y_n(t_0,\mu) = x(\mu)
        \end{align*}
        Differentiating we get
        \begin{equation*}
            {\pdv{y_n}{\mu}}(t;\mu)
        \end{equation*}
        which we may call $J_n(t;\mu)$. We want to prove that the sequence of functions $J_n$ converges uniformly to $J$. This makes sense since $A$ and $F$ uniformly converge. Moreover, under this definition of $J_n$, we have that
        \begin{equation*}
            J_{n+1}'(t;\mu) = \underbrace{{\pdv{f}{z}}(t,y_n(t;\mu);\mu)}_{A_n(t;\mu)}J_n(t;\mu)+\underbrace{{\pdv{f}{\mu}}(t,y_n(t;\mu);\mu)}_{F_n(t;\mu)}
            ,\quad
            J_n(t_0;\mu) = {\pdv{x}{\mu}}(\mu)
        \end{equation*}
        Thus, Step 1 is to show that $\{\norm{J_n}\}$ is bounded on $[t_0,t_0+T]$. To do so, we note that
        \begin{equation*}
            \norm{J_{n+1}} \leq \frac{1}{2}\norm{J_n}+\sup\left| \pdv{f}{\mu} \right|
        \end{equation*}
        so that $\norm{J_n}$ forms a bounded sequence. By induction,
        \begin{equation*}
            \norm{J_n} \leq 2C
        \end{equation*}
        We now embark on Step 2: Proving $J_n\to J$ uniformly. First off, we have that
        \begin{align*}
            (J-J_{n+1})'(t;\mu) ={}& \dv{t}(J(t;\mu)-J_{n+1}(t;\mu))\\
            ={}& A(t;\mu)J(t;\mu)+F(t;\mu)-A_n(t;\mu)J_n(t;\mu)-F_n(t;\mu)\\
            \begin{split}
                ={}& A(t;\mu)J(t;\mu)+A_n(t;\mu)J(t;\mu)-A_n(t;\mu)J(t;\mu)\\
                &-A_n(t;\mu)J_n(t;\mu)+F(t;\mu)-F_n(t;\mu)
            \end{split}\\
            ={}& A_n(t;\mu)(J-J_n)(t;\mu)+(A-A_n)(t;\mu)J(t;\mu)+(F-F_n)(t;\mu)
        \end{align*}
        and
        \begin{equation*}
            J(t_0;\mu)-J_{n+1}(t_0;\mu) = 0
        \end{equation*}
        Integrating once again on $[t_0,t_0+T]$, we get
        \begin{equation*}
            \norm{J-J_{n+1}} \leq \frac{1}{2}\norm{J-J_n}+\delta_n
        \end{equation*}
        where $\delta_n\to 0$ since we "obviously" have that $A_n\to A$ and $F_n\to F$ uniformly.\par
        We now proceed via a standard analysis argument. Fix $\delta>0$, choose $N$ such that $\delta_n<\delta$ for $n\geq N$. Then we can control it by $\frac{1}{2}\norm{J-J_n}+\delta$ for $n\geq N$. Then
        \begin{equation*}
            \norm{J-J_{n+1}}-2\delta \leq \frac{1}{2}\norm{J-J_n}-2\delta
        \end{equation*}
        for all $n\geq N$, so we have by iteration that $\norm{J-J_{n+1}}\leq 2\delta+\frac{1}{2^{n-N}}\norm{J-J_N}$, so $\lim_{n\to\infty}\norm{J-J_n}<2\delta$ for arbitrary $\delta>0$. Therefore, $\norm{J-J_n}\to 0$, so $J_n\to J$ uniformly.\par
        So in conclusion, $J_n\to J$ uniformly and we recall that $J_n=\pdv*{y_n}{\mu}$ where $y_n\to y$ uniformly.
    \end{proof}
    \item We now look at examples. The ones in the HW will be no more difficult than these.
    \item Example (same one as last time):
    \begin{itemize}
        \item Consider $y'=\mu y$ with $y(0)=1$.
        \item In order to find asymptotic expansion wrt. $\mu$, we use the \textbf{ansatz} $y(t;\mu)=y_0+y_1\mu+y_2\mu^2+\cdots+y_n\mu^n+O(\mu^{n+1})$.
        \begin{itemize}
            \item The differentiation theorem asserts that $y(t;\mu)$ can be differentiated wrt. $\mu$ so many times.
        \end{itemize}
        \item We can compute
        \begin{equation*}
            \mu y(t;\mu) = 0+y_0\mu+y_1\mu^2+\cdots+y_{n-1}\mu^n+O(\mu^{n+1})
        \end{equation*}
        and
        \begin{equation*}
            y'(t;\mu) = y_0'+y_1'\mu+y_2'\mu^2+\cdots+y_n'\mu^n+O(\mu^{n+1})
        \end{equation*}
        and set them equal to yield a system of differential equations.
        \item The initial conditions are $y_0(0)=1$ and then $y_1(0)=\cdots=y_n(0)=0$.
        \item $y_0'=0$ with $y_0(0)=1$ implies that $y_0(t)=1$.
        \item Then the first order approximation is $y_1'=y_0=1$, so solving and applying the initial conditions, we get $y_1(t)=t$.
        \item Continuing on, the second order approximation is $y_2(t)=t^2/2$.
        \item Inductively, $y_m(t)=t^m/m!$.
        \item In conclusion, we obtain the desired approximate solution.
    \end{itemize}
    \item \textbf{Ansatz}: The form of the solution that you guess.
    \item In general, this shows the technique well: Use a polynomial ansatz and compare terms to yield an inductive sequence of explicitly solvable equations up to a certain point.
    \item Example: Mathematical pendulum.
    \begin{itemize}
        \item Suppose that the length of the rope is $\ell$ and the gravitational acceleration is $g$. Then
        \begin{equation*}
            \theta''(t;\mu) = -\frac{g}{\ell}\sin[\theta(t;\mu)]
        \end{equation*}
        \item Assume a small angle, $\theta(0)=\mu$ and $\theta'(0)=0$.
        \item Substitute $\omega_0^2=g/\ell$.
        \item In HS, we learned that the harmonic oscillator approximation of the mathematical pendulum is justified for small $\theta$. We now justify this.
        \item Ansatz: $\theta_0+\theta_1\mu+\theta_2\mu^2+\theta_3\mu^3+O(\mu^4)$.
        \item Recall that
        \begin{equation*}
            \sin\theta = \theta-\frac{\theta^3}{6}+O(\theta^5)
        \end{equation*}
        \item First step, solve to determine $\theta_0=0$.
        \item Then we only have a term of order $O(\mu)$ and $O(\mu^3)$ to worry about.
        \item Substitute the expansion in:
        \begin{align*}
            \sin\theta &= \theta-\frac{\theta^3}{6}+O(\theta^5)\\
            &= \left( \theta_0+\theta_1\mu+\theta_2\mu^2+\theta_3\mu^3 \right)-\frac{1}{6}\left( \theta_0+\theta_1\mu+\theta_2\mu^2+\theta_3\mu^3 \right)^3\\
            &= 0+\theta_1\mu+\theta_2\mu^2+\left( \theta_3-\frac{\theta_1^3}{6} \right)\mu^3+O(\mu^4)
        \end{align*}
        \item We also have that
        \begin{equation*}
            \theta''(t;\mu) = \theta_1''\mu+\theta_2''\mu^2+\theta_3''\mu^3+O(\theta^4)
        \end{equation*}
        and
        \begin{equation*}
            -\omega_0^2\sin(\theta_1\mu+\theta_2\mu^2+\theta_3\mu^3+O(\mu^4)) = -\omega_0^2\theta_1\mu-\omega_0^2\theta_2\mu^2-\omega_0^2\left( \theta_3-\frac{\theta_1^3}{6} \right)\mu^3+O(\mu^4)
        \end{equation*}
        \item Initial conditions: $\theta_0=0$, $\theta_1(0)=1$, and $\theta_2(0) = \theta_3(0) = \theta_1'(0) = \cdots = \theta_3'(0) = 0$.
        \item First order: $\theta_1''=-\omega_0^2\theta_1$, $\theta_1(0)=1$, $\theta_1'(0)=0$. Implies $\theta_1(t)=\cos\omega_0t$. This is why we can use the harmonic oscillator approximation.
        \item Second order: $\theta_2=-\omega_0^2\theta_2$. Initial conditions imply $\theta_2(t)=0$.
        \item Third order: $\theta_3''=-\omega_0^2\theta_3+\frac{\omega_0^2\theta_1^3}{6}$. Implies that
        \begin{equation*}
            \theta_3(t) = \frac{\omega_0t}{16}\sin\omega_0t+\frac{1}{192}(\cos\omega_0t-\cos 3\omega_0t)
        \end{equation*}
        \begin{itemize}
            \item We have to apply some trigonometric identities to verify this??
        \end{itemize}
        \item In conclusion, we have the approximation of our solution up to order $O(\mu^3)$ as
        \begin{equation*}
            \theta(t;\mu) = \mu\cos\omega_0t+\mu^3\left[ \frac{\omega_0t}{16}\sin\omega_0t+\frac{1}{192}(\cos\omega_0t-\cos 3\omega_0t) \right]+O(\mu^4)
        \end{equation*}
        \begin{itemize}
            \item This approximation is only good for $T$ in a fixed, small time interval because the second term is not periodic.
        \end{itemize}
    \end{itemize}
    \item We now investigate the period of the mathematical pendulum.
    \begin{itemize}
        % \item Expectation: $T(\mu)=4$ (first positive zero of $\theta(t;\mu)$).
        % \begin{equation*}
        %     \frac{\theta(t;\mu)}{\mu} = \cos\omega t+\mu^2\left[ \frac{\omega_0t}{16}\sin\omega_0t+\frac{1}{192}(\cos\omega_0t-\cos 3\omega_0t) \right]+O(\mu^4)
        % \end{equation*}
        % \item $F(t;\mu) = \cos\omega t+\mu^2\left[ \frac{\omega_0t}{16}\sin\omega_0t+\frac{1}{192}(\cos\omega_0t-\cos 3\omega_0t) \right]+O(\mu^4)=0$. Want $F(f(\mu);\mu)=0$. Apply the implicit function theorem at $(t_0,\mu_0)=(\pi/2\omega_0,0)$.
        % \item Take ${\pdv{F}{t}}(t_0,\mu_0)=-\omega_0^2\sin\omega_0t_0+\mu_0^2\left( \omega_0/16*\sin\omega_0t_0+\frac{\omega_0^2}{16}t_0\cos\omega_0t_0+\frac{1}{192}(\dots) \right)$. The second term cancels. Thus, this derivative equals $-\omega_0^2\neq 0$. It follows that ?? is indeed uniquely determined by ??.
        % \item Using the law of implicit differentiation, we get
        % \begin{equation*}
        %     f(\mu) = \frac{\pi}{2\omega_0}+b_1\mu+b_2\mu^2+O(\mu^3)
        % \end{equation*}
        % \item Take
        % \begin{align*}
        %     F(f(\mu);\mu) &= \cos(\frac{\pi}{2}+\omega_0b_1\mu+\omega_0b_2\mu^2+O(\mu^3))+\mu^2(\frac{1}{16}(\frac{\pi}{2}++\omega_0b_1\mu+\omega_0b_2\mu^2+O(\mu^3)+...))\\
        %     &= -\omega_0b_1\mu+(\frac{\pi}{32}-\omega_0b_2)\mu^2+O(\mu^3)
        % \end{align*}
        % \item Implicit relation implies that the above equals zero. Thus, we must have $b_1=0$ and $b_2=\pi/32\omega_0$.
        % \item In conclusion, the period is $T(\mu)=2\pi\sqrt{\ell/g}(1+\frac{\pi^2}{16}\mu^2+O(\mu^3))$.
        % \item In conclusion, the ODE induces a second order correction, which will be very small.
        \item The first order approximation (harmonic oscillator) gives the period as $T\approx 2\pi/\omega_0=2\pi\sqrt{\ell/g}$.
        \item Let $T(\mu)$ denote the period of the mathematical pendulum as a function of the starting angle $\mu$.
        \item $T(\mu)$ should be approximately equal to the period of $\theta(t;\mu)$. Additionally, thinking about the mathematical pendulum intuitively, the period $T(\mu)$ should be about four times the first positive zero of $\theta(t;\mu)$.
        \begin{itemize}
            \item Indeed, in a full cycle, the pendulum must go from the positive extreme, to zero, to the negative extreme, back to zero, and back to the original position, so there are our four parts.
            \item Example: In the harmonic oscillator approximation, the first zero is at $\pi/2\omega_0$, and the period is $2\pi/\omega_0=4\cdot\pi/2\omega_0$.
        \end{itemize}
        \item Thus, determining the period $T(\mu)$ becomes a problem of finding $t$ such that $\theta(t;\mu)=0$.
        \item The zeroes of $\theta(t;\mu)$ will be equal to the zeroes of $\theta(t;\mu)/\mu$, so we seek $t$ such that the implicit function
        \begin{equation*}
            F(t;\mu) = \frac{\theta(t;\mu)}{\mu}
            = \cos\omega_0t+\mu^2\left[ \frac{\omega_0t}{16}\sin\omega_0t+\frac{1}{192}(\cos\omega_0t-\cos 3\omega_0t) \right]
            = 0
        \end{equation*}
        \item When $\mu=0$, the mathematical pendulum is stationary, but this does technically mean that it has a zero at $(\pi/2\omega_0;0)$. This point is important because for $\mu$ small enough that the harmonic oscillator approximation is good, the first zero should be very close to $\pi/2\omega_0$. Thus, we choose to solve $F(t;\mu)=0$ around $(t_0;\mu_0)=(\pi/2\omega_0;0)$.
        \item The requirement for the Implicit Function Theorem is met since
        \begin{align*}
            {\pdv{F}{t}}(t_0;\mu_0) &= -\omega_0\sin\omega_0t_0+\mu_0^2\left( \frac{\omega_0}{16}\sin\omega_0t_0+\frac{\omega_0^2t_0}{16}\cos\omega_0t_0+\frac{1}{192}(-\omega_0\sin\omega_0t_0+3\omega_0\sin 3\omega_0t_0) \right)\\
            &= -\omega_0\sin\frac{\pi}{2}+0^2(\dots)\\
            &= -\omega_0\\
            &\neq 0
        \end{align*}
        \item Thus, there exists $t_1(\mu)$ smooth defined on some neighborhood of $\mu_0=0$ satisfying $t_1(0)=\pi/2\omega_0$ and $F(t_1(\mu);\mu)=0$.
        \item We cannot (easily??) obtain $t_1(\mu)$ directly, so we will look for its second-order Taylor expansion
        \begin{equation*}
            t_1(\mu) = \frac{\pi}{2\omega_0}+b_1\mu+b_2\mu^2+O(\mu^3)
        \end{equation*}
        \item We need not compute a bunch of derivatives to find $b_1,b_2$, though. Indeed, we can just substitute into $F(t_1(\mu);\mu)=0$ and compare different powers of $\mu$. Doing so, we obtain
        \begin{align*}
            0 ={}& F(t_1(\mu);\mu)\\
            \begin{split}
                ={}& \cos(\frac{\pi}{2}+\omega_0b_1\mu+\omega_0b_2\mu^2+O(\mu^3))\\
                &+ \mu^2\left[ \frac{1}{16}\left( \frac{\pi}{2}+\omega_0b_1\mu+\omega_0b_2\mu^2+O(\mu^3) \right)\sin(\frac{\pi}{2}+\omega_0b_1\mu+\omega_0b_2\mu^2+O(\mu^3)) \right.\\
                &+ \left. \frac{1}{192}\left( \cos(\frac{\pi}{2}+\omega_0b_1\mu+\omega_0b_2\mu^2+O(\mu^3))-\cos 3\left( \frac{\pi}{2}+\omega_0b_1\mu+\omega_0b_2\mu^2+O(\mu^3) \right) \right) \right]
            \end{split}\\
            ={}& -\omega_0b_1\mu+\left( \frac{\pi}{32}-\omega_0b_2 \right)\mu^2+O(\mu^3)
        \end{align*}
        from which we can determine that
        \begin{align*}
            0 &= -\omega_0b_1&
            0 &= \frac{\pi}{32}-\omega_0b_2\\
            b_1 &= 0&
            b_2 &= \frac{\pi}{32\omega_0}
        \end{align*}
        \item Thus,
        \begin{align*}
            T(\mu) &= 4\cdot t_1(\mu)\\
            &= \frac{2\pi}{\omega_0}+\frac{\pi}{8\omega_0}\mu^2+O(\mu^3)\\
            &= 2\pi\sqrt{\frac{\ell}{g}}\left( 1+\frac{1}{16}\mu^2+O(\mu^3) \right)
        \end{align*}
    \end{itemize}
    \item We calculate an accumulation that is a perturbation of an ODE in the bonus this week, reproducing Einstein's work.
\end{itemize}



\section{Chapter 2: Initial Value Problems}
\emph{From \textcite{bib:Teschl}.}
\subsection*{Section 2.4: Dependence on the Initial Condition}
\begin{itemize}
    \item \marginnote{11/15:}In applications from which ODEs are derived, we usually only know several data approximately. In other words, we're primarily concerned with \textbf{well-posed} IVPs.
    \item \textbf{Well-posed} (IVP): An IVP for which, from an intuitive standpoint, small changes in the data result in small changes of the solution.
    \item That an IVP (under certain conditions) is well-posed will be proven by our next theorem.
    \item To prove this theorem, we will need the following lemma.
    \item Lemma 2.7 (Generalized Gr\"{o}nwall's inequality): Suppose $\psi(t)$ satisfies
    \begin{equation*}
        \psi(t) \leq \alpha(t)+\int_0^t\beta(s)\psi(s)\dd{s}
    \end{equation*}
    for all $t\in[0,T]$. Suppose also that $\alpha(t)\in\R$ and $\beta(t)\geq 0$ for all $t\in[0,T]$. Then
    \begin{equation*}
        \psi(t) \leq \alpha(t)+\int_0^t\alpha(s)\beta(s)\exp(\int_s^t\beta(r)\dd{r})\dd{s}
    \end{equation*}
    for all $t\in[0,T]$.\par
    If, in addition, $\alpha(s)\leq\alpha(t)$ for $s\leq t$, then
    \begin{equation*}
        \psi(t) \leq \alpha(t)\exp(\int_0^t\beta(s)\dd{s})
    \end{equation*}
    for all $t\in[0,T]$.
    \begin{proof}
        Let
        \begin{equation*}
            \phi(t) := \exp(-\int_0^t\beta(s)\dd{s})
        \end{equation*}
        Then we have
        \begin{align*}
            \dv{t}(\phi(t)\int_0^t\beta(s)\psi(s)\dd{s}) &= -\beta(t)\phi(t)\cdot\int_0^t\beta(s)\psi(s)\dd{s}+\phi(t)\cdot\beta(t)\psi(t)\\
            &= \beta(t)\phi(t)\left( \phi(t)-\int_0^t\beta(s)\phi(s)\dd{s} \right)\\
            &\leq \alpha(t)\beta(t)\phi(t)
        \end{align*}
        where the first equality holds by the product rule and the FTC, and the last inequality above holds by the first assumption in the statement of the lemma. Integrating the above inequality with respect to $t$ and dividing the result by $\phi(t)$ shows that
        \begin{align*}
            \int_0^t\beta(s)\psi(s)\dd{s} &\leq \int_0^t\alpha(s)\beta(s)\frac{\phi(s)}{\phi(t)}\dd{s}
            \alpha(t)+\int_0^t\beta(s)\psi(s)\dd{s} &\leq \alpha(t)+\int_0^t\alpha(s)\beta(s)\exp(\int_s^t\beta(r)\dd{r})\dd{s}
        \end{align*}
        It follows that
        \begin{equation*}
            \psi(t) \leq \alpha(t)+\int_0^t\beta(s)\psi(s)\dd{s}
            \leq \alpha(t)+\int_0^t\alpha(s)\beta(s)\exp(\int_s^t\beta(r)\dd{r})\dd{s}
        \end{equation*}
        as desired.\par
        The proof of the second claim is covered in Problem 2.11 (and is not applicable to course content).
    \end{proof}
    \item A simple consequence of the generalized Gr\"{o}nwall's inequality.
    \begin{itemize}
        \item If
        \begin{equation*}
            \psi(t) \leq \alpha+\int_0^t(\beta\psi(s)+\gamma)\dd{s}
        \end{equation*}
        for all $t\in[0,T]$, where $\alpha,\gamma\in\R$ and $\beta\geq 0$, then
        \begin{equation*}
            \psi(t) \leq \alpha\e[\beta t]+\frac{\gamma}{\beta}(\e[\beta t]-1)
        \end{equation*}
        for all $t\in[0,T]$.
        \item See Problem \ref{prb:2.12} for the proof.
    \end{itemize}
    \item We can now show that the IVP is well-posed.
    \item Theorem 2.8: Suppose $f,g\in C(U,\R^n)$ and let $f$ be locally Lipschitz continuous in the second argument, uniformly with respect to the first. If $x(t),y(t)$ are respective solutions of the IVPs
    \begin{align*}
        \dot{x} &= f(t,x)&
            \dot{y} &= g(t,y)\\
        x(t_0) &= x_0&
            y(t_0) &= y_0
    \end{align*}
    then
    \begin{equation*}
        |x(t)-y(t)| \leq |x_0-y_0|\e[L|t-t_0|]+\frac{M}{L}(\e[L|t-t_0|]-1)
    \end{equation*}
    where $L$ is the Lipschitz constant of $f:V\to\R^n$, $M=\norm{f-g}$ for $f,g:V\to\R^n$, and $V\subset U$ contains $G(x),G(y)$.
    \begin{proof}
        WLOG let $t_0=0$. Then
        \begin{align*}
            |x(t)-y(t)| &\leq |x_0-y_0|+\int_0^t|f(s,x(s))-g(s,y(s))|\dd{s}\\
            &\leq |x_0-y_0|+\int_0^t(L|x(s)-y(s)|+M)\dd{s}
        \end{align*}
        Thus, taking
        \begin{equation*}
            \underbrace{|x(t)-y(t)|}_{\psi(t)} \leq \underbrace{|x_0-y_0|}_\alpha+\int_0^t(\underbrace{\vphantom{|}L}_\beta\underbrace{|x(s)-y(s)|}_{\phi(s)}+\underbrace{\vphantom{|}M}_\gamma)\dd{s}
        \end{equation*}
        we have by the above consequence of the generalized Gr\"{o}nwall's inequality that
        \begin{equation*}
            |x(t)-y(t)| \leq |x_0-y_0|\e[Lt]+\frac{M}{L}(\e[Lt]-1)
        \end{equation*}
        as desired.
    \end{proof}
    \item Establishing continuous dependence on the initial condition.
    \begin{itemize}
        \item Denote the solution of the IVP by $\phi(t,t_0,x_0)$ to emphasize the dependence on the initial condition.
        \item Then in the special case $f=g$ (i.e., where $M=0$), Theorem 2.8 implies that
        \begin{equation*}
            |\phi(t,t_0,x_0)-\phi(t,t_0,y_0)| \leq |x_0-y_0|\e[L|t-t_0|]
        \end{equation*}
        \item In other words, $\phi$ depends continuously on the initial value.
        \item Of course, this bound blows up exponentially as $t$ increases, but the linear equation $\dot{x}=x$ shows that we cannot define a better bound in general.
    \end{itemize}
    \item We now formalize the above notion.
    \item Theorem 2.9: Suppose $f\in C(U,\R^n)$ is locally Lipschitz continuous in the second argument, uniformly with respect to the first. Around each point $(t_0,x_0)\in U$, we can find a compact set $I\times B\subset U$ such that $\phi(t,s,x)\in C(I\times I\times B,\R^n)$. Moreover, $\phi(t,t_0,x_0)$ is Lipschitz continuous with
    \begin{equation*}
        |\phi(t,t_0,x_0)-\phi(s,s_0,y_0)| \leq |x_0-y_0|\e[L|t-t_0|]+(|t-s|+|t_0-s_0|\e[L|t-s_0|])M
    \end{equation*}
    where $L$ is the Lipschitz constant of $f:V\to\R^n$, $M=\norm{f}$ for $f:V\to\R^n$, and $V\subset U$ compact contains $I\times\phi(I\times I\times B)$.
    \begin{proof}
        By the Picard-Lindel\"{o}f theorem, there exists $V=[t_0-\varepsilon,t_0+\varepsilon]\times\overline{B_\delta(x_0)}$ such that $\phi(t,t_0,x_0)$ exists and is continuous for $|t-t_0|<\varepsilon$. It can be shown that $\phi(t,t_1,x_1)$ exists for $|t-t_1|\leq\varepsilon/2$, provided that $|t_1-t_0|\leq\varepsilon/2$ and $|x_1-x_0|\leq\delta/2$. Thus, choose $I=[t_0-\varepsilon/2,t_0+\varepsilon/2]$ and $B=\overline{B_{\delta/2}(x_0)}$.\par
        Moreover, we have that
        \begin{align*}
            \begin{split}
                |\phi(t,t_0,x_0)-\phi(s,s_0,y_0)| \leq{}& |\phi(t,t_0,x_0)-\phi(t,t_0,y_0)|\\
                &+|\phi(t,t_0,y_0)-\phi(t,s_0,y_0)|\\
                &+|\phi(t,s_0,y_0)-\phi(s,s_0,y_0)|
            \end{split}\\
            \begin{split}
                \leq{}& |x_0-y_0|\e[L|t-t_0|]\\
                &+\left| \int_{t_0}^tf(r,\phi(r,t_0,y_0))\dd{r}-\int_{s_0}^tf(r,\phi(r,s_0,y_0))\dd{r} \right|\\
                &+\left| \int_s^tf(r,\phi(r,s_0,y_0))\dd{r} \right|
            \end{split}
        \end{align*}
        We estimated the first term using the note on continuous dependence directly preceding this theorem and proof. We can estimate the third term to be $M|t-s|$. We can estimate the second term as follows: Abbreviating $\Delta(t):=|\phi(t,t_0,y_0)-\phi(t,s_0,y_0)|$ and assuming WLOG that $t_0\leq s_0\leq t$, we have that
        \begin{align*}
            \Delta(t) &= \left| \int_{t_0}^tf(r,\phi(r,t_0,y_0))\dd{r}-\int_{s_0}^tf(r,\phi(r,s_0,y_0))\dd{r} \right|\\
            &= \left| \int_{t_0}^{s_0}f(r,\phi(r,t_0,y_0))\dd{r}+\int_{s_0}^tf(r,\phi(r,t_0,y_0))\dd{r}-\int_{s_0}^tf(r,\phi(r,s_0,y_0))\dd{r} \right|\\
            &\leq \int_{t_0}^{s_0}|f(r,\phi(r,t_0,y_0))|\dd{r}+\int_{s_0}^t|f(r,\phi(r,t_0,y_0))-f(r,\phi(r,s_0,y_0))|\dd{r}\\
            &\leq |t_0-s_0|M+L\int_{s_0}^t\Delta(r)\dd{r}
        \end{align*}
        Therefore, by Gr\"{o}nwall's inequality (as in Problem \ref{prb:2.12}; note that $\gamma=0$ here),
        \begin{equation*}
            \Delta(t) \leq |t_0-s_0|M\e[L|t-s_0|]
        \end{equation*}
        as desired.
    \end{proof}
    \item By Problem 1.8, we have $\phi(t,t_0,x_0)=\phi(t-t_0,0,x_0)$ for an autonomous system, so we may consider $\phi(t,x_0)=\phi(t,0,x_0)$.
    \item The previous result of \emph{continuous} dependence on initial conditions is not good enough in every situation; sometimes, we need to be able to \emph{differentiate} with respect to the initial condition.
    \item \marginnote{11/23:}Before we prove that a certain set of conditions will imply the existence of the derivative of a solution $\phi$ with respect to $x$, we will assume such a derivative exists and investigate some of its properties (so as to motivate the following theorem and proof).
    \begin{itemize}
        \item Suppose $\phi(t,t_0,x)$ is differentiable with respect to $x$.
        \item If we assume $f\in C^k(U,\R^n)$ for some $k\geq 1$ and $\phi$ is sufficiently regular that its partial derivatives commute as well, then we may write\footnote{Note that dot-denoted derivatives refer to derivatives with respect to $t$.}
        \begin{align*}
            \dot{\phi}(t,t_0,x) &= f(t,\phi(t,t_0,x))\\
            \pdv{\phi}{t} &= f(t,\phi)\\
            \pdv{\phi}{x}{t} &= \pdv{f}{\phi}\pdv{\phi}{x}\\
            \pdv{\phi}{t}{x} &= \pdv{f}{\phi}\pdv{\phi}{x}\\
            \pdv{t}\pdv{\phi}{x} &= \pdv{f}{\phi}\pdv{\phi}{x}
        \end{align*}
        \item Thus, if $\pdv*{\phi}{x}$ exists, then it satisfies the \textbf{first variational equation} with $A(t,x)=\pdv*{f}{\phi}$.
        \item It also necessarily satisfies the corresponding integral equation
        \begin{equation*}
            \underbrace{\vphantom{\pdv{f}{\phi}}{\pdv{\phi}{x}}(t,t_0,x)}_{y(t)} = \underbrace{\vphantom{\pdv{f}{\phi}}{\pdv{\phi}{x}}(t_0,t_0,x)}_\mathbb{I}+\int_{t_0}^t\underbrace{{\pdv{f}{\phi}}(s,\phi(s,t_0,x))}_{A(s,x)}\cdot\underbrace{\vphantom{\pdv{f}{\phi}}{\pdv{\phi}{x}}(s,t_0,x)}_{y(s)}\dd{s}
        \end{equation*}
    \end{itemize}
    \item \textbf{First variational equation}: The following differential equation. \emph{Given by}
    \begin{equation*}
        \dot{y} = A(t,x)y
    \end{equation*}
    \begin{itemize}
        \item Which is the correct first variational equation?? This one, or the one with $F(t;\mu)$ term?
    \end{itemize}
    \item Indeed, we have shown that $\pdv*{\phi}{x}$ necessarily satisfies the above integral equation if it exists. But using fixed point techniques, we can further show that said equation \emph{has} a solution (which is $\pdv*{\phi}{x}$) under certain conditions. We will not go into this argument in depth, but it will be used in our next theorem.
    \item Theorem 2.10: Suppose $f\in C^k(U,\R^n)$, $k\geq 1$. Around each point $(t_0,x_0)\in U$, we can find an open set $I\times B\subset U$ such that $\phi(t,s,x)\in C^k(I\times I\times B,\R^n)$. Moreover, ${\pdv*{\phi}{t}}(t,s,x)\in C^k(I\times I\times B,\R^n)$ and if $D_k$ is a partial derivative of order $k$, then $D_k\phi$ satisfies the higher order variational equation obtained from
    \begin{equation*}
        \pdv{t}D_k\phi(t,s,x) = D_k\pdv{t}\phi(t,s,x)
        = D_kf(t,\phi(t,s,x))
    \end{equation*}
    by applying the chain rule repeatedly. In particular, this equation is linear in $D_k\phi$ and it also follows that the corresponding higher-order derivatives commute.
    \begin{proof}
        % The average slope is equal to the beginning slope plus the average of the change. The second term and last term in the integral cancel.

        % Page 7.


        To prove the claim, we induct on $k$. For the base case $k=1$, we need only prove that $\phi(t,x)$ is \emph{differentiable} at every $x_1\in B$. We now define some terms that will be useful in our argument.\par
        Using the trick on \textcite[7]{bib:Teschl}, add $t$ to the dependent variables to make the ODE autonomous. This allows us to consider $\phi(t,x)=\phi(t,0,x)$ WLOG. Thus, we have by Theorem 2.9 that there exists a set $I\times B\subset U$ such that $\phi(t,x_0)\in C(I\times B,\R^n)$. We may take $I=(-T,T)$ and $B=B_\delta(x_0)$ for some $T,\delta>0$ such that $\overline{I\times B}\subset U$. Additionally, let $x_1=0$ WLOG. Furthermore, let $\phi(t):=\phi(t,x_1)$, $A(t):=A(t,x_1)$, and $\psi(t)$ denote the solution to the first variational equation $\dot{\psi}(t)=A(t)\psi(t)$ corresponding to the initial condition $\psi(t_0)=\mathbb{I}$ ($\psi(t)$ is guaranteed to exist by the aforementioned fixed point argument). Lastly, let
        \begin{equation*}
            \theta(t,x) := \frac{\phi(t,x)-\phi(t)-\psi(t)x}{|x|}
        \end{equation*}\par
        The proof strategy is thus: If we can show that $\lim_{x\to x_1=0}\theta(t,x)=0$, then we will have proven that $\pdv*{\phi}{x}$ exists and is equal to $\psi$. To do so, we will derive a bound on $|\theta(t,x)|$ that we can more easily control. Let's begin.\par
        Since $f\in C^1$, we have that
        \begin{align*}
            f(y)-f(x) &= \int_x^y{\pdv{f}{x}}(t)\dd{t}\\
            \frac{f(y)-f(x)}{y-x} &= \frac{1}{y-x}\int_x^y{\pdv{f}{x}}(t)\dd{t}\\
            &= \int_0^1{\pdv{f}{x}}(x+t(y-x))\dd{t}\\
            &= {\pdv{f}{x}}(x)+\int_0^1\left( {\pdv{f}{x}}(x+t(y-x))-{\pdv{f}{x}}(x) \right)\dd{t}\\
            f(y)-f(x) &= {\pdv{f}{x}}(x)(y-x)+|y-x|R(y,x)
        \end{align*}
        where
        \begin{equation*}
            |R(y-x)| \leq \max_{t\in[0,1]}\norm{{\pdv{f}{x}}(x+t(y-x))-{\pdv{f}{x}}(x)}
        \end{equation*}
        Let's take another minute to justify the above algebraic manipulations from a geometric perspective. The first line should be a fairly straightforward application of the FTC. The second line rephrases the equality as two different ways of calculating the average slope of $f$ on $[x,y]$; specifically, we may either do rise over run (LHS) or find the area under the derivative and divide by the "length" to get the average "height." The third line shrinks the domain of integration to an interval of unit length, compressing all of the information in $\pdv*{f}{x}$ along with it, and making it so that we no longer have to divide through by the "length." The fourth line adds and subtracts the starting point so that geometrically, we take the area in two parts: All of the area beneath the first point, and then all of the area that "changes." The last line sees us multiply both sides by $y-x$ and then take the integral to be a sort of "remainder." Alternatively, we may justify
        \begin{equation*}
            f(y) = f(x)+{\pdv{f}{x}}(x)(y-x)+\left( \int_0^1\left( {\pdv{f}{x}}(x+t(y-x))-{\pdv{f}{x}}(x) \right)\dd{t} \right)(y-x)
        \end{equation*}
        by analogy: If $f(x)=x^2$ and thus $\dv*{f}{x}=2x$, then $4^2$ equals $2^2$, plus the area under the curve $2x$ from $x=2$ to $x=4$ partitioned into the rectangle with base length $y-x=2$ and height ${\dv*{f}{x}}(2)=4$ and the triangle sitting on top of the aforementioned rectangle and below the graph of $\dv*{f}{x}$. Note that all of these justifications are taken from the perspective of $f$ being a single-variable function; to justify the transformations in the multivariable case would likely be much more complicated and is beyond my grasp at the moment (recall that $\pdv*{f}{x}$ is actually a matrix!). Also note that as such, the norm in the bound on $|R(y,x)|$ given above is the matrix norm.\par
        Since $f\in C^1$, its partial derivatives are uniformly continuous in a neighborhood of $x_1$. It follows that $\lim_{y\to x}|R(y,x)|=0$ where the argument converges uniformly in $x$ in some neighborhood of $x_1=0$.\par
        Using the above expression for $f(y)-f(x)$, we have that
        \begin{align*}
            \dot{\theta}(t,x) &= \frac{1}{|x|}[\dot{\phi}(t,x)-\dot{\phi}(t)-\dot{\psi}(t)x]\\
            &= \frac{1}{|x|}[f(\phi(t,x))-f(\phi(t))-A(t)\psi(t)x]\\
            &= \frac{1}{|x|}\left[ {\pdv{f}{x}}(\phi(t))(\phi(t,x)-\phi(t))+|\phi(t,x)-\phi(t)|R(\phi(t,x),\phi(t))-A(t)\psi(t)x \right]\\
            &= A(t)\cdot\frac{\phi(t,x)-\phi(t)-\psi(t)x}{|x|}+\frac{|\phi(t,x)-\phi(t)|}{|x|}R(\phi(t,x),\phi(t))\\
            &= A(t)\theta(t,x)+\frac{|\phi(t,x)-\phi(t)|}{|x|}R(\phi(t,x),\phi(t))
        \end{align*}
        Integrating and taking absolute values yields
        \begin{align*}
            |\theta(t,x)| &= |\theta(t,x)-\theta(0,x)|\\
            &= \left| \int_0^t\left( A(s)\theta(s,x)+\frac{|\phi(s,x)-\phi(s)|}{|x|}R(\phi(s,x),\phi(s)) \right)\dd{s} \right|\\
            &\leq \int_0^t\frac{1}{|x|}|\phi(s,x)-\phi(s,0)|\cdot|R(\phi(s,x),\phi(s))|\dd{s}+\int_0^t\norm{A(s)}|\theta(s,x)|\dd{s}\\
            &\leq \int_0^t\frac{1}{|x|}|x-0|\e[L|s-0|]\cdot|R(\phi(s,x),\phi(s))|\dd{s}+\int_0^t\norm{A(s)}|\theta(s,x)|\dd{s}\\
            &\leq \e[LT]\int_0^T|R(\phi(s,x),\phi(s))|\dd{s}+\int_0^t\norm{A(s)}|\theta(s,x)|\dd{s}
        \end{align*}
        Note that $\theta(0,x)=0$ since we have taken $t_0=x_1=0$ WLOG. Also note that we use the continuous dependence on initial conditions equation to get from line 2 to line 3. Lastly, note that from the next to the last to the last line, we use the inequality $\e[Ls]\leq\e[LT]$ to transform the exponential function into a constant that bounds it (recall that $s\leq T$ by definition).\par
        Define
        \begin{equation*}
            \tilde{R}(x) = \e[LT]\int_0^T|R(\phi(s,x),\phi(s))|\dd{s}
        \end{equation*}
        so that
        \begin{equation*}
            |\theta(t,x)| \leq \tilde{R}(x)+\int_0^t\norm{A(s)}|\theta(s,x)|\dd{s}
        \end{equation*}
        It follows by the Generalized Gr\"{o}nwall's inequality that
        \begin{equation*}
            |\theta(t,x)| \leq \tilde{R}(x)\exp(\int_0^T\norm{A(s)}\dd{s})
        \end{equation*}
        This combined with the fact from earlier that $\lim_{y\to x}|R(y,x)|=0$ and hence $\lim_{x\to 0}\tilde{R}(x)=0$ implies that $\lim_{x\to 0}\theta(t,x)=0$, as desired.\par
        Additionally, $\pdv*{\phi}{x}$ is continuous as the solution to the first variational equation. This completes the base case.\par\smallskip
        Now suppose via strong induction that the claim holds for $1,\dots,k$, and let $f\in C^{k+1}$. Then $\phi(t,x)\in C^1$ and $\pdv*{\phi}{x}$ solves the first variational equation, as per the base case. But since $A(t,x)\in C^k$ and hence $\pdv*{\phi}{x}\in C^k$, we have by Lemma 2.3 that $\phi(t,x)\in C^{k+1}$.
    \end{proof}
    \item This theorem also allows us to handle dependence on parameters.
    \begin{itemize}
        \item In particular, if $f$ depends on some paramenters $\lambda\in\Lambda\subset\R^p$ such that we are now solving the IVP
        \begin{equation*}
            \dot{x}(t) = f(t,x,\lambda)
            ,\quad
            x(t_0) = x_0
        \end{equation*}
        for a solution $\phi(t,t_0,x_0,\lambda)$, then we have the following result.
    \end{itemize}
    \item Theorem 2.11: Suppose $f\in C^k(U\times\Lambda,\R^n)$, $k\geq 1$. Around each point $(t_0,x_0,\lambda_0)\in U\times\Lambda$ we can find an open set $I\times B\times\Lambda_0\subset U\times\Lambda$ such that $\phi(t,s,x,\lambda)\in C^k(I\times I\times B\times\Lambda_0,\R^n)$.
    \begin{proof}
        Largely follows from Theorem 2.10. Noteworthy modificatinos: We add the parameters $\lambda$ to the dependent variables and require $\dot{\lambda}=0$ (i.e., that the parameters do not change with time and therefore uniquely determine a solution over time).
    \end{proof}
\end{itemize}

\subsubsection*{Problems}
\begin{enumerate}[label={\textbf{2.\arabic*.}},ref={2.\arabic*},leftmargin=3.5em]
    \setcounter{enumi}{11}
    \item \label{prb:2.12}\marginnote{11/15:}Show that if
    \begin{equation*}
        \psi(t) \leq \alpha+\int_0^t(\beta\psi(s)+\gamma)\dd{s}
    \end{equation*}
    for all $t\in[0,T]$, where $\alpha,\gamma\in\R$ and $\beta\geq 0$, then
    \begin{equation*}
        \psi(t) \leq \alpha\e[\beta t]+\frac{\gamma}{\beta}(\e[\beta t]-1)
    \end{equation*}
    for all $t\in[0,T]$. \emph{Hint}: Introduce $\tilde{\psi}(t)=\psi(t)+\gamma/\beta$.
    \begin{proof}
        Taking the hint and substituting $\psi(t)=\tilde{\psi}(t)-\gamma/\beta$, we get
        \begin{align*}
            \tilde{\psi}(t)-\frac{\gamma}{\beta} &\leq \alpha+\int_0^t\left( \beta\left( \tilde{\psi}(t)-\frac{\gamma}{\beta} \right)+\gamma \right)\dd{s}\\
            \tilde{\psi}(t) &\leq \alpha+\frac{\gamma}{\beta}+\int_0^t\beta\tilde{\psi}(t)\dd{s}
        \end{align*}
        It follows by the generalized Gr\"{o}nwall's inequality that
        \begin{align*}
            \tilde{\psi}(t) &\leq \alpha+\frac{\gamma}{\beta}+\int_0^t\left( \alpha+\frac{\gamma}{\beta} \right)\beta\exp(\int_s^t\beta\dd{r})\dd{s}\\
            &= \alpha+\frac{\gamma}{\beta}+\int_0^t(\alpha\beta+\gamma)\e[\beta(t-s)]\dd{s}\\
            &= \alpha+\frac{\gamma}{\beta}+(\alpha\beta+\gamma)\e[\beta t]\int_0^t\e[-\beta s]\dd{s}\\
            &= \alpha+\frac{\gamma}{\beta}+(\alpha\beta+\gamma)\e[\beta t]\left( \frac{1}{-\beta}\e[-\beta t]-\frac{1}{-\beta}\e[0] \right)\\
            &= \alpha+\frac{\gamma}{\beta}+\left( \alpha+\frac{\gamma}{\beta} \right)\e[\beta t](1-\e[-\beta t])\\
            &= \alpha+\frac{\gamma}{\beta}+\left( \alpha+\frac{\gamma}{\beta} \right)(\e[\beta t]-1)\\
            &= \alpha+\frac{\gamma}{\beta}+\alpha\e[\beta t]-\alpha+\frac{\gamma}{\beta}(\e[\beta t]-1)\\
            &= \frac{\gamma}{\beta}+\alpha\e[\beta t]+\frac{\gamma}{\beta}(\e[\beta t]-1)
        \end{align*}
        Subtracting $\gamma/\beta$ from both sides and returning the substitution yields the desired result.
    \end{proof}
\end{enumerate}


\subsection*{Section 2.5: Regular Perturbation Theory}
\begin{itemize}
    \item \marginnote{12/6:}Goal of this section: Justify the perturbation method proposed in Problem \ref{prb:1.2} using Theorem 2.11.
    \item \textbf{Regular perturbation problem}: An IVP of the following form. \emph{Given by}
    \begin{equation*}
        \dot{x} = f(t,x,\varepsilon)
        ,\quad
        x(t_0) = x_0
    \end{equation*}
    \item Let's try to solve such a problem in the general case first.
    \begin{itemize}
        \item Theorem 2.11: $f\in C^1$ implies $\phi(t,\varepsilon)=\phi(t,\varepsilon,t_0,x_0)\in C^1$.
        \item If $\phi\in C^1$, then we have the following Taylor expansion.
        \begin{equation*}
            \phi(t,\varepsilon) = \phi_0(t)+\phi_1(t)\varepsilon+o(\varepsilon)
        \end{equation*}
        \item If $\varepsilon=0$, $\phi(t,\varepsilon)=\phi_0(t)$. Thus, $\phi_0(t)$ is the solution to the unperturbed equation
        \begin{equation*}
            \dot{\phi}_0 = f_0(t,\phi_0)
            ,\quad
            \phi_0(t_0) = x_0
        \end{equation*}
        where $f_0(t,x)=f(t,x,0)$.
        \item Additionally, we can deduce from the Taylor series expansion that
        \begin{align*}
            \phi &= \phi_0+\phi_1\varepsilon\\
            \pdv{\phi}{\varepsilon} &= \phi_1
        \end{align*}
        \item Thus, we have from the original ODE a new kind of first variational equation satisfied by $\phi_1=\pdv*{\phi}{\varepsilon}$, which we can use to solve for said quantity.
        \begin{align*}
            \pdv{\phi}{t} &= f(t,\phi(t,\varepsilon),\varepsilon)\\
            \pdv{\phi}{\varepsilon}{t} &= \pdv{f}{t}\pdv{t}{\varepsilon}+\pdv{f}{x}\pdv{\phi}{\varepsilon}+\pdv{f}{\varepsilon}\pdv{\varepsilon}{\varepsilon}\\
            \pdv{t}\pdv{\phi}{\varepsilon} &= \pdv{f}{x}\pdv{\phi}{\varepsilon}+\pdv{f}{\varepsilon}\\
            \pdv{t}\underbrace{\eval{\pdv{\varepsilon}\phi(t,\varepsilon)}_{\varepsilon=0}}_{\phi_1} &= \underbrace{\eval{\pdv{x}f(t,\phi_0(t),0)}_{\varepsilon=0}}_{f_{10}(t,\phi_0(t))}\phi_1+\underbrace{\eval{\pdv{\varepsilon}f(t,\phi_0(t),\varepsilon)}_{\varepsilon=0}}_{f_{11}(t,\phi_0(t))}
        \end{align*}
        Note that the corresponding initial condition is
        \begin{align*}
            x(t_0) &= x_0\\
            \phi(t_0,\varepsilon) &= x_0\\
            \eval{\pdv{\varepsilon}\phi(t_0,\varepsilon)}_{\varepsilon=0} &= 0\\
            \phi_1(t_0) &= 0
        \end{align*}
        \item "Hence, once we have the solution of the unperturbed problem $\phi_0(t)$, we can then compute the correction term $\phi_1(t)$ by solving another linear equation" \parencite[49]{bib:Teschl}.
        \item Therefore, the "plug in the ansatz and compare coefficients" procedure is justified.
    \end{itemize}
    \item Example: Consider the equation
    \begin{equation*}
        \dot{v} = -\varepsilon v-g
        ,\quad
        v(0) = 0
        ,\quad
        \varepsilon \geq 0
    \end{equation*}
    which models the velocity of a falling object with air resistance.
    \begin{itemize}
        \item We can calculate the explicit solution and use it to check our answer.
        \begin{equation*}
            \phi(t,\varepsilon) = \frac{g}{\varepsilon}(\e[-\varepsilon t]-1)
        \end{equation*}
        \item Applying perturbation techniques, we start with the unperturbed problem
        \begin{equation*}
            \dot{\phi}_0 = -g
            ,\quad
            \phi_0(0) = 0
        \end{equation*}
        and solve to get
        \begin{equation*}
            \phi_0(t) = -gt
        \end{equation*}
        \item Now we use the first variational equation. We have
        \begin{align*}
            f_{10}(t,\phi_0(t)) &= \eval{\pdv{x}f(t,\phi_0(t),0)}_{\varepsilon=0}&
                f_{11}(t,\phi_0(t)) &= \eval{\pdv{\varepsilon}f(t,\phi_0(t),\varepsilon)}_{\varepsilon=0}\\
            &= \eval{\pdv{x}(-0\cdot\phi_0(t)-g)}_{\varepsilon=0}&
                &= \eval{\pdv{\varepsilon}(-\varepsilon\phi_0(t)-g)}_{\varepsilon=0}\\
            &= \eval{\pdv{x}(-g)}_{\varepsilon=0}&
                &= -\phi_0(t)|_{\varepsilon=0}\\
            &= 0|_{\varepsilon=0}&
                &= gt|_{\varepsilon=0}\\
            &= 0&
                &= gt
        \end{align*}
        so that
        \begin{align*}
            \dot{\phi}_1 &= 0\cdot\phi_1+gt\\
            \dot{\phi}_1 &= gt
            ,\quad
            \phi_1(0) = 0
        \end{align*}
        which we can solve to get
        \begin{equation*}
            \phi_1(t) = \frac{g}{2}t^2
        \end{equation*}
        \item Therefore, our overall approximation is
        \begin{equation*}
            v(t) = -g\left( t-\varepsilon\cdot\frac{t^2}{2}+o(\varepsilon) \right)
        \end{equation*}
        which does indeed correspond to the Taylor expansion of the exact solution.
        \item Note that the approximation is only valid for fixed time and will get worse as $t$ increases; indeed, the appoximation diverges in the long run while the actual solution converges to $g/\varepsilon$.
    \end{itemize}
    \item Extending this procedure.
    \item Theorem 2.12: Let $\Lambda$ be some open interval. Suppose $f\in C^k(U\times\Lambda,\R^n)$, $k\geq 1$ and fix some values $(t_0,x_0,\varepsilon_0)\in U\times\Lambda$. Let $\phi(t,\varepsilon)\in C^k(I\times\Lambda_0,\R^n)$ be the solution of the initial value problem
    \begin{equation*}
        \dot{x} = f(t,x,\varepsilon)
        ,\quad
        x(t_0) = x_0
    \end{equation*}
    guaranteed to exist by Theorem 2.11. Then
    \begin{equation*}
        \phi(t,\varepsilon) = \sum_{j=0}^k\frac{\phi_j(t)}{j!}(\varepsilon-\varepsilon_0)^j+o((\varepsilon-\varepsilon_0)^k)
    \end{equation*}
    where the coefficients can be obtained by recursively solving
    \begin{equation*}
        \dot{\phi}_j = f_j(t,\phi_0,\dots,\phi_j,\varepsilon_0)
        ,\quad
        \phi_j(t_0) =
        \begin{cases}
            x_0 & j = 0\\
            0 & j\geq 1
        \end{cases}
    \end{equation*}
    where the function $f_j$ is recursively defined via
    \begin{equation*}
        f_{j+1}(t,x_0,\dots,x_{j+1},\varepsilon) = {\pdv{f_j}{\varepsilon}}(t,x_0,\dots,x_j,\varepsilon)+\sum_{k=0}^j{\pdv{f_j}{x_k}}(t,x_0,\dots,x_j,\varepsilon)x_{k+1}
        ,\quad
        f_0(t,x_0,\varepsilon) = f(t,x_0,\varepsilon)
    \end{equation*}
    If we assume $f\in C^{k+1}$, the error term will be $O((\varepsilon-\varepsilon_0)^{k+1})$ uniformly for $t\in I$.
    \begin{proof}
        We plug the power series definition of $\phi(t,\varepsilon)$ given above into the differential equation and compare powers of $\varepsilon$.\par
        Estimating the remainder in the Taylor expansion: Since $f\in C^{k+1}$, we know that $\pdv*[k+1]{\phi}{\varepsilon}$ is continuous and hence bounded on $I\times\Lambda_0$, as desired.
    \end{proof}
    \item If we're willing to deal with the Taylor series in more than one variable, we can definitely admit more than one parameter.
    \item Include the case where the initial condition depends on $\varepsilon$ by simply replacing the initial conditions for $\phi_j(t_0)$ by the corresponding expansion coefficients of $x_0(\varepsilon)$.
    \item The Taylor series will converge if $f$ is analytic with respect to all variables (see Theorem 4.2).
\end{itemize}

\subsubsection*{Problems}
\begin{enumerate}[label={\textbf{2.\arabic*.}},ref={2.\arabic*},leftmargin=3.5em]
    \setcounter{enumi}{15}
    \item \label{prb:2.16}Compute the next term $\phi_2$ in the above example.
\end{enumerate}




\end{document}