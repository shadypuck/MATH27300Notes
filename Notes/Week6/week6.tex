\documentclass[../notes.tex]{subfiles}

\pagestyle{main}
\renewcommand{\chaptermark}[1]{\markboth{\chaptername\ \thechapter\ (#1)}{}}
\setcounter{chapter}{5}

\begin{document}




\chapter{Qualitative Theory of ODEs}
\section{More Cauchy-Lipschitz and Intro to Continuous Dependence}
\begin{itemize}
    \item \marginnote{10/31:}Last time, we built up a proof to the Cauchy-Lipschitz theorem intuitively.
    \begin{itemize}
        \item We begin today with a direct proof that is very similar, but slightly different.
    \end{itemize}
    \item Theorem (Cauchy-Lipschitz theorem): Let $f(t,z)$ be defined on an open subset $\Omega\subset\R\times\R^n$, let $(t_0,y_0)\in\Omega$, let $|f|$ be bounded on $\Omega$, and let $f$ be Lipschitz continuous in $z$ and continuous wrt. $t$ in some neighborhood of $(t_0,y_0)$. Then the IVP $y'(t)=f(t,y(t))$, $y(t_0)=y_0$ has a unique solution on $[t_0,t_0+T]$ for some $T>0$ such that $y(t)$ does not escape $\Omega$.
    \begin{proof}
        Let $f(t,z)$ be defined for $(t,z)\in[t_0,t_0+a]\times\bar{B}(y_0,b)\subset\Omega$. Let $|f(t,z)|\leq M$. Let $|f(t,z_1)-f(t,z_2)|\leq L|z_1-z_2|$ for all $z_1,z_2\in\bar{B}(y_0,b)$.\par
        Define $\{y_n\}$ recursively, starting from $y_0(t)=y_0$, by
        \begin{equation*}
            y_{k+1}(t) = y_0+\int_{t_0}^tf(\tau,y_k(\tau))\dd\tau
        \end{equation*}
        Since $f$ is continuous with respect to $t$, it is integrable with respect to $t$, so the above sequence is well-defined on $[t_0,t_0+T]$. Choose $T=\min(a,b/M,1/2L)$. Then
        \begin{equation*}
            \norm{y_k-y_0} \leq T\cdot M
            \leq \frac{b}{M}\cdot M
            = b
        \end{equation*}
        so no $y_k$ escapes $\bar{B}(y_0,b)$. Additionally,
        \begin{align*}
            \norm{y_{k+1}-y_k} &\leq \int_{t_0}^t\norm{f(\tau,y_k(\tau))-f(\tau,y_{k-1}(\tau))}\dd\tau\\
            &\leq TL\norm{y_k-y_{k-1}}\\
            &\leq \frac{1}{2}\norm{y_k-y_{k-1}}\\
            &\leq \left( \frac{1}{2} \right)^k\norm{y_1-y_0}
        \end{align*}
        Thus, the difference between successive terms in the sequence is controlled by a geometric progression, so $\{y_n\}$ is a Cauchy sequence in the function space. It follows that $\{y_k\}$ is uniformly convergent to some continuous $y:[t_0,t_0+T]\to\R^n$.
    \end{proof}
    % \item Consider the IVP $y'=f(t,y)$, $y(t_0)=y_0$. Let $f(t,z)$ be defined for $(t,z)\in[t_0,t_0+a]\times\bar{B}(y_0,b)$, let $|f(t,z)|\leq M$, and let $f$ be $L$ - Lipschitz wrt. $z$, i.e., $|f(t,z_1)-f(t,z_2)|\leq L|z_1-z_2|$ for all $z_1,z_2\in\bar{B}(y_0,b)$.
    % \item Form an iterative sequence $\{y_n\}$ starting from $y_0(t)=y_0$, recursively defined by
    % \begin{equation*}
    %     y_{k+1}(t) = y_0+\int_{t_0}^tf(\tau,y_k(\tau))\dd\tau
    % \end{equation*}
    % \begin{itemize}
    %     \item This sequence is well defined on $[t_0,t_0+T]$.
    %     \item Choose $T=\min(a,b/M,1/2L)$.
    % \end{itemize}
    % \item We have that
    % \begin{equation*}
    %     \norm{y_{k+1}-y_0} \leq T\cdot M \leq b
    % \end{equation*}
    % \begin{itemize}
    %     \item This implies that $\{y_k(t)\}$ does not escape $\bar{B}(y_0,b)$.
    % \end{itemize}
    % \item We have
    % \begin{equation*}
    %     y_{k+1}(t)-y_k(t) = \int_{t_0}^t[f(\tau,y_k(\tau))-f(\tau,y_{k-1}(\tau))]\dd\tau
    % \end{equation*}
    % \begin{itemize}
    %     \item Here we can apply the Lipschitz condition.
    % \end{itemize}
    % \item Taking $\sup_{t\in[t_0,t_0+T]}$, we get
    % \begin{equation*}
    %     \norm{y_{k+1}-y_k} \leq T\sup_{\tau\in[t_0,t_0+T]}|f(\tau,y_k(\tau))-f(\tau,y_{k-1}(\tau))| \leq TL\norm{y_k-y_{k-1}}
    % \end{equation*}
    % where we have applied the Lipschitz condition in the second step.
    % \item By our choice of $T$, $TL\leq 1/2$.
    % \begin{itemize}
    %     \item Thus, the difference between successive terms in the sequence is controlled by a geometric progression.
    %     \item As a result, $\{y_k\}$ is a Cauchy sequence in the function space. In other words, $y_k$ is uniformly convergent to some continuous $y(t)$.
    %     \item For uniformly convergent functions, we can always exchange the limit and the integral.
    % \end{itemize}
    \item This completes the proof. Although it's more concrete than the contraction mapping one, they are virtually the same: In both cases, we obtain an approximate sequence controlled by a geometric progression.
    \item Examples of the Picard iteration:
    \begin{enumerate}
        \item Consider an linear autonomous systems $y'=Ay$, $A$ an $n\times n$ matrix, and $y(0)=y_0$.
        \begin{itemize}
            \item We know that the solution is $y(t)=\e[tA]y_0$. However, we can derive this using the Picard iteration.
            \item Indeed, via this procedure, let's determine the first couple of Picard iterates.
            \begin{align*}
                y_0(t) &= y_0&
                    y_1(t) &= y_0+\int_0^tAy_0(\tau)\dd\tau&
                        y_2(t) &= y_0+\int_0^tAy_1(\tau)\dd\tau\\
                &&
                    &= y_0+tAy_0&
                        &= y_0+tAy_0+\frac{1}{2}t^2A^2y_0
            \end{align*}
            \item It follows inductively that
            \begin{equation*}
                y_k(t) = \sum_{j=0}^k\frac{t^jA^j}{j!}y_0
            \end{equation*}
            \item Since the term above is exactly the power series definition of $\e[tA]$, we have that $y_k(t)\to\e[tA]y_0$ with local uniformity in $t$, as desired.
        \end{itemize}
        \item Consider the ODE $y'=y^2$, $y(0)=1$.
        \begin{itemize}
            \item We know that the solution is $y(t)=1/(1-t)$. We will now also derive this via the Picard iteration.
            \item Choose $b=1$, so that
            \begin{equation*}
                \bar{B}(y_0,b) = \{y\mid |y-y(0)|\leq 1\}
                = \{y\mid |y-1|\leq 1\}
                = [0,2]
            \end{equation*}
            \item On this interval, $f(t,y)=y^2$ has maximum slope $L=4$. Thus, we should take $T\leq 1/2L=1/8$.
            \item It follows that $|y_1^2-y_2^2|\leq 4|y_1-y_2|$ for all $y_1,y_2\in\bar{B}(y_0,b)$.
            \item Calculate the first few Picard iterates.
            \begin{gather*}
                y_1(t) = 1+\int_0^t(y_0(\tau))^2\dd\tau
                    = 1+t\\
                y_2(t) = 1+\int_0^t(1+\tau)^2\dd\tau
                    = 1+t+t^2+\frac{t^3}{3}\\
                y_3(t) = 1+\int_0^t\left( 1+\tau+\tau^2+\frac{\tau^3}{3} \right)^2\dd\tau
                    = 1+t+t^2+t^3+\frac{2t^4}{3}+\frac{t^5}{3}+\frac{t^6}{9}+\frac{t^7}{63}
            \end{gather*}
            \item It follows by induction that
            \begin{align*}
                |y_k(t)-(1+t+\cdots+t^k)| &\leq t^{k+1}\\
                \left| y_k(t)-\frac{1-t^{k+1}}{1-t} \right| &\leq t^{k+1}
            \end{align*}
            It follows that $|t|<1/8$.
            \item For $|t|<1/8$, $y(t)=1/(1-t)$. Blows up as $t\to 1$.
            \item Some more details on the bounding of the error term are presented in the lecture notes document.
        \end{itemize}
    \end{enumerate}
    \item Lemma (Gr\"{o}nwall's inequality): Let $\varphi(t)$ be a real function defined for $t\in[t_0,t_0+T]$ such that
    \begin{equation*}
        \varphi(t) \leq f(t)+a\int_{t_0}^t\varphi(\tau)\dd\tau
    \end{equation*}
    Then
    \begin{equation*}
        \varphi(t) \leq f(t)+a\int_{t_0}^t\e[a(t-\tau)]f(\tau)\dd\tau
    \end{equation*}
    \begin{proof}
        Multiply both sides by $\e[-at]$:
        \begin{align*}
            \e[-at]\varphi(t)-a\e[-at]\int_{t_0}^t\varphi(\tau)\dd\tau &\leq \e[-at]f(t)\\
            \dv{t}(\e[-at]\int_{t_0}^t\varphi(\tau)\dd\tau) &\leq \e[-at]f(t)\\
            \e[-at]\int_{t_0}^t\varphi(\tau)\dd\tau &\leq \int_{t_0}^t\e[-a\tau]f(\tau)\dd\tau\\
            \int_{t_0}^t\varphi(\tau)\dd\tau &\leq \int_{t_0}^t\e[a(t-\tau)]f(\tau)\dd\tau
        \end{align*}
        Substituting back into the original equality yields the result at this point.
    \end{proof}
    \item Note that there is no sign condition on $f(t)$ or $a$.
    \item Gr\"{o}nwall's inequality is very important and we should remember it.
    \item It is also exactly what we need to prove continuous dependence.
    \item Theorem: Let $f(t,z),g(t,z)$ be defined on $\Omega\subset\R_t^1\times\R_z^n$, an open and bounded a region containing $(t_0,y_0)$ and $(t_0,w_0)$. Let the functions be $L$ - Lipschitz wrt. $z$. Consider two initial value problems $y'=f(t,y)$, $y(t_0)=y_0$ and $w'=g(t,w)$, $w(t_0)=w_0$. If $|f(t,z)-g(t,z)|<M$, then for $t\in[t_0,t_0+T]$,
    \begin{equation*}
        |y(t)-w(t)| \leq \e[LT]|y_0-w_0|+\frac{M}{L}(\e[LT]-1)
    \end{equation*}
    \begin{proof}
        We have that
        \begin{align*}
            |y(t)-w(t)| &= \left| \left[ y_0+\int_{t_0}^tf(\tau,y(\tau))\dd\tau \right]-\left[ w_0+\int_{t_0}^tg(\tau,y(\tau))\dd\tau \right] \right|\\
            &= \left| [y_0-w_0]+\int_{t_0}^t[f(\tau,y(\tau))-g(\tau,y(\tau))]\dd\tau \right|\\
            &\leq |y_0-w_0|+\left| \int_{t_0}^t[f(\tau,y(\tau))-g(\tau,w(\tau))]\dd\tau \right|\\
            &\leq |y_0-w_0|+\int_{t_0}^t|f(\tau,y(\tau))-g(\tau,w(\tau))|\dd\tau
        \end{align*}
        where we get from the second to the third line using the triangle inequality, and the third to the fourth line using Theorem 13.26 of Honors Calculus IBL. We also know that
        \begin{align*}
            |f(\tau,y(\tau))-g(\tau,w(\tau))| &\leq |f(\tau,y(\tau))-f(\tau,w(\tau))|+|f(\tau,w(\tau))-g(\tau,w(\tau))|\\
            &\leq L|y(\tau)-w(\tau)|+M
        \end{align*}
        Combining what we've obtained, we have
        \begin{align*}
            \underbrace{|y(t)-w(t)|}_{\psi(t)} &\leq \underbrace{|y_0-w_0|+M(t-t_0)}_{f(t)}+\underbrace{\vphantom{|}L}_a\int_{t_0}^t\underbrace{|y(\tau)-w(\tau)|}_{\psi(t)}\dd\tau\\
            &\leq MT+|y_0-w_0|+L\int_{t_0}^t\e[L(t-\tau)][|y_0-w_0|+M(t-\tau)]\dd\tau\tag*{Gr\"{o}nwall}\\
            &\leq \e[LT]|y_0-w_0|+\frac{M}{L}(\e[TL]-1)
        \end{align*}
        as desired.
    \end{proof}
    \item Note: Getting from directly from Gr\"{o}nwall's inequality in the second line above to the last line above is quite messy. A consequence of Gr\"{o}nwall's inequality explored in the book makes this much easier. \emph{Prove Equation 2.38 via Problem 2.12.}
    \item Implication: The IVP is not just solvable itself, but is solvable wrt. perturbation of the initial conditions and RHS within a small, finite interval in time.
    \item Suppose $y'=0$, $y(0)=1$ and $w'=\varepsilon w$, $w(0)=1$. Then $y(t)=1$ and $w(t)=\e[\varepsilon t]$ and solutions are only close when $t$ is small.
    \begin{itemize}
        \item $t\leq 1/\varepsilon$??
    \end{itemize}
    \item This is important in physics. In most physical scenarios, the RHS is $C^1$. This is called determinism.
\end{itemize}



\section{Differentiability With Respect To Parameters}
\begin{itemize}
    \item \marginnote{11/2:}Review: Implicit Function Theorem.
    \begin{itemize}
        \item Gives you a sufficient condition for which an implicit relation defines a function.
        \item Does not give you the function, but tells you that it must exist and that it is unique.
    \end{itemize}
    \item Theorem (Implicit Function Theorem): Let $F:\R^n\times\R^m\to\R^m$ be $C^k$ in some neighborhood of $(x_0,y_0)\in\R^n\times\R^m$ a point satisfying $F(x_0,y_0)=0$. If the truncated Jacobian matrix ${\pdv{F}{y}}(x_0,y_0)$, which is $m\times m$, is invertible, then there is a neighborhood $U$ of $x_0$ such that there is a unique function $f:U\to\R^m$ with $y_0=f(x_0)$ and $F(x,f(x))=0$ and
    \begin{equation*}
        f'(x) = -\left( {\pdv{F}{y}}(x,y) \right)^{-1}\cdot{\pdv{F}{x}}(x,f(x))
    \end{equation*}
    \begin{itemize}
        \item The proof is based on the Banach fixed point theorem (this may be false?? I think Shao is confusing the proof of this theorem with the proof of the Inverse Function Theorem).
        \item The motivation for the last equality (the line above) is that if $F(x,f(x))=0$, then by the chain rule for partial derivatives,
        \begin{align*}
            0 &= \dv{x}(F(x,f(x)))\\
            &= {\pdv{F}{x}}(x,f(x))\cdot\dv{x}{x}+\left[ {\pdv{F}{y}}(x,y) \right]\cdot\dv{f}{x}\\
            &= {\pdv{F}{x}}(x,f(x))+\left[ {\pdv{F}{y}}(x,y) \right]\cdot f'(x)\\
            f'(x) &= -\left( {\pdv{F}{y}}(x,y) \right)^{-1}\cdot{\pdv{F}{x}}(x,f(x))
        \end{align*}
        \item Recall that we know that the matrix bracketed in line 2 is invertible by hypothesis.
        \item Additionally, since $\pdv*{F}{x}=A$ is $n\times m$ and $\pdv*{F}{y}=B$ is $m\times m$, $f'=-A^{-1}B$ is $n\times m$, as it should be for a function $f:\R^n\to\R^m$.
    \end{itemize}
    \item Consider the IVP
    \begin{equation*}
        y' = f(t,y;\mu)
        ,\quad
        y(t_0) = x(\mu)
    \end{equation*}
    \begin{itemize}
        \item This ODE and its initial condition both depend on a parameter $\mu\in B(0,r)\subset\R^m$ (usually we take $m=1$ so $\mu$ is just real).
        \item We denote the solution by $y(t;\mu)$.
        \item Suppose $|x(\mu)|<C$ for $\mu\in B(0,r)$ and $x(\mu)\in C^1$. Suppose the RHS $f(t,z;\mu)$ of the ODE is defined on $[t_0,t_0+a]\times\bar{B}(x(0),b+C)\times B(0,r)$, is $C^1$ in all variables, is bounded by $M$ on its domain, and is $L$-Lipschitz in $z$.
    \end{itemize}
    \item By Cauchy-Lipschitz, for small
    \begin{equation*}
        T \leq \min\left( a,\frac{b}{M},\frac{1}{2L} \right)
    \end{equation*}
    and $\mu\in B(0,r)$ ($r$ small), the solution \emph{exists} on $[t_0,t_0+T]$ and its value does not escape $\bar{B}(x(0),b+C)$.
    \begin{itemize}
        \item We now aim to show that the solution is \emph{differentiable} wrt. $\mu$ on this interval.
    \end{itemize}
    \item If $y(t;\mu)$ satisfies $y'(t;\mu)=f(t,y(t;\mu);\mu)$ and if the Jacobian matrix $J=\pdv*{y}{\mu}$ exists, then $J$ satisfies the \textbf{first variation equation}.
    \item \textbf{First variation equation}: The following linear differential equation. \emph{Given by}
    \begin{equation*}
        \dv{t}~\underbrace{{\pdv{y}{\mu}}(t;\mu)}_{J(t;\mu)} = \underbrace{{\pdv{f}{z}}(t,y(t;\mu);\mu)}_{A(t;\mu)}\cdot\underbrace{{\pdv{y}{\mu}}(t;\mu)}_{J(t;\mu)}+\pdv{f}{\mu}~(t,y(t;\mu);\mu)
        ,\quad
        \pdv{y}{\mu}~(t_0,\mu) = \pdv{x}{\mu}~(\mu)
    \end{equation*}
    \item The first variation equation has a unique solution, but we do not yet know that $y(t;\mu)$ is even differentiable with respect to $\mu$. We presently verify this claim.
    \item Theorem\footnote{See the proof from the book, transcribed below.}: $y(t;\mu)$ is $C^1$ in $\mu$ and ${\pdv*{y}{\mu}}(t;\mu)$ satisfies the first variation equation.
    \begin{proof}
        Let $\Theta(t;\mu)=y(t;\mu+h)-y(t;\mu)-J(t;\mu)h$ for $h$ small. Aim, show that $\Theta(t;\mu)=o(h)$ as $h\to 0$.\par
        We compute
        \begin{align*}
            \dv{t}\Theta(t;\mu) &= y'(t;\mu+h)-y'(t;\mu)-J'(t;\mu)h\\
            &= \underbrace{f(t,y(t;\mu+h);\mu+h)-f(t,y(t;\mu);\mu)}_I-\underbrace{{\pdv{f}{z}}(t,y(t;\mu);\mu)J(t;\mu)+{\pdv{f}{\mu}}(t,y(t;\mu);\mu)}_{II}
        \end{align*}
        $I$ denotes the first term; $II$ denotes the second term.\par
        We have that
        \begin{align*}
            I &= {\pdv{f}{z}}(t,y(t;\mu);\mu)[y(t;\mu+h)-y(t;\mu)]+{\pdv{f}{\mu}}(t,y(t;\mu);\mu)h+\underbrace{R(t;\mu,h)}_{o(h)}
        \end{align*}
        \emph{color coding}
        \begin{align*}
            I-II &= \underbrace{\text{green}-\text{blue}}_{\Theta(t;\mu)}+R(t;\mu,h)\\
            &= \dv{t}\Theta(t;\mu) = \Theta(t;\mu)+\underbrace{R(t;\mu,h)}_{o(h)}\\
            \Theta(t_0;\mu) &= o(h)\\
            |\Theta(t;\mu)| &\leq C\int_{t_0}^t|R(\tau;\mu,h)|\dd\tau\tag*{Gr\"{o}nwall}\\
            &= o(h)
        \end{align*}
        circle terms cancel.
    \end{proof}
    \item Example: First order derivatives must satisfy the first variational equation
    \begin{equation*}
        \dv{t}~{\pdv{y}{\mu}}(t;\mu) = {\pdv{f}{z}}(t,y(t;\mu);\mu)\cdot{\pdv{y}{\mu}}(t;\mu)
    \end{equation*}
    and the second order derivative must satisfy the second variational equation
    \begin{equation*}
        \dv{t}~\pdv[2]{y}{\mu} = {\pdv[2]{f}{z}}\left( \pdv{y}{\mu}\pdv[2]{y}{\mu} \right)+\pdv{f}{z}{\mu}\pdv{y}{\mu}+{\pdv{f}{\mu}{z}}(-)\pdv{y}{\mu}+{\pdv[2]{f}{\mu}}(-)
    \end{equation*}
    \item Corollary: If $f(t,z;\mu)$ is $C^k$ in $(t,z,\mu)$, $y(t_0)=x(\mu)$ is $C^k$, then $y(t;\mu)$ is $C^k$ in $\mu$.
    \item The Taylor expansion
    \begin{equation*}
        y(t;\mu) = y(t;0)+y_1\mu+y_2\mu^2+\cdots+y_k\mu^k+O(\mu^{k+1})
    \end{equation*}
    of $y(t;\mu)$ about 0 gives an approximation of said function up to order $k$ in $\mu$.
    \begin{itemize}
        \item Misc notes: but you can cut off the expansion at $k$?? $y(t;0)$ being solvable implies inductively that the rest are solvable??
        \item We can take this Taylor expansion because we assume that $y$ is continuously differentiable $k$ times with respect to $\mu$.
        \item The coefficients $y_j$ are given as follows.
        \begin{equation*}
            y_j = \frac{1}{j!}\pdv[j]{y}{\mu}~(t;0)
        \end{equation*}
    \end{itemize}
    \item Application of the Taylor expansion: It can be substituted into the ODE as follows.
    \begin{align*}
        \dot{y} &= f(t,y;\mu)\\
        \dv{t}(y(t;\mu)) &= f(t,y(t;\mu);\mu)\\
        \dv{t}(y(t;0))+\dv{y_1}{t}\mu+\cdots+\dv{y_k}{t}\mu^k+O(\mu^{k+1}) &= f(t,y(t;0)+y_1\mu+\cdots+y_k\mu^k+O(\mu^{k+1});\mu)
    \end{align*}
    \begin{itemize}
        \item Then you can match coefficients of the various $\mu$ terms on the LHS and RHS and solve for $y_0,\dots,y_k$.
        \item When to use this method: Sometimes, you can view equations that aren't explicitly solvable as perturbations of an easily solvable system.
    \end{itemize}
    \item Simple example (more complex ones next lecture):
    \begin{equation*}
        \dv{y}{t} = \mu y
        ,\quad
        y(0) = 1
    \end{equation*}
    \begin{itemize}
        % \item We have $\dv*{y_0}{t}=0$ and $y_0(0)=1$ so $y_0(t)=1$.
        % \item First:
        % \begin{equation*}
        %     y_0'(t)+\mu y_1'(t)+O(\mu^2) = \mu(y_0(t)+\mu y_1(t)+O(\mu^2))
        % \end{equation*}
        % so $y_1'(t)=\mu y_0(t)$, $y_1(0)=0$. Implies that $y_1(t)=t\mu$.
        % \item Second:
        % \begin{equation*}
        %     y_0'+\mu y_1'+\mu^2y_2'+O(\mu^3) = \mu(y_0+\mu y_1+\mu^2y_2+O(\mu^3))
        % \end{equation*}
        % so $y_2'(t)=y_1$, $y_2(0)=0$. Implies that $y_2(t)=\frac{1}{2}t^2$.
        % \item Note that $y(t)=1+t\mu+\frac{1}{2}t^2\mu^2+O(\mu^3)$ does indeed give the first three terms in the Taylor series of $\e[\mu t]$.
        \item First off, we know that there is an explicit solution ($y(t)=\e[\mu t]$). Thus, we will be able to check our final answer.
        \item Suppose $y\in C^2$ with respect to $\mu$. Then
        \begin{equation*}
            y(t;\mu) = y_0+y_1\mu+y_2\mu^2+O(\mu^3)
        \end{equation*}
        \item It follows by substituting into the above differential equation that
        \begin{align*}
            \dv{y}{t} &= \mu y\\
            \dv{t}(y_0+y_1\mu+y_2\mu^2) &= \mu(y_0+y_1\mu+y_2\mu^2)\\
            \dv{y_0}{t}+\dv{y_1}{t}\mu+\dv{y_2}{t}\mu^2 &= 0+y_0\mu+y_1\mu^2+y_2\mu^3
        \end{align*}
        \item By comparing coefficients, this yields the sequentially solvable differential equations
        \begin{align*}
            \dv{y_0}{t} &= 0&
            \dv{y_1}{t} &= y_0&
            \dv{y_2}{t} &= y_1
        \end{align*}
        where we apply the initial condition $y_0(0)=1$ to solve the left ODE above.
        \item Solving, we get
        \begin{align*}
            y_0(t) &= 1&
            y_1(t) &= t&
            y_2(t) &= \frac{t^2}{2}
        \end{align*}
        \begin{itemize}
            \item Where do the other initial conditions (all zero) come from??
        \end{itemize}
        \item Therefore, our approximate solution is
        \begin{equation*}
            y(t) = 1+t\mu+\frac{1}{2}t^2\mu^2+O(\mu^3)
        \end{equation*}
        which does indeed give the first three terms in the Taylor series expansion of the solution $\e[\mu t]$.
    \end{itemize}
    \item The perturbative solution fails in large time intervals --- polynomials inevitably grow slower than exponential functions.
    \item Next time: Several examples applying what we've learned today.
    \item This week's homework: Some basic Lipschitz definitions and also computations with the perturbative series.
\end{itemize}



\section{Variational Examples}
\begin{itemize}
    \item \marginnote{11/4:}We begin today with a more direct and less involved proof of the variation of parameters theorem.
    \begin{proof}
        Let $y'(t;\mu)=f(t,y(t;\mu);\mu)$ with $y(t_0;\mu)=x(\mu)$. Assume Lipschitz continuity and $C^1$-ness of the ODE and the initial condition on $\mu$. Then differentiation with respect to $\mu$ must satisfy the first variational equation. In particular, let $J(t;\mu)$ be the solution of
        \begin{equation*}
            J'(t;\mu) = \underbrace{{\pdv{f}{z}}(t,y(t;\mu);\mu)}_{A(t;\mu)}J(t,\mu)+\underbrace{{\pdv{f}{\mu}}(t,y(t;\mu);\mu)}_{F(t;\mu)}
            ,\quad
            J(t_0;\mu) = \pdv{x}{\mu}
        \end{equation*}
        Consider the Picard iteration sequence defined by
        \begin{align*}
            y_{n+1}(t;\mu) = \underbrace{f(t;y_n(t;\mu);\mu)}_{A_n(t;\mu)}
            ,\quad
            y_n(t_0,\mu) = x(\mu)
        \end{align*}
        Differentiating we get
        \begin{equation*}
            {\pdv{y_n}{\mu}}(t;\mu)
        \end{equation*}
        which we may call $J_n(t;\mu)$. We want to prove that the sequence of functions $J_n$ converges uniformly to $J$. This makes sense since $A$ and $F$ uniformly converge. Moreover, under this definition of $J_n$, we have that
        \begin{equation*}
            J_{n+1}'(t;\mu) = \underbrace{{\pdv{f}{z}}(t,y_n(t;\mu);\mu)}_{A_n(t;\mu)}J_n(t;\mu)+\underbrace{{\pdv{f}{\mu}}(t,y_n(t;\mu);\mu)}_{F_n(t;\mu)}
            ,\quad
            J_n(t_0;\mu) = {\pdv{x}{\mu}}(\mu)
        \end{equation*}
        Thus, Step 1 is to show that $\{\norm{J_n}\}$ is bounded on $[t_0,t_0+T]$. To do so, we note that
        \begin{equation*}
            \norm{J_{n+1}} \leq \frac{1}{2}\norm{J_n}+\sup\left| \pdv{f}{\mu} \right|
        \end{equation*}
        so that $\norm{J_n}$ forms a bounded sequence. By induction,
        \begin{equation*}
            \norm{J_n} \leq 2C
        \end{equation*}
        We now embark on Step 2: Proving $J_n\to J$ uniformly. First off, we have that
        \begin{align*}
            (J-J_{n+1})'(t;\mu) ={}& \dv{t}(J(t;\mu)-J_{n+1}(t;\mu))\\
            ={}& A(t;\mu)J(t;\mu)+F(t;\mu)-A_n(t;\mu)J_n(t;\mu)-F_n(t;\mu)\\
            \begin{split}
                ={}& A(t;\mu)J(t;\mu)+A_n(t;\mu)J(t;\mu)-A_n(t;\mu)J(t;\mu)\\
                &-A_n(t;\mu)J_n(t;\mu)+F(t;\mu)-F_n(t;\mu)
            \end{split}\\
            ={}& A_n(t;\mu)(J-J_n)(t;\mu)+(A-A_n)(t;\mu)J(t;\mu)+(F-F_n)(t;\mu)
        \end{align*}
        and
        \begin{equation*}
            J(t_0;\mu)-J_{n+1}(t_0;\mu) = 0
        \end{equation*}
        Integrating once again on $[t_0,t_0+T]$, we get
        \begin{equation*}
            \norm{J-J_{n+1}} \leq \frac{1}{2}\norm{J-J_n}+\delta_n
        \end{equation*}
        where $\delta_n\to 0$ since we "obviously" have that $A_n\to A$ and $F_n\to F$ uniformly.\par
        We now proceed via a standard analysis argument. Fix $\delta>0$, choose $N$ such that $\delta_n<\delta$ for $n\geq N$. Then we can control it by $\frac{1}{2}\norm{J-J_n}+\delta$ for $n\geq N$. Then
        \begin{equation*}
            \norm{J-J_{n+1}}-2\delta \leq \frac{1}{2}\norm{J-J_n}-2\delta
        \end{equation*}
        for all $n\geq N$, so we have by iteration that $\norm{J-J_{n+1}}\leq 2\delta+\frac{1}{2^{n-N}}\norm{J-J_N}$, so $\lim_{n\to\infty}\norm{J-J_n}<2\delta$ for arbitrary $\delta>0$. Therefore, $\norm{J-J_n}\to 0$, so $J_n\to J$ uniformly.\par
        So in conclusion, $J_n\to J$ uniformly and we recall that $J_n=\pdv*{y_n}{\mu}$ where $y_n\to y$ uniformly.
    \end{proof}
    \item We now look at examples. The ones in the HW will be no more difficult than these.
    \item Example (same one as last time):
    \begin{itemize}
        \item Consider $y'=\mu y$ with $y(0)=1$.
        \item In order to find asymptotic expansion wrt. $\mu$, we use the \textbf{ansatz} $y(t;\mu)=y_0+y_1\mu+y_2\mu^2+\cdots+y_n\mu^n+O(\mu^{n+1})$.
        \begin{itemize}
            \item The differentiation theorem asserts that $y(t;\mu)$ can be differentiated wrt. $\mu$ so many times.
        \end{itemize}
        \item We can compute
        \begin{equation*}
            \mu y(t;\mu) = 0+y_0\mu+y_1\mu^2+\cdots+y_{n-1}\mu^n+O(\mu^{n+1})
        \end{equation*}
        and
        \begin{equation*}
            y'(t;\mu) = y_0'+y_1'\mu+y_2'\mu^2+\cdots+y_n'\mu^n+O(\mu^{n+1})
        \end{equation*}
        and set them equal to yield a system of differential equations.
        \item The initial conditions are $y_0(0)=1$ and then $y_1(0)=\cdots=y_n(0)=0$.
        \item $y_0'=0$ with $y_0(0)=1$ implies that $y_0(t)=1$.
        \item Then the first order approximation is $y_1'=y_0=1$, so solving and applying the initial conditions, we get $y_1(t)=t$.
        \item Continuing on, the second order approximation is $y_2(t)=t^2/2$.
        \item Inductively, $y_m(t)=t^m/m!$.
        \item In conclusion, we obtain the desired approximate solution.
    \end{itemize}
    \item \textbf{Ansatz}: The form of the solution that you guess.
    \item In general, this shows the technique well: Use a polynomial ansatz and compare terms to yield an inductive sequence of explicitly solvable equations up to a certain point.
    \item Example: Mathematical pendulum.
    \begin{itemize}
        \item Suppose that the length of the rope is $\ell$ and the gravitational acceleration is $g$. Then
        \begin{equation*}
            \theta''(t;\mu) = -\frac{g}{\ell}\sin[\theta(t;\mu)]
        \end{equation*}
        \item Assume a small angle, $\theta(0)=\mu$ and $\theta'(0)=0$.
        \item Substitute $\omega_0^2=g/\ell$.
        \item In HS, we learned that the harmonic oscillator approximation of the mathematical pendulum is justified for small $\theta$. We now justify this.
        \item Ansatz: $\theta_0+\theta_1\mu+\theta_2\mu^2+\theta_3\mu^3+O(\mu^4)$.
        \item Recall that
        \begin{equation*}
            \sin\theta = \theta-\frac{\theta^3}{6}+O(\theta^5)
        \end{equation*}
        \item First step, solve to determine $\theta_0=0$.
        \item Then we only have a term of order $O(\mu)$ and $O(\mu^3)$ to worry about.
        \item Substitute the expansion in:
        \begin{align*}
            \sin\theta &= \theta-\frac{\theta^3}{6}+O(\theta^5)\\
            &= \left( \theta_0+\theta_1\mu+\theta_2\mu^2+\theta_3\mu^3 \right)-\frac{1}{6}\left( \theta_0+\theta_1\mu+\theta_2\mu^2+\theta_3\mu^3 \right)^3\\
            &= 0+\theta_1\mu+\theta_2\mu^2+\left( \theta_3-\frac{\theta_1^3}{6} \right)\mu^3+O(\mu^4)
        \end{align*}
        \item We also have that
        \begin{equation*}
            \theta''(t;\mu) = \theta_1''\mu+\theta_2''\mu^2+\theta_3''\mu^3+O(\theta^4)
        \end{equation*}
        and
        \begin{equation*}
            -\omega_0^2\sin(\theta_1\mu+\theta_2\mu^2+\theta_3\mu^3+O(\mu^4)) = -\omega_0^2\theta_1\mu-\omega_0^2\theta_2\mu^2-\omega_0^2\left( \theta_3-\frac{\theta_1^3}{6} \right)\mu^3+O(\mu^4)
        \end{equation*}
        \item Initial conditions: $\theta_0=0$, $\theta_1(0)=1$, and $\theta_2(0) = \theta_3(0) = \theta_1'(0) = \cdots = \theta_3'(0) = 0$.
        \item First order: $\theta_1''=-\omega_0^2\theta_1$, $\theta_1(0)=1$, $\theta_1'(0)=0$. Implies $\theta_1(t)=\cos\omega_0t$. This is why we can use the harmonic oscillator approximation.
        \item Second order: $\theta_2=-\omega_0^2\theta_2$. Initial conditions imply $\theta_2(t)=0$.
        \item Third order: $\theta_3''=-\omega_0^2\theta_3+\frac{\omega_0^2\theta_1^3}{6}$. Implies that
        \begin{equation*}
            \theta_3(t) = \frac{\omega_0t}{16}\sin\omega_0t+\frac{1}{192}(\cos\omega_0t-\cos 3\omega_0t)
        \end{equation*}
        \begin{itemize}
            \item We have to apply some trigonometric identities to verify this??
        \end{itemize}
        \item In conclusion, we have the approximation of our solution up to order $O(\mu^3)$ as
        \begin{equation*}
            \theta(t;\mu) = \mu\cos\omega_0t+\mu^3\left[ \frac{\omega_0t}{16}\sin\omega_0t+\frac{1}{192}(\cos\omega_0t-\cos 3\omega_0t) \right]+O(\mu^4)
        \end{equation*}
        \begin{itemize}
            \item This approximation is only good for $T$ in a fixed, small time interval because the second term is not periodic.
        \end{itemize}
    \end{itemize}
    \item We now investigate the period of the mathematical pendulum.
    \begin{itemize}
        % \item Expectation: $T(\mu)=4$ (first positive zero of $\theta(t;\mu)$).
        % \begin{equation*}
        %     \frac{\theta(t;\mu)}{\mu} = \cos\omega t+\mu^2\left[ \frac{\omega_0t}{16}\sin\omega_0t+\frac{1}{192}(\cos\omega_0t-\cos 3\omega_0t) \right]+O(\mu^4)
        % \end{equation*}
        % \item $F(t;\mu) = \cos\omega t+\mu^2\left[ \frac{\omega_0t}{16}\sin\omega_0t+\frac{1}{192}(\cos\omega_0t-\cos 3\omega_0t) \right]+O(\mu^4)=0$. Want $F(f(\mu);\mu)=0$. Apply the implicit function theorem at $(t_0,\mu_0)=(\pi/2\omega_0,0)$.
        % \item Take ${\pdv{F}{t}}(t_0,\mu_0)=-\omega_0^2\sin\omega_0t_0+\mu_0^2\left( \omega_0/16*\sin\omega_0t_0+\frac{\omega_0^2}{16}t_0\cos\omega_0t_0+\frac{1}{192}(\dots) \right)$. The second term cancels. Thus, this derivative equals $-\omega_0^2\neq 0$. It follows that ?? is indeed uniquely determined by ??.
        % \item Using the law of implicit differentiation, we get
        % \begin{equation*}
        %     f(\mu) = \frac{\pi}{2\omega_0}+b_1\mu+b_2\mu^2+O(\mu^3)
        % \end{equation*}
        % \item Take
        % \begin{align*}
        %     F(f(\mu);\mu) &= \cos(\frac{\pi}{2}+\omega_0b_1\mu+\omega_0b_2\mu^2+O(\mu^3))+\mu^2(\frac{1}{16}(\frac{\pi}{2}++\omega_0b_1\mu+\omega_0b_2\mu^2+O(\mu^3)+...))\\
        %     &= -\omega_0b_1\mu+(\frac{\pi}{32}-\omega_0b_2)\mu^2+O(\mu^3)
        % \end{align*}
        % \item Implicit relation implies that the above equals zero. Thus, we must have $b_1=0$ and $b_2=\pi/32\omega_0$.
        % \item In conclusion, the period is $T(\mu)=2\pi\sqrt{\ell/g}(1+\frac{\pi^2}{16}\mu^2+O(\mu^3))$.
        % \item In conclusion, the ODE induces a second order correction, which will be very small.
        \item The first order approximation (harmonic oscillator) gives the period as $T\approx 2\pi/\omega_0=2\pi\sqrt{\ell/g}$.
        \item Let $T(\mu)$ denote the period of the mathematical pendulum as a function of the starting angle $\mu$.
        \item $T(\mu)$ should be approximately equal to the period of $\theta(t;\mu)$. Additionally, thinking about the mathematical pendulum intuitively, the period $T(\mu)$ should be about four times the first positive zero of $\theta(t;\mu)$.
        \begin{itemize}
            \item Indeed, in a full cycle, the pendulum must go from the positive extreme, to zero, to the negative extreme, back to zero, and back to the original position, so there are our four parts.
            \item Example: In the harmonic oscillator approximation, the first zero is at $\pi/2\omega_0$, and the period is $2\pi/\omega_0=4\cdot\pi/2\omega_0$.
        \end{itemize}
        \item Thus, determining the period $T(\mu)$ becomes a problem of finding $t$ such that $\theta(t;\mu)=0$.
        \item The zeroes of $\theta(t;\mu)$ will be equal to the zeroes of $\theta(t;\mu)/\mu$, so we seek $t$ such that the implicit function
        \begin{equation*}
            F(t;\mu) = \frac{\theta(t;\mu)}{\mu}
            = \cos\omega_0t+\mu^2\left[ \frac{\omega_0t}{16}\sin\omega_0t+\frac{1}{192}(\cos\omega_0t-\cos 3\omega_0t) \right]
            = 0
        \end{equation*}
        \item When $\mu=0$, the mathematical pendulum is stationary, but this does technically mean that it has a zero at $(\pi/2\omega_0;0)$. This point is important because for $\mu$ small enough that the harmonic oscillator approximation is good, the first zero should be very close to $\pi/2\omega_0$. Thus, we choose to solve $F(t;\mu)=0$ around $(t_0;\mu_0)=(\pi/2\omega_0;0)$.
        \item The requirement for the Implicit Function Theorem is met since
        \begin{align*}
            {\pdv{F}{t}}(t_0;\mu_0) &= -\omega_0\sin\omega_0t_0+\mu_0^2\left( \frac{\omega_0}{16}\sin\omega_0t_0+\frac{\omega_0^2t_0}{16}\cos\omega_0t_0+\frac{1}{192}(-\omega_0\sin\omega_0t_0+3\omega_0\sin 3\omega_0t_0) \right)\\
            &= -\omega_0\sin\frac{\pi}{2}+0^2(\dots)\\
            &= -\omega_0\\
            &\neq 0
        \end{align*}
        \item Thus, there exists $t_1(\mu)$ smooth defined on some neighborhood of $\mu_0=0$ satisfying $t_1(0)=\pi/2\omega_0$ and $F(t_1(\mu);\mu)=0$.
        \item We cannot (easily??) obtain $t_1(\mu)$ directly, so we will look for its second-order Taylor expansion
        \begin{equation*}
            t_1(\mu) = \frac{\pi}{2\omega_0}+b_1\mu+b_2\mu^2+O(\mu^3)
        \end{equation*}
        \item We need not compute a bunch of derivatives to find $b_1,b_2$, though. Indeed, we can just substitute into $F(t_1(\mu);\mu)=0$ and compare different powers of $\mu$. Doing so, we obtain
        \begin{align*}
            0 ={}& F(t_1(\mu);\mu)\\
            \begin{split}
                ={}& \cos(\frac{\pi}{2}+\omega_0b_1\mu+\omega_0b_2\mu^2+O(\mu^3))\\
                &+ \mu^2\left[ \frac{1}{16}\left( \frac{\pi}{2}+\omega_0b_1\mu+\omega_0b_2\mu^2+O(\mu^3) \right)\sin(\frac{\pi}{2}+\omega_0b_1\mu+\omega_0b_2\mu^2+O(\mu^3)) \right.\\
                &+ \left. \frac{1}{192}\left( \cos(\frac{\pi}{2}+\omega_0b_1\mu+\omega_0b_2\mu^2+O(\mu^3))-\cos 3\left( \frac{\pi}{2}+\omega_0b_1\mu+\omega_0b_2\mu^2+O(\mu^3) \right) \right) \right]
            \end{split}\\
            ={}& -\omega_0b_1\mu+\left( \frac{\pi}{32}-\omega_0b_2 \right)\mu^2+O(\mu^3)
        \end{align*}
        from which we can determine that
        \begin{align*}
            0 &= -\omega_0b_1&
            0 &= \frac{\pi}{32}-\omega_0b_2\\
            b_1 &= 0&
            b_2 &= \frac{\pi}{32\omega_0}
        \end{align*}
        \item Thus,
        \begin{align*}
            T(\mu) &= 4\cdot t_1(\mu)\\
            &= \frac{2\pi}{\omega_0}+\frac{\pi}{8\omega_0}\mu^2+O(\mu^3)\\
            &= 2\pi\sqrt{\frac{\ell}{g}}\left( 1+\frac{1}{16}\mu^2+O(\mu^3) \right)
        \end{align*}
    \end{itemize}
    \item We calculate an accumulation that is a perturbation of an ODE in the bonus this week, reproducing Einstein's work.
\end{itemize}



\section{Chapter 2: Initial Value Problems}
\emph{From \textcite{bib:Teschl}.}
\subsection*{Section 2.4: Dependence on the Initial Condition}
\begin{itemize}
    \item \marginnote{11/15:}In applications from which ODEs are derived, we usually only know several data approximately. In other words, we're primarily concerned with \textbf{well-posed} IVPs.
    \item \textbf{Well-posed} (IVP): An IVP for which, from an intuitive standpoint, small changes in the data result in small changes of the solution.
    \item That an IVP (under certain conditions) is well-posed will be proven by our next theorem.
    \item To prove this theorem, we will need the following lemma.
    \item Lemma 2.7 (Generalized Gr\"{o}nwall's inequality): Suppose $\psi(t)$ satisfies
    \begin{equation*}
        \psi(t) \leq \alpha(t)+\int_0^t\beta(s)\psi(s)\dd{s}
    \end{equation*}
    for all $t\in[0,T]$. Suppose also that $\alpha(t)\in\R$ and $\beta(t)\geq 0$ for all $t\in[0,T]$. Then
    \begin{equation*}
        \psi(t) \leq \alpha(t)+\int_0^t\alpha(s)\beta(s)\exp(\int_s^t\beta(r)\dd{r})\dd{s}
    \end{equation*}
    for all $t\in[0,T]$.\par
    If, in addition, $\alpha(s)\leq\alpha(t)$ for $s\leq t$, then
    \begin{equation*}
        \psi(t) \leq \alpha(t)\exp(\int_0^t\beta(s)\dd{s})
    \end{equation*}
    for all $t\in[0,T]$.
    \begin{proof}
        Let
        \begin{equation*}
            \phi(t) := \exp(-\int_0^t\beta(s)\dd{s})
        \end{equation*}
        Then we have
        \begin{align*}
            \dv{t}(\phi(t)\int_0^t\beta(s)\psi(s)\dd{s}) &= -\beta(t)\phi(t)\cdot\int_0^t\beta(s)\psi(s)\dd{s}+\phi(t)\cdot\beta(t)\psi(t)\\
            &= \beta(t)\phi(t)\left( \phi(t)-\int_0^t\beta(s)\phi(s)\dd{s} \right)\\
            &\leq \alpha(t)\beta(t)\phi(t)
        \end{align*}
        where the first equality holds by the product rule and the FTC, and the last inequality above holds by the first assumption in the statement of the lemma. Integrating the above inequality with respect to $t$ and dividing the result by $\phi(t)$ shows that
        \begin{align*}
            \int_0^t\beta(s)\psi(s)\dd{s} &\leq \int_0^t\alpha(s)\beta(s)\frac{\phi(s)}{\phi(t)}\dd{s}
            \alpha(t)+\int_0^t\beta(s)\psi(s)\dd{s} &\leq \alpha(t)+\int_0^t\alpha(s)\beta(s)\exp(\int_s^t\beta(r)\dd{r})\dd{s}
        \end{align*}
        It follows that
        \begin{equation*}
            \psi(t) \leq \alpha(t)+\int_0^t\beta(s)\psi(s)\dd{s}
            \leq \alpha(t)+\int_0^t\alpha(s)\beta(s)\exp(\int_s^t\beta(r)\dd{r})\dd{s}
        \end{equation*}
        as desired.\par
        The proof of the second claim is covered in Problem 2.11 (and is not applicable to course content).
    \end{proof}
    \item A simple consequence of the generalized Gr\"{o}nwall's inequality.
    \begin{itemize}
        \item If
        \begin{equation*}
            \psi(t) \leq \alpha+\int_0^t(\beta\psi(s)+\gamma)\dd{s}
        \end{equation*}
        for all $t\in[0,T]$, where $\alpha,\gamma\in\R$ and $\beta\geq 0$, then
        \begin{equation*}
            \psi(t) \leq \alpha\e[\beta t]+\frac{\gamma}{\beta}(\e[\beta t]-1)
        \end{equation*}
        for all $t\in[0,T]$.
        \item See Problem \ref{prb:2.12} for the proof.
    \end{itemize}
    \item We can now show that the IVP is well-posed.
    \item Theorem 2.8: Suppose $f,g\in C(U,\R^n)$ and let $f$ be locally Lipschitz continuous in the second argument, uniformly with respect to the first. If $x(t),y(t)$ are respective solutions of the IVPs
    \begin{align*}
        \dot{x} &= f(t,x)&
            \dot{y} &= g(t,y)\\
        x(t_0) &= x_0&
            y(t_0) &= y_0
    \end{align*}
    then
    \begin{equation*}
        |x(t)-y(t)| \leq |x_0-y_0|\e[L|t-t_0|]+\frac{M}{L}(\e[L|t-t_0|]-1)
    \end{equation*}
    where $L$ is the Lipschitz constant of $f:V\to\R^n$, $M=\norm{f-g}$ for $f,g:V\to\R^n$, and $V\subset U$ contains $G(x),G(y)$.
    \begin{proof}
        WLOG let $t_0=0$. Then
        \begin{align*}
            |x(t)-y(t)| &\leq |x_0-y_0|+\int_0^t|f(s,x(s))-g(s,y(s))|\dd{s}\\
            &\leq |x_0-y_0|+\int_0^t(L|x(s)-y(s)|+M)\dd{s}
        \end{align*}
        Thus, taking
        \begin{equation*}
            \underbrace{|x(t)-y(t)|}_{\psi(t)} \leq \underbrace{|x_0-y_0|}_\alpha+\int_0^t(\underbrace{\vphantom{|}L}_\beta\underbrace{|x(s)-y(s)|}_{\phi(s)}+\underbrace{\vphantom{|}M}_\gamma)\dd{s}
        \end{equation*}
        we have by the above consequence of the generalized Gr\"{o}nwall's inequality that
        \begin{equation*}
            |x(t)-y(t)| \leq |x_0-y_0|\e[Lt]+\frac{M}{L}(\e[Lt]-1)
        \end{equation*}
        as desired.
    \end{proof}
    \item Establishing continuous dependence on the initial condition.
    \begin{itemize}
        \item Denote the solution of the IVP by $\phi(t,t_0,x_0)$ to emphasize the dependence on the initial condition.
        \item Then in the special case $f=g$ (i.e., where $M=0$), Theorem 2.8 implies that
        \begin{equation*}
            |\phi(t,t_0,x_0)-\phi(t,t_0,y_0)| \leq |x_0-y_0|\e[L|t-t_0|]
        \end{equation*}
        \item In other words, $\phi$ depends continuously on the initial value.
        \item Of course, this bound blows up exponentially as $t$ increases, but the linear equation $\dot{x}=x$ shows that we cannot define a better bound in general.
    \end{itemize}
    \item We now formalize the above notion.
    \item Theorem 2.9: Suppose $f\in C(U,\R^n)$ is locally Lipschitz continuous in the second argument, uniformly with respect to the first. Around each point $(t_0,x_0)\in U$, we can find a compact set $I\times B\subset U$ such that $\phi(t,s,x)\in C(I\times I\times B,\R^n)$. Moreover, $\phi(t,t_0,x_0)$ is Lipschitz continuous with
    \begin{equation*}
        |\phi(t,t_0,x_0)-\phi(s,s_0,y_0)| \leq |x_0-y_0|\e[L|t-t_0|]+(|t-s|+|t_0-s_0|\e[L|t-s_0|])M
    \end{equation*}
    where $L$ is the Lipschitz constant of $f:V\to\R^n$, $M=\norm{f}$ for $f:V\to\R^n$, and $V\subset U$ compact contains $I\times\phi(I\times I\times B)$.
    \begin{proof}
        By the Picard-Lindel\"{o}f theorem, there exists $V=[t_0-\varepsilon,t_0+\varepsilon]\times\overline{B_\delta(x_0)}$ such that $\phi(t,t_0,x_0)$ exists and is continuous for $|t-t_0|<\varepsilon$. It can be shown that $\phi(t,t_1,x_1)$ exists for $|t-t_1|\leq\varepsilon/2$, provided that $|t_1-t_0|\leq\varepsilon/2$ and $|x_1-x_0|\leq\delta/2$. Thus, choose $I=[t_0-\varepsilon/2,t_0+\varepsilon/2]$ and $B=\overline{B_{\delta/2}(x_0)}$.\par
        Moreover, we have that
        \begin{align*}
            \begin{split}
                |\phi(t,t_0,x_0)-\phi(s,s_0,y_0)| \leq{}& |\phi(t,t_0,x_0)-\phi(t,t_0,y_0)|\\
                &+|\phi(t,t_0,y_0)-\phi(t,s_0,y_0)|\\
                &+|\phi(t,s_0,y_0)-\phi(s,s_0,y_0)|
            \end{split}\\
            \begin{split}
                \leq{}& |x_0-y_0|\e[L|t-t_0|]\\
                &+\left| \int_{t_0}^tf(r,\phi(r,t_0,y_0))\dd{r}-\int_{s_0}^tf(r,\phi(r,s_0,y_0))\dd{r} \right|\\
                &+\left| \int_s^tf(r,\phi(r,s_0,y_0))\dd{r} \right|
            \end{split}
        \end{align*}
        We estimated the first term using the note on continuous dependence directly preceding this theorem and proof. We can estimate the third term to be $M|t-s|$. We can estimate the second term as follows: Abbreviating $\Delta(t):=|\phi(t,t_0,y_0)-\phi(t,s_0,y_0)|$ and assuming WLOG that $t_0\leq s_0\leq t$, we have that
        \begin{align*}
            \Delta(t) &= \left| \int_{t_0}^tf(r,\phi(r,t_0,y_0))\dd{r}-\int_{s_0}^tf(r,\phi(r,s_0,y_0))\dd{r} \right|\\
            &= \left| \int_{t_0}^{s_0}f(r,\phi(r,t_0,y_0))\dd{r}+\int_{s_0}^tf(r,\phi(r,t_0,y_0))\dd{r}-\int_{s_0}^tf(r,\phi(r,s_0,y_0))\dd{r} \right|\\
            &\leq \int_{t_0}^{s_0}|f(r,\phi(r,t_0,y_0))|\dd{r}+\int_{s_0}^t|f(r,\phi(r,t_0,y_0))-f(r,\phi(r,s_0,y_0))|\dd{r}\\
            &\leq |t_0-s_0|M+L\int_{s_0}^t\Delta(r)\dd{r}
        \end{align*}
        Therefore, by Gr\"{o}nwall's inequality (as in Problem \ref{prb:2.12}; note that $\gamma=0$ here),
        \begin{equation*}
            \Delta(t) \leq |t_0-s_0|M\e[L|t-s_0|]
        \end{equation*}
        as desired.
    \end{proof}
    \item By Problem 1.8, we have $\phi(t,t_0,x_0)=\phi(t-t_0,0,x_0)$ for an autonomous system, so we may consider $\phi(t,x_0)=\phi(t,0,x_0)$.
    \item The previous result of \emph{continuous} dependence on initial conditions is not good enough in every situation; sometimes, we need to be able to \emph{differentiate} with respect to the initial condition.
\end{itemize}

\subsubsection*{Problems}
\begin{enumerate}[label={\textbf{2.\arabic*.}},ref={2.\arabic*},leftmargin=3.5em]
    \setcounter{enumi}{11}
    \item \label{prb:2.12}Show that if
    \begin{equation*}
        \psi(t) \leq \alpha+\int_0^t(\beta\psi(s)+\gamma)\dd{s}
    \end{equation*}
    for all $t\in[0,T]$, where $\alpha,\gamma\in\R$ and $\beta\geq 0$, then
    \begin{equation*}
        \psi(t) \leq \alpha\e[\beta t]+\frac{\gamma}{\beta}(\e[\beta t]-1)
    \end{equation*}
    for all $t\in[0,T]$. \emph{Hint}: Introduce $\tilde{\psi}(t)=\psi(t)+\gamma/\beta$.
    \begin{proof}
        Taking the hint and substituting $\psi(t)=\tilde{\psi}(t)-\gamma/\beta$, we get
        \begin{align*}
            \tilde{\psi}(t)-\frac{\gamma}{\beta} &\leq \alpha+\int_0^t\left( \beta\left( \tilde{\psi}(t)-\frac{\gamma}{\beta} \right)+\gamma \right)\dd{s}\\
            \tilde{\psi}(t) &\leq \alpha+\frac{\gamma}{\beta}+\int_0^t\beta\tilde{\psi}(t)\dd{s}
        \end{align*}
        It follows by the generalized Gr\"{o}nwall's inequality that
        \begin{align*}
            \tilde{\psi}(t) &\leq \alpha+\frac{\gamma}{\beta}+\int_0^t\left( \alpha+\frac{\gamma}{\beta} \right)\beta\exp(\int_s^t\beta\dd{r})\dd{s}\\
            &= \alpha+\frac{\gamma}{\beta}+\int_0^t(\alpha\beta+\gamma)\e[\beta(t-s)]\dd{s}\\
            &= \alpha+\frac{\gamma}{\beta}+(\alpha\beta+\gamma)\e[\beta t]\int_0^t\e[-\beta s]\dd{s}\\
            &= \alpha+\frac{\gamma}{\beta}+(\alpha\beta+\gamma)\e[\beta t]\left( \frac{1}{-\beta}\e[-\beta t]-\frac{1}{-\beta}\e[0] \right)\\
            &= \alpha+\frac{\gamma}{\beta}+\left( \alpha+\frac{\gamma}{\beta} \right)\e[\beta t](1-\e[-\beta t])\\
            &= \alpha+\frac{\gamma}{\beta}+\left( \alpha+\frac{\gamma}{\beta} \right)(\e[\beta t]-1)\\
            &= \alpha+\frac{\gamma}{\beta}+\alpha\e[\beta t]-\alpha+\frac{\gamma}{\beta}(\e[\beta t]-1)\\
            &= \frac{\gamma}{\beta}+\alpha\e[\beta t]+\frac{\gamma}{\beta}(\e[\beta t]-1)
        \end{align*}
        Subtracting $\gamma/\beta$ from both sides and returning the substitution yields the desired result.
    \end{proof}
\end{enumerate}




\end{document}