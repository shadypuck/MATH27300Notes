\documentclass[../notes.tex]{subfiles}

\pagestyle{main}
\renewcommand{\chaptermark}[1]{\markboth{\chaptername\ \thechapter\ (#1)}{}}
\setcounter{chapter}{6}

\begin{document}




\chapter{Solution Existence and Stability}
\section{Peano Existence Theorem}
\begin{itemize}
    \item \marginnote{11/7:}Today: Peano Existence Theorem.
    \item For an IVP of a first-order differential system, as long as the RHS is continuous, we get at least one solution.
    \item The proof provides an algorithm that can be really useful in computing the solution provided that uniqueness exists.
    \item We will need a theorem from analysis to start.
    \item Theorem (Arzel\`{a}-Ascoli\footnote{This is not the full Arzel\`{a}-Ascoli theorem, but a special case. The proof is similar, regardless, though. See Honors Analysis in $\R^n$ I Notes.}): Let $h_k:[a,b]\to\R^n$ be a sequence of functions that is uniformly bounded and uniformly Lipschitz continuous wrt. $L$. Then $\{h_k\}$ contains a uniformly convergent subsequence and the limit has the same bound and Lipschitz constant.
    \begin{proof}
        Recall the property of sequential compactness\footnote{The Bolzano-Weierstrass Theorem/Theorem 15.18 from Honors Calculus IBL.}, i.e., that every bounded sequence of numbers contains a convergent subsequence. We want to prove this for a sequence of functions. To do so, we will need the Cantor diagonalization technique.\par
        $\Q$ is countable. Thus, we can enumerate the rationals in $[a,b]$ by $r_1,r_2,r_3,\dots$. Since $\{h_k(r_1)\}$ is a bounded sequence of numbers, we have by the above that there is a subsequence $C_1$ --- say $h_1^{(1)},h_2^{(1)},h_3^{(1)},\dots$ --- such that $C_1=\{h_k^{(1)}(r_1)\}$ is a convergent subsequence in $\R^n$ of the original sequence. Now $C_1$ is still a bounded sequence, so we can obtain a subsequence $C_2$ of \emph{it} --- say $h_1^{(2)},h_2^{(2)},h_3^{(2)},\dots$ --- such that $C_2=\{h_k^{(2)}(r_2)\}$ is a convergent subsequence in $\R^n$ at $r_2$ (and, by inductive hypothesis, at $r_1$!). Inductively, we can obtain $C_\ell=\{h_k^{(\ell)}\}_{\ell,k=1}^\infty$ convergent at $r_1,r_2,\dots,r_\ell$. We then write down the elements of the sequences as a table. (For example, the $k^\text{th}$ row of the table is a sequence that converges at $r_1,\dots,r_k$.)
        \begin{equation*}
            \begin{matrix}
                h_1^{(1)} & h_2^{(1)} & h_3^{(1)} & \cdots\\
                h_1^{(2)} & h_2^{(2)} & h_3^{(2)} & \cdots\\
                h_1^{(3)} & h_2^{(3)} & h_3^{(3)} & \cdots\\
                \vdots & \vdots & \vdots & \ddots\\
            \end{matrix}
        \end{equation*}
        Consider the diagonal sequence $\{f_\ell\}_{\ell=1}^\infty$ where $f_\ell=h_\ell^{(\ell)}$. By definition, it converges at all rational points. We now seek to prove that it converges uniformly at \emph{all} points.\par
        To prove that $\{f_\ell\}$ is a uniformly convergent sequence of functions, it will suffice to show that for all $\varepsilon>0$, there exists $N$ such that if $k,\ell>N$, then $|f_k(t)-f_\ell(t)|<\varepsilon$ for all $t\in[a,b]$. Let $\varepsilon>0$ be arbitrary. Divide $[a,b]$ into $m$ congruent subintervals $I_\alpha$ ($\alpha=1,\dots,m$) such that $|I_\alpha|\leq\varepsilon/3L$ for all $\alpha$. This guarantees that the oscillation of each $f_k$ on any $I_\alpha$ is $\leq\varepsilon/3$ since if $x,y\in I_\alpha$ for some $\alpha$, then
        \begin{equation*}
            |f_\ell(x)-f_\ell(y)| \leq L|x-y|
            \leq L\cdot\frac{\varepsilon}{3L}
            = \frac{\varepsilon}{3}
        \end{equation*}
        Using the fact that $\{f_\ell\}$ is convergent and hence Cauchy on the rationals, pick $N$ large enough so that $r_\alpha\in I_\alpha$ implies $|f_k(r_\alpha)-f_\ell(r_\alpha)|<\varepsilon/3$ for $k,\ell>N$. We will choose this $N$ to be our $N$. Now let $t\in[a,b]$ be arbitrary. By their definition, we know $t\in I_\alpha$ for some $\alpha$. Therefore,
        \begin{align*}
            |f_k(t)-f_\ell(t)| &\leq |f_k(t)-f_k(r_\alpha)|+|f_k(r_\alpha)-f_\ell(r_\alpha)|+|f_\ell(r_\alpha)-f_\ell(t)|\\
            &< \frac{\varepsilon}{3}+\frac{\varepsilon}{3}+\frac{\varepsilon}{3}\\
            &= \varepsilon
        \end{align*}
        as desired.\par
        Lastly, we can prove that the limit function $f$ of $\{f_\ell\}$ is $L$-Lipschitz as follows. Let $t,t'\in[a,b]$ be arbitrary. Then
        \begin{equation*}
            \left| \frac{f(t)-f(t')}{t-t'} \right| = \lim_{k\to\infty}\left| \frac{f_k(t)-f_k(t')}{t-t'} \right|
            \leq \lim_{k\to\infty}\left| \frac{L|t-t'|}{t-t'} \right|
            = \lim_{k\to\infty}|L|
            = L
        \end{equation*}
        as desired.
    \end{proof}
    \item Now we come to the proof of the Peano Existence Theorem.
    \item Theorem (Peano Existence Theorem): Let $f:[t_0,t_0+a]\times\bar{B}(y_0,b)\to\R^n$ be bounded ($|f(t,z)|\leq M$) and continuous. Then the IVP
    \begin{equation*}
        y'(t)=f(t,y(t))
        ,\quad
        y(t_0)=b
    \end{equation*}
    has at least one solution for $t\in[t_0,t_0+T]$ where $T=\min(a,b/M)$.
    \begin{proof}
        % Fix $T=\min(a,b/M)$. Divide $[t_0,t_0+T]$ into $m$ congruent subintervals $I_\alpha$ ($\alpha=1,\dots,m$). Suppose that $m$ is large so that every subinterval is small. In particular, on each $I_\alpha$, we may replace $y'(t)$ with the difference quotient $h=|I_\alpha|=I/m$. Thus, let $y_m(t_{\alpha+1})=y_m(t_\alpha)+f(t_\alpha,y_m(t_\alpha))h$. That is, we define inductively the value of the solution as the value at the previous node plus the increment. Very similar to Euler's method. The second term is approximately equal to $f'(t_\alpha,y(t_\alpha))h$. In between the nodes, we assume that the function is linear. We connect all nodes by linear functions and say that the piecewise linear function is approximately equal to the solution. We want to use the Arzel\`{a}-Ascoli theorem to prove that something converges uniformly on this time interval. Fix a scalar of error $\varepsilon>0$. Let $f(t,z)$ be uniformly continuous for $(t,z)$, i.e., $|f(t,z)-f(t',z')|\leq\varepsilon/T$ when $|t-t'|+|z-z'|\leq\delta$.


        Since there is no Lipschitz condition, we use another strategy to find approximate solutions.\par
        \emph{picture}
        Fix $T=\min(a,b/M)$. We divide $[t_0,t_0+T]$ into $m$ congruent closed subintervals $I_\alpha$ ($\alpha=0,\dots,m-1$), each of length $h_m=T/m$. Define a continuous function $y_m(t)$ as follows: The values at the nodes $t_\alpha$ (the intersection points of adjacent congruent subintervals) are defined inductively via
        \begin{equation*}
            y_m(t_{\alpha+1}) = y_m(t_\alpha)+f(t_\alpha,y_m(t_\alpha))h_m
        \end{equation*}
        for $\alpha=0,\dots,m-1$, and $y_m$ is taken to be linear between the nodes\footnote{Note that this construction is quite similar to that employed in Euler's method.}. The idea is that we replace the derivative $y'(t)$ by the difference quotient $[y(t+h)-y(t)]/h$. It follows by the construction that every function in the set $\{y_k(t):[t_0,t_0+T]\to\bar{B}(y_0,b)\}$ is piecewise linear (hence continuous), uniformly bounded, and uniformly $M$-Lipschitz continuous. Therefore, by the Arzel\'{a}-Ascoli theorem, $\{y_k\}$ contains a uniformly convergent subsequence $y_{m_k}\to y$.\par
        It remains to verify that $y$ is a solution to the integral equation
        \begin{equation*}
            y(t) = y_0+\int_{t_0}^tf(\tau,y(\tau))\dd\tau
        \end{equation*}
        Observe that the domain of $f$ is a closed and bounded subset of the real numbers. Thus, it is compact by the Heine-Borel theorem\footnote{Theorem 10.16 of Honors Calculus IBL.}. Moreover, since $f$ is a continuous function on a compact domain, we have by the Heine-Cantor theorem\footnote{Theorem 13.6 of Honors Calculus IBL.} that $f$ is uniformly continuous. Thus, for any $\varepsilon>0$, there exists $N$ such that if $m>N$, then
        \begin{equation*}
            |f(t,y_m(t))-f(t_\alpha,y_m(t_\alpha))| < \frac{\varepsilon}{T}
        \end{equation*}
        for all $\alpha=0,\dots,m-1$ and $t\in I_\alpha$. Additionally, observe that
        \begin{equation*}
            y_{m_k}(t) = y_0+\sum_{\alpha=0}^{m-1}\int_{t_\alpha}^{t_{\alpha+1}}\chi_t(\tau)f(t_\alpha,y_{m_k}(t_\alpha))\dd\tau
        \end{equation*}
        where $\chi_t(\tau)$ denotes the \textbf{characteristic function} of $[t_0,t]$. To see this, compare with the original inductive definition of $y_m(t_{\alpha+1})$.
        \emph{picture}
        We thus see that $y_0$ in the above equation corresponds to $y_m(t_0)=y(t_0)$, as we would expect. We see that we are summing a series of side-by-side integrals so that in the end, we integrate over all of $[t_0,t_0+T]$. We see that the characteristic function restricts us to integrating over the ODE only up until $t$, as we would want for an approximation $y_{m_k}(t)$ at $t$ using Euler's method. And we see that since $f(t_\alpha,y_{m_k}(t_\alpha))$ is constant and $h_m=t_{\alpha+1}-t_\alpha$, the integral does take on the expected value $f(t_\alpha,y_m(t_\alpha))h_m$. Moving right along, we see that
        \begin{align*}
            \left| y_{m_k}(t)-y_0-\int_{t_0}^tf(\tau,y_{m_k}(\tau))\dd\tau \right| &\leq \sum_{\alpha=0}^{m-1}\int_{t_\alpha}^{t_{\alpha+1}}\chi_t(\tau)|f(t_\alpha,y_{m_k}(t_\alpha))-f(\tau,y_{m_k}(\tau))|\dd\tau\\
            &< \int_{t_0}^{t_0+T}\chi_t(\tau)\cdot\frac{\varepsilon}{T}\dd\tau\\
            &= \int_{t_0}^t\frac{\varepsilon}{T}\dd\tau\\
            &= \varepsilon\cdot\frac{t-t_0}{T}\\
            &\leq \varepsilon
        \end{align*}
        Thus, by uniform convergence, $\int_{t_0}^tf(\tau,y_{m_k}(\tau))\dd\tau\to\int_{t_0}^tf(\tau,y(\tau))\dd\tau$ uniformly, so $y$ does satisfy the integral equation, as desired.
    \end{proof}
    \item \textbf{Characteristic function} (of $[a,b]$): The function defined as follows. \emph{Denoted by} $\bm{\chi_{[a,b]}}$. \emph{Given by}
    \begin{equation*}
        \chi_{[a,b]}(t) =
        \begin{cases}
            1 & x\in[a,b]\\
            0 & x\notin[a,b]
        \end{cases}
    \end{equation*}
    \item Utility of the Peano Existence Theorem: Proves the \emph{existence} of a solution, but the proof is not constructive; it does not give an algorithm for finding the desired sequence. Nor does the PET make any statement on uniqueness.
    \item We now look to use a related method to define a sequence of functions that will converge to the desired solution of the ODE.
    \begin{itemize}
        \item While the PET does not require it, in practice, most $f$ we would be interested in will satisfy an additional Lipschitz condition.
        \item Define the integral operator
        \begin{equation*}
            \Phi[u] = y_0+\int_{t_0}^tf(\tau,u(\tau))\dd\tau
        \end{equation*}
        We will prove that $\Phi$ is a contraction on the function space. This will imply that $\Phi^N[u]$ converges across the entire interval $[t_0,t_0+T]$ to the solution $y$ for any $u:[t_0,t_0+T]\to\bar{B}(y_0,b)$, giving us our desired computational strategy. Let's begin.
        \item To prove that $\Phi$ is a contraction, it will suffice to show that $\norm{\Phi^j[u_1]-\Phi^j[u_2]}\to 0$ as $j\to\infty$. Thus, we wish to put a bound on $\norm{\Phi^j[u_1]-\Phi^j[u_2]}$ that decreases as $j$ increases. To that end, we will prove that
        \begin{equation*}
            \norm{\Phi^j[u_1]-\Phi^j[u_2]} \leq \frac{(LT)^j}{j!}\cdot\norm{u_1-u_2}
        \end{equation*}
        for all $j$.
        \begin{itemize}
            \item We induct on $j$. For the base case $j=1$, we have that
            \begin{align*}
                |\Phi[u_1](t)-\Phi[u_2](t)| &\leq \int_{t_0}^tL|u_1(\tau)-u_2(\tau)|\dd\tau\\
                &\leq L(t-t_0)\norm{u_1-u_2}\\
                &\leq LT\norm{u_1-u_2}\\
                &= \frac{(LT)^1}{1!}\cdot\norm{u_1-u_2}
            \end{align*}
            for all $t$.
            \item Now suppose inductively that $\norm{\Phi^j[u_1]-\Phi^j[u_2]}\leq(LT)^j/j!\cdot\norm{u_1-u_2}$. Then we have that
            \begin{align*}
                |\Phi^{j+1}[u_1](t)-\Phi^{j+1}[u_2](t)| &\leq \int_{t_0}^tL|\Phi^j[u_1](\tau)-\Phi^j[u_2](\tau)|\dd\tau\\
                &\leq \int_{t_0}^tL\cdot\frac{(LT)^j}{j!}\cdot\norm{u_1-u_2}\dd\tau\\
                &= \cdots\\
                &\leq \frac{(LT)^{j+1}}{j!}\cdot\norm{u_1-u_2}
            \end{align*}
            for all $t$, implying the desired result.
        \end{itemize}
        \item We now estimate the error between $y_m$ and $y$ in terms of $y_m$, alone. Indeed, we have from the above that
        \begin{align*}
            \norm{y_m-\Phi^N[y_m]} &\leq \sum_{j=0}^{N-1}\norm{\Phi^j[y_m]-\Phi^{j+1}[y_m]}\\
            &\leq \norm{y_m-\Phi[y_m]}\sum_{j=0}^{N-1}\frac{(TL)^j}{j!}\\
            \norm{y_m-y} &\leq \norm{y_m-\Phi[y_m]}\e[TL]
        \end{align*}
        where we get from the second to the third line by letting $N\to\infty$.
        \begin{itemize}
            \item The proof of the PET guarantees that $\norm{y_m-\Phi[y_m]}$ is small when $m$ is large, no matter whether $y_m$ itself converges or not.
            \item In fact, when $f\in C^1$, the error is estimated as
            \begin{equation*}
                \norm{y_m-y} \leq \frac{LT\e[TL]}{m}
            \end{equation*}
            for $L=\norm{f}_{C^1}$.
        \end{itemize}
    \end{itemize}
    \item Takeaway: This polygon method gives rise to an algorithm to solve ODEs. Theoretically, it converges much slower than the Picard iteration, but in practice, it has the advantage that we do not need to do any numerical integration. Indeed, to obtain the desired precision using the Picard iteration, the numerical integration will need more and more steps and the total accumulated error will not be less than this polygon method.
    \item Better difference methods include Runge-Kutta or Heun, but please refer to monographs on numerical ODEs for these.
\end{itemize}



\section{Asymptotic Stability}
\begin{itemize}
    \item \marginnote{11/9:}Going forward, we restrict ourselves to autonomous ODEs $y'=f(y)$, where $f:\R^n\to\R^n$ is a smooth vector field.
    \item For every $x\in\R^n$, the IVP
    \begin{equation*}
        y' = f(y)
        ,\quad
        y(0) = x
    \end{equation*}
    has a unique maximal solution $\phi_t(x)$ for $t\in I_x$.
    \item \textbf{Orbit}: The following set. \emph{Given by}
    \begin{equation*}
        \{\phi_t(x):t\in I_x\}
    \end{equation*}
    \item Let $K\subset\R^n$ be compact.
    \begin{itemize}
        \item Then there exists $T_K\in\R$ such that $\phi_t(x)$ is defined for all $x\in K$ and $|t|\leq T_K$.
        \item Moreover, the map from $K\to\R^n$ defined by $x\mapsto\phi_t(x)$ is injective due to uniqueness (and therefore a \textbf{homeomorphism}). We get one such map for each $t$.
        \item Similar to the diffeomorphism idea from \textcite{bib:DifferentialForms}.
    \end{itemize}
    \item \textbf{Invariant set}: A subset of $\R^n$ such that any orbit starting within it never leaves it.
    \item Compact invariant sets are quite interesting.
    \item Proposition: Let $\Omega\subset\R^n$ be a domain with a piecewise smooth boundary $\partial\Omega$. Suppose $f(x)$ is transversal to $\partial\Omega$ and inward pointing: That is, if $\nu$ is the inward pointing unit normal, then $f(x)\cdot\nu(x)\geq 0$ for all $x\in\partial\Omega$. Then $\bar{\Omega}$ is an invariant set: That is, any orbit starting from a point $\bar{\Omega}$ exists throughout the time and never leaves $\bar{\Omega}$.
    \begin{proof}[Proof idea]
        $x\in\partial\Omega$ ensures that $\phi_t(x)$ must be in $\Omega$ for small $t$. Hence, it suffices to consider $x\in\Omega$. In that case, pick the smallest $T>0$ such that $\phi_T(x)\in\partial\Omega$. Then by transversality it must turn back into $\Omega$.
    \end{proof}
    \item This simple proposition is especially useful when establishing global attraction of the orbits.
    \item \textbf{Fixed point}: A point in $\R^n$ at which $f$ evaluates to zero. \emph{Denoted by} $\bm{x_0}$.
    \begin{itemize}
        \item This means that the vector at $x_0$ is zero.
    \end{itemize}
    \item \textbf{Lyapunov stable} (fixed point): A fixed point $x_0$ such that for any neighborhood $B(x_0,\varepsilon)$, there exists a neighborhood $B(x_0,\delta)$ such that $\phi_t(x)\in B(x_0,\varepsilon)$ for any $t\geq 0$ and $x\in B(x_0,\delta)$.
    \item \textbf{Asymptotically stable} (fixed point): A Lyapunov stable fixed point $x_0$ such that $\phi_t(x)\to x_0$ as $t\to+\infty$ for $x\in B(x_0,\delta)$.
    \item Example of a system that is Lyapunov stable but not asymptotically stable: The system
    \begin{equation*}
        y' =
        \begin{pmatrix}
            0 & 1\\
            -1 & 0\\
        \end{pmatrix}
        y
    \end{equation*}
    where $A$ denotes a rotation.
    \begin{itemize}
        \item The orbits are concentric circles and never converge to 0.
    \end{itemize}
    \item Investigation: The local behavior near a fixed point.
    \begin{itemize}
        \item Consider $y'=f(y)$ as a perturbation of the linearized system $y'=f'(x_0)y$. In this case,
        \begin{equation*}
            f(x) = f'(x_0)(x-x_0)+O(|x-x_0|^2)
        \end{equation*}
        as $x\to x_0$.
    \end{itemize}
    \item Theorem: Let $f(x_0)=0$. If the eigenvalues of the linearization $A=f'(x_0)$ all have negative real parts, then the fixed point $x=x_0$ is asymptotically stable.
    \begin{proof}
        WLOG let $x_0=0$. Write $f(x)=Ax+g(x)$, where $g(x)=O(|x|^2)$. Since every $\lambda\in\sigma(A)$ has negative real part, there exist $a,C>0$ (let $C>1$ WLOG) such that
        \begin{equation*}
            |\e[tA]x| \leq C\e[-at]|x|
        \end{equation*}
        The $C$ arises because the matrix norm of $\e[tA]$ is bounded as $t\to +\infty$ if all eigenvalues are negative. The $\e[-at]$ arises similarly, and reflects the exponential decrease in magnitude happening along all subspaces on which $\e[tA]$ acts.\par
        Let $\delta$ be such that $|g(x)|\leq a|x|/2C$ when $|x|\leq\delta$. Now consider the IVP
        \begin{equation*}
            y' = Ay+g(y)
            ,\quad
            y(0) \in \bar{B}\left( 0,\frac{\delta}{2C} \right)
        \end{equation*}
        Then at least for small $t$ (i.e., $t$ such that $|y(t)|\leq\delta$),
        \begin{equation*}
            |y(t)| \leq C\e[-at]|y(0)|+\frac{a}{2C}\int_0^t\e[-a(t-\tau)]|y(\tau)|\dd\tau
        \end{equation*}
        It follows from Gr\"{o}nwall's inequality that
        \begin{equation*}
            \e[at]|y(t)| \leq C|y(0)|\e[at/2]
        \end{equation*}
        hence
        \begin{equation*}
            |y(t)| \leq \frac{\delta}{2}\e[-at/2]
            < \delta
        \end{equation*}
        Hence, any orbit of the system starting from $\bar{B}(0,\delta/2C)$ stays in $\bar{B}(0,\delta)$. So the maximal time of existence $T$ is $+\infty$. This is because if not then, then the IVP starting from $y(T)$ is still solvable, contradicting the definition of $T$. Thus, we have proven that
        \begin{equation*}
            |y(t)| \leq \frac{\delta}{2}\e[-at/2]
        \end{equation*}
        for all $t\geq 0$ as long as $|y(0)|\leq\delta/2C$.
    \end{proof}
    \item This is the last rigorous proof given in this course.
    \item A similar theorem:
    \item Theorem: Let $f(0)=0$. If one of the eigenvalues of $A=f'(0)$ has positive real part, then the fixed point $x=0$ is not Lyapunov stable.
    \item Initial application: Nonlinear mechanical system with frictions, e.g., ideal pendulum with friction.
    \begin{equation*}
        ml\theta''+b\theta' = -mg\sin\theta
    \end{equation*}
    \begin{itemize}
        \item Substitute $\eta=b/ml$ and $\omega=\theta'$ to get a nonlinear system
        \begin{equation*}
            \begin{pmatrix}
                \theta\\
                \omega\\
            \end{pmatrix}'
            =
            \begin{pmatrix}
                \omega\\
                -\eta\omega-g/l\sin\theta\\
            \end{pmatrix}
        \end{equation*}
        \item At the equilibrium position $(\theta,\omega)=(0,0)$, we have
        \begin{equation*}
            A =
            \begin{pNiceMatrix}
                \pdv{\theta}(\omega) & \pdv{\omega}(\omega)\\
                \pdv{\theta}(-\eta\omega-g/l\sin\theta) & \pdv{\omega}(-\eta\omega-g/l\sin\theta)\\
            \end{pNiceMatrix}
            \approx
            \begin{pmatrix}
                0 & 1\\
                -g/l & -\eta\\
            \end{pmatrix}
        \end{equation*}
        i.e.,
        \begin{equation*}
            \begin{pmatrix}
                \theta\\
                \omega\\
            \end{pmatrix}'
            =
            \begin{pmatrix}
                0 & 1\\
                -g/l & -\eta\\
            \end{pmatrix}
            \begin{pmatrix}
                \theta\\
                \omega\\
            \end{pmatrix}
            +O(|\theta|^2+|\omega|^2)
        \end{equation*}
        \begin{itemize}
            \item Since $\eta>0$, the eigenvalues have a common negative real part, so the equilibrium is asymptotically stable.
        \end{itemize}
        \item At the equilibrium $(\pi,0)$, we have
        \begin{equation*}
            \begin{pmatrix}
                \theta\\
                \omega\\
            \end{pmatrix}'
            =
            \begin{pmatrix}
                0 & 1\\
                g/l & -\eta\\
            \end{pmatrix}
            \begin{pmatrix}
                \theta-\pi\\
                \omega\\
            \end{pmatrix}
            +O(|\theta-\pi|^2+|\omega|^2)
        \end{equation*}
        \begin{itemize}
            \item For $\eta\geq 0$, there is one positive and one negative eigenvalue, so this equilibrium is unstable.
        \end{itemize}
        \item These results should make intuitive sense: If a pendulum is resting at the bottom, that is a stable equilibrium. If a pendulum is resting at the top, that is not a stable equilibrium.
    \end{itemize}
\end{itemize}



\section{Applications of the Lyapunov Method}
\begin{itemize}
    \item \marginnote{11/11:}Purely imaginary eigenvalues can still lead to Lyapunov stability.
    \item \textbf{Lyapunov function} (of a system $y'=f(y)$ with fixed point $x_0$ near $x_0$): A continuous real function on $\R^n$ such that the following two axioms hold. \emph{Denoted by} $\bm{L}$.
    \begin{enumerate}
        \item $L(x_0)=0$ and $L(x)>0$ for all $x\in\mathring{B}(x_0,\delta)=B(x_0,\delta)\setminus\{x_0\}$.
        \item $\dot{L}(x)=\nabla L(x)\cdot f(x)\leq 0$ for all $x\in\mathring{B}(x_0,\delta)=B(x_0,\delta)\setminus\{x_0\}$.
    \end{enumerate}
    \item Since
    \begin{equation*}
        \dv{t}L(\phi_t(x)) = \nabla L(\phi_t(x))\cdot f(\phi_t(x))
    \end{equation*}
    the second condition is equivalent to saying that the function $L$ is decreasing along the orbits starting near $x_0$.
    \item \textbf{Strict} (Lyapunov function): A Lyapunov function for which the decreasing is strict.
    \item Theorem: For the autonomous system $y'=f(y)$, a fixed point $x_0$ is
    \begin{enumerate}
        \item Stable if there is a Lyapunov function near it;
        \begin{proof}
            Pick a small number $\delta>0$. Let\footnote{Intuitively (in 2D), we take a ring around $x_0$, find the nonzero value of $L(x)$ at each point on the ring, and take the minimum among them. Imagine a circular valley with hills rising all around the bottommost point; we are essentially looking for the hill that rises the least.}
            \begin{equation*}
                m := \min\{L(x):|x-x_0|=\delta\}
            \end{equation*}
            Since $x_0$ does not satisfy $|x-x_0|=\delta>0$, we know from the first constraint on Lyapunov functions that $L(x)>0$ for all $x$ satisfying said relation. Thus, $m>0$. Consequently, any orbit starting from $\{x\mid L(x)<m\}\cap B(x_0,\delta)$ can never meet $\partial B(x_0,\delta)$ since $L(x)$ is decreasing along any orbit (and we would have to go up to get to the boundary). So $L(\phi_t(x))<m$ for all $x\in\{x\mid L(x)<m\}\cap B(x_0,\delta)$. But this means that $\{x\mid L(x)<m\}\cap B(x_0,\delta)$ is in fact an invariant set. Therefore, $x_0$ is Lyapunov stable.
        \end{proof}
        \item Asymptotically stable if there is a strict Lyapunov function near it.
        \begin{proof}
            If $x\in\{x\mid L(x)<m\}\cap B(x_0,\delta)$, then $L(\phi_t(x))$ is strictly decreasing. As $t\to +\infty$, $\phi_t(x)$ has a partial limit $z_0$, say $\phi_{t_k}(x)\to z_0$ (Lemma 6.6 of \textcite{bib:Teschl}). If $z_0\neq x_0$, then the orbit $\{\phi_t(z_0)\mid t\in I_{z_0}\}$ is not a single point: Since $L$ is a strict Lyapunov function, we have $L(\phi_t(z_0))<L(z_0)$ for all $t>0$. When $k$ is large, $\phi_{t_k}(x)$ is close to $z_0$, so by continuity,
            \begin{equation*}
                L(\phi_{t+t_k}(x)) = L(\phi_t(\phi_{t_k}(x))) < L(z_0)
            \end{equation*}
            But this contradicts $L(\phi_t(x))>L(z_0)$ (which we must have if there are arbitrarily large $t$ such that $\phi_t(x)$ is close to $z_0$). Therefore, $x_0=z_0$.
        \end{proof}
    \end{enumerate}
    \item If all eigenvalues of $A$ have negative real parts, then the perturbed system
    \begin{equation*}
        y' = Ay+g(y)
    \end{equation*}
    has a strict Lyapunov function around the fixed point $x=0$.
    \begin{itemize}
        \item This observation yields another proof of the stability theorem.
    \end{itemize}
    \item Advantage of the Lyapunov function: Can be constructed globally and thus gives us global information on the system.
    \item Examples in studying the global behavior of a phase portrait:
    \begin{itemize}
        \item Consider a mass point moving along the real axis in a potential field $U(x)$. Then
        \begin{equation*}
            mx'' = -U'(x)
        \end{equation*}
        \begin{itemize}
            \item The total energy
            \begin{equation*}
                E = \frac{m}{2}|x'|^2+U(x)
            \end{equation*}
            is always a constant along any solution.
            \item Introducing the velocity allows us to obtain a planar system
            \begin{equation*}
                \begin{pmatrix}
                    x\\
                    v\\
                \end{pmatrix}'
                =
                \begin{pmatrix}
                    v\\
                    -U'(x)/m\\
                \end{pmatrix}
            \end{equation*}
            \item Thus, $E(x,v)$ is a global Lyapunov function.
            \item Any fixed point of the system must be of the form $(x_0,0)$, where $U'(x_0)=0$.
            \begin{itemize}
                \item Intuitively, this means that the velocity must be zero (that makes sense) and the position must be such that we are at a critical point of the potential.
            \end{itemize}
            \item Because of this, the linearization at a fixed point must be of the following form.
            \begin{equation*}
                \begin{pmatrix}
                    v\\
                    -U'(x)/m\\
                \end{pmatrix}
                =
                \begin{pmatrix}
                    0 & 1\\
                    -U''(x_0)/m & 0\\
                \end{pmatrix}
                \begin{pmatrix}
                    x-x_0\\
                    v\\
                \end{pmatrix}
                +O(|x-x_0|^2+|v|^2)
            \end{equation*}
            \item Thus, $(x_0,0)$ is Lyapunov stable if $U$ has a nondegenerate local minimum at $x_0$ and unstable if $U$ has a nondegenerate local maximum at $x_0$.
            \begin{itemize}
                \item In the former case, the orbits near $(x_0,0)$ are closed curves, corresponding to periodic oscillations near $x_0$ (e.g., harmonic oscillator and ideal pendulum again).
            \end{itemize}
        \end{itemize}
        \item Prey-predator model with capacity:
        \begin{equation*}
            \begin{pmatrix}
                x\\
                y\\
            \end{pmatrix}'
            =
            \begin{pmatrix}
                (1-y-\lambda x)x\\
                \alpha(x-1-\mu y)y\\
            \end{pmatrix}
        \end{equation*}
        $\alpha,\lambda,\mu>0$.
        \begin{itemize}
            \item $x$ is the number of rabbits and $y$ is the number of wolves.
            \item Different ranges of $\lambda$ induce different global behavior (thus, this is an example of \textbf{bifurcation}).
            \item General observation: $(x,y)=(0,0)$ is a saddle point since the linearization there is $\diag(1,-\alpha)$.
            \item For $x=0$ or $y=0$, the equation is of separable form; the positive $x,y$-axes are invariant sets.
            \begin{itemize}
                \item Implication: No orbit in the first quadrant can escape it (compatible with meaning as population).
            \end{itemize}
            \item Jacobian:
            \begin{equation*}
                \begin{pmatrix}
                    1-y-2\lambda x & -x\\
                    \alpha y & \alpha(x-1)-2\alpha\mu y\\
                \end{pmatrix}
            \end{equation*}
            \item When $\lambda,\mu=0$, we're back to the Lotka-Volterra system, where there is a single fixed point $(1,1)$.
            \begin{itemize}
                \item In that case,
                \begin{equation*}
                    (y-\log y-1)+\alpha(x-\log x-1)
                \end{equation*}
                is a Lyapunov function.
                \item However, it is not a strict Lyapunov function since it is constant along any orbit.
                \item Moreover, the function is convex, so all level sets are closed curves around the fixed point.
                \item This is, indeed, the behavior we observe in Figure \ref{fig:LotkaVolteraSolns}.
            \end{itemize}
            \item Other cases: $\lambda\geq 1$.
            \begin{itemize}
                \item There is only one additional fixed point of interest: $(1/\lambda,0)$. Note that there are other fixed points, but these do not lie in the first quadrant and thus we are not interested.
                \item For $\lambda>1$, the fixed point is stable (a sink) and when $\lambda=1$, one eigenvalue is 0 since the linearization at that point is $\diag(-1,\alpha(1/\lambda-1))$.
            \end{itemize}
            \item $0<\lambda<1$.
            \begin{itemize}
                \item $(1/\lambda,0)$ becomes a saddle point, and there is a third fixed point
                \begin{equation*}
                    (x_0,y_0) = \left( \frac{1+\mu}{1+\mu\lambda},\frac{1-\lambda}{1+\mu\lambda} \right)
                \end{equation*}
            \end{itemize}
            \item More on this case in Chapter 7 of \textcite{bib:Teschl}. This is relevant here!
        \end{itemize}
    \end{itemize}
\end{itemize}



\section{Chapter 2: Initial Value Problems}
\emph{From \textcite{bib:Teschl}.}
\subsection*{Section 2.6: Extensibility of Solutions}
\begin{itemize}
    \item \marginnote{12/6:}Investigating the maximal interval on which a solution to an IVP can be defined.
    \item Not really something we covered in class (certainly not from a theoretical point of view).
\end{itemize}


\subsection*{Section 2.7: Euler's Method and the Peano Theorem}
\begin{itemize}
    \item Mostly review from class; a few interesting points noted below.
    \item We can derive the Peano proof technique from Taylor's theorem by approximating
    \begin{equation*}
        \phi(t_0+h) = x_0+\dot{\phi}(t_0)h+o(h)
        = x_0+f(t_0,x_0)h+o(h)
    \end{equation*}
    eliminating the error term, and rearranging.
    \item \textbf{Euler's method}: A method for approximating the solution to an ODE via
    \begin{equation*}
        x_h(t_{m+1}) = x_h(t_m)+f(t_m,x_h(t_m))h
        ,\quad
        t_m = t_0+mh
    \end{equation*}
    using linear interpolation in between.
    \item \textbf{Equicontinuous} (family of functions): A family of functions $\{x_m\}$ such that for every $\varepsilon>0$, there is a $\delta>0$ such that if $|t-s|<\delta$ and $m\in\N$, then
    \begin{equation*}
        |x_m(t)-x_m(s)| \leq \varepsilon
    \end{equation*}
    \begin{itemize}
        \item Note that each function in an equicontinuous family of functions is uniformly continuous.
    \end{itemize}
    \item Theorem 2.18 (Arzel\`{a}-Ascoli): Suppose the sequence of functions $x_m(t)\in C(I,\R^n)$, $m\in\N$, on a compact inverval $I$ is equicontinuous. If the sequence $x_m$ is bounded, then there is a uniformly convergent subsequence.
    \item Theorem 2.19 (Peano): Suppose $f$ is continuous on $V=[t_0,t_0+T]\times\overline{B_\delta(x_0)}\subset U$ and denote the maximum of $|f|$ by $M$. Then there exists at least one solution of the initial value problem
    \begin{equation*}
        \dot{x} = f(t,x)
        ,\quad
        x(t_0) = x_0
    \end{equation*}
    for $t\in[t_0,t_0+T_0]$ which remains in $\overline{B_\delta(x_0)}$, where $T_0=\min(T,\delta/M)$. The analogous result holds for the interval $[t_0-T_0,t_0]$.
    \item The Euler algorithm is not the most effective one available today.
    \begin{itemize}
        \item Variations on it usually take more terms in the Taylor expansion, resulting in an algorithm that converges faster but requires more calculations at each step.
        \item A good compromise between more terms (but not too many more terms) is the \textbf{Runge-Kutta algorithm}.
        \item Even better ones appear in the literature on numerical methods for ODEs.
    \end{itemize}
    \item \textbf{Runge-Kutta algorithm}: An algorithm which approximates $\phi(t_0+h)$ up to the fourth order in $h$, setting $t_m=t_0+hm$ and $x_m=x_n(t_m)$ to yield
    \begin{equation*}
        x_{m+1} = x_m+\frac{h}{6}(k_{1,m}+2k_{2,m}+2k_{3,m}+k_{4,m})
    \end{equation*}
    where
    \begin{align*}
        k_{1,m} &= f(t_m,x_m)&
            k_{2,m} &= f(t_m+\tfrac{h}{2},x_m+\tfrac{h}{2}\cdot k_{1,m})\\
        k_{3,m} &= f(t_m+\tfrac{h}{2},x_m+\tfrac{h}{2}\cdot k_{2,m})&
            k_{4,m} &= f(t_{m+1},x_m+hk_{3,m})
    \end{align*}
\end{itemize}

\subsubsection*{Problems}
\begin{enumerate}[label={\textbf{2.\arabic*.}},ref={2.\arabic*},leftmargin=3.5em]
    \setcounter{enumi}{22}
    \item \label{prb:2.23}\textbf{Heun's method} (or \textbf{improved Euler}) is given by
    \begin{equation*}
        x_{m+1} = x_m+\frac{h}{2}(f(t_m,x_m)+f(t_{m+1},y_m))
        ,\quad
        y_m = x_m+f(t_m,x_m)h
    \end{equation*}
    Show that using this method, the error during one step is of $O(h^3)$, provided $f\in C^2$:
    \begin{equation*}
        \phi(t_0+h) = x_0+\frac{h}{2}(f(t_0,x_0)+f(t_1,y_0))+O(h^3)
    \end{equation*}
    Note that this is not the only possible scheme with this error order since
    \begin{equation*}
        \phi(t_0+h) = x_0+\frac{h}{2}(f(t_1,x_0)+f(t_0,y_0))+O(h^3)
    \end{equation*}
    as well.
\end{enumerate}



\section{Chapter 6: Dynamical Systems}
\emph{From \textcite{bib:Teschl}.}
\subsection*{Section 6.1: Dynamical Systems}
\begin{itemize}
    \item Good intuition for what a dynamical system is.
    \item \textbf{Semigroup}: An algebraic structure consisting of a set together with an associative internal binary operation on it.
    \begin{itemize}
        \item Thus, like a \textbf{group}, a semigroup's operation is associative. However, we do not postulate the existence of an identity element or inverses in this case.
    \end{itemize}
    \item \textbf{Dynamical system}: The action of a semigroup $G$ with identity element $e$\footnote{So a \textbf{monoid}?? A monoid is, by definition, an algebraic structure consisting of a set together with an associative internal binary operation and an identity element.} on a set $M$.
    \begin{itemize}
        \item In particular, a dynamical system is a map $T:G\times M\to M$ which sends $(g,x)\mapsto T_g(x)$ such that
        \begin{align*}
            T_g\circ T_h &= T_{g\circ h}&
            T_e &= \mathbb{I}
        \end{align*}
        \item Intuition: We often think of a dynamical system very similar to a diffeomorphism, in that as we slide $t$ up and down, a set of points gets distorted according to some field. Here, we're taking the formalization of time \emph{acting on} the points to move them around.
        \item This is an incredibly minimal/broad/general definition; the dynamical systems we're interested in usually have far more structure.
    \end{itemize}
    \item \textbf{Invertible dynamical system}: A dynamical system for which $G$ is a group.
    \item \textbf{Discrete dynamical system}: A dynamical system for which $G\in\{\N_0,\Z\}$.
    \item \textbf{Continuous dynamical system}: A dynamical system for which $G\in\{\R^+,\R\}$.
    \item Example: Iterated map, i.e., $f^n$.
    \item Example: The flow of an autonomous differential equation, where $T_t=\Phi_t$ and $G=\R$; we consider this example in the next section.
\end{itemize}


\subsection*{Section 6.2: The Flow of an Autonomous Equation}
\begin{itemize}
    \item Herein, we consider the system
    \begin{equation*}
        \dot{x} = f(x)
        ,\quad
        x(0) = x_0
    \end{equation*}
    \item For the remainder of \textcite{bib:Teschl}, we assume $f\in C^k(M,\R^n)$ ($k\geq 1$).
    \begin{itemize}
        \item We also assume $M$ is an open subset of $\R^n$.
    \end{itemize}
    \item Such a system can be regarded as a \textbf{vector field} on $\R^n$.
    \begin{itemize}
        \item Solutions are curves in $M$ which are tangent to the vector field at each point.
    \end{itemize}
    \item \textbf{Integral curve}: A solution to an autonomous IVP. \emph{Also known as} \textbf{trajectory}.
    \begin{itemize}
        \item We say "$\phi$ is an integral curve at $x_0$" if $\phi(0)=x_0$.
    \end{itemize}
    \item By Theorem 2.13: Every point $x\in M$ has an associated (unique) \textbf{maximal integral curve}.
    \item \textbf{Maximal integral curve} (at $x$): The unique integral curve at $x$, the domain of which is a \textbf{maximal interval}. \emph{Denoted by} $\bm{\phi_x}$.
    \item \textbf{Maximal interval}: The interval for an integral curve at $x$ containing all other possible intervals on which the integral curve can be defined. \emph{Denoted by} $\bm{I_x}$, $\bm{(T_-(x),T_+(x))}$.
    \item We define a set which contains information about the maximal interval of the integral curve at $x$ for all $x$:
    \begin{equation*}
        W = \bigcup_{x\in M}I_x\times\{x\}
        \subset \R\times M
    \end{equation*}
    \item \textbf{Flow} (of a differential equation): The map from $W$ to $M$ which pairs every starting point $x$ and time $t$ to the point to which the differential equation will have moved $x$ after time $t$ has elapsed. \emph{Denoted by} $\bm{\Phi}$. \emph{Given by}
    \begin{equation*}
        (t,x) \mapsto \phi(t,x)
    \end{equation*}
    where $\phi(t,x)$ is the maximal integral curve at $x$.
    \item Notation: We sometimes write
    \begin{equation*}
        \Phi(t,x) = \Phi_x(t) = \Phi_t(x)
    \end{equation*}
    \item If $\phi(\cdot)$ is the maximal integral curve at $x$, then $\phi(\cdot +s)$ is the maximal integral curve at $y=\phi(x)$ and $I_x=s+I_y$. It follows that for all $x\in M$ and $s\in I_x$, we have
    \begin{equation*}
        \Phi(s+t,x) = \Phi(t,\Phi(s,x))
    \end{equation*}
    for all $t\in I_{\Phi(s,x)}=I_x-s$.
    \item We now state formally the ideas we've just developed informally.
    \item Theorem 6.1: Suppose $f\in C^k(M,\R^n)$. For all $x\in M$, there exists an interval $I_x\subset\R$ containing 0 and a corresponding unique maximal integral curve $\Phi(\cdot,x)\in C^k(I_x,M)$ at $x$. Moreover, the set $W$ defined as above is open and $\Phi\in C^k(W,M)$ is a (local) flow on $M$, that is,
    \begin{gather*}
        \Phi(0,x) = x\\
        \Phi(t+s,x) = \Phi(t,\Phi(s,x)),\quad
            x\in M,\quad
            x,t+s\in I_x
    \end{gather*}
    \begin{proof}
        Given.
    \end{proof}
    \item Example: Let $M=\R$ and $f(x)=x^3$. Then $W=\{(t,x)\mid 2tx^2<1\}$\footnote{This condition is equivalent to all $(t,x)$ such that $1-2x^2t>0$, i.e., that the denominator of the flow is positive.} and
    \begin{equation*}
        \Phi(t,x) = \frac{x}{\sqrt{1-2x^2t}}
    \end{equation*}
    We have $T_-(x)=-\infty$ and $T_+(x)=1/(2x^2)$.
    \item \textbf{Fixed point}: A point at which $f$ evaluates to 0. \emph{Denoted by} $\bm{x_0}$.
    \item Lemma 6.2 (Straightening out vector fields): Suppose $f(x_0)\neq 0$. Then there is a local coordinate transform $y=\varphi(x)$ such that $\dot{x}=f(x)$ is transformed to
    \begin{equation*}
        \dot{y} = (1,0,\dots,0)
    \end{equation*}
\end{itemize}


\subsection*{Section 6.3: Orbits and Invariant Sets}
\begin{itemize}
    \item Some of the definitions herein come up in class, some do not, but many are interesting and IMO grant a deeper understanding of dynamical systems.
    \item \textbf{Orbit} (of $x$): The image under the flow of the maximal interval of the maximal integral curve at $x$. \emph{Denoted by} $\bm{\gamma(x)}$. \emph{Given by}
    \begin{equation*}
        \gamma(x) = \Phi(I_x\times\{x\})
    \end{equation*}
    \item $y\in\gamma(x)$ implies $y=\Phi(t,x)$ for some $t$, and hence (by Theorem 6.1) $\gamma(x)=\gamma(y)$.
    \item Implication: Distinct orbits are disjoint.
    \begin{itemize}
        \item Formalism: The orbits partition $M$, i.e., we have an equivalence relation on $M$ defined by $x\sim y$ iff $\gamma(x)=\gamma(y)$.
    \end{itemize}
    \item \textbf{Fixed point} (of $\Phi$): A point $x\in M$ for which $\gamma(x)=\{x\}$. \emph{Also known as} \textbf{singular point}, \textbf{stationary point}, \textbf{equilibrium point}.
    \item \textbf{Regular point} (of $\Phi$): A point $x\in M$ that is not a fixed point of $\Phi$.
    \item If $x$ is a regular point, then $\Phi(\cdot,x):I_x\hookrightarrow M$\footnote{Notation: $\hookrightarrow$ indicates an injective function.}.
    \item \textbf{Forward} (orbit of $x$): The image under the flow of the \emph{positive portion} of the maximal interval of the maximal integral curve at $x$. \emph{Denoted by} $\bm{\gamma_+(x)}$. \emph{Given by}
    \begin{equation*}
        \gamma_+(x) = \Phi((0,T_+(x))\times\{x\})
    \end{equation*}
    \item \textbf{Backward} (orbit of $x$): The image under the flow of the \emph{negative portion} of the maximal interval of the maximal integral curve at $x$. \emph{Denoted by} $\bm{\gamma_-(x)}$. \emph{Given by}
    \begin{equation*}
        \gamma_-(x) = \Phi((T_-(x),0)\times\{x\})
    \end{equation*}
    \item Relating the orbit, forward orbit, and backward orbit:
    \begin{equation*}
        \gamma(x) = \gamma_-(x)\cup\{x\}\cup\gamma_+(x)
    \end{equation*}
    \item \textbf{Periodic point} (of $\Phi$): A point $x\in M$ for which there exists $T>0$ such that $\Phi(T,x)=x$.
    \item \textbf{Period} (of a periodic point $x$): The lower bound on the set of $T$ for which $\Phi(T,x)=x$. \emph{Denoted by} $\bm{T(x)}$. \emph{Given by}
    \begin{equation*}
        T(x) = \inf\{T>0\mid\Phi(T,x)=x\}
    \end{equation*}
    \item The continuity of $\Phi$ guarantees that
    \begin{equation*}
        \Phi(T(x),x) = x
    \end{equation*}
    for $T(x)$ as defined.
    \item By the flow property (Theorem 6.1), we have
    \begin{equation*}
        \Phi(t,T(x),x) = \Phi(t,x)
    \end{equation*}
    \item \textbf{Periodic orbit}: An orbit for which one point (hence all points) of the orbit is/are periodic. \emph{Also known as} \textbf{closed orbit}.
    \begin{itemize}
        \item Reason for the moniker "closed orbit:" $x$ is periodic iff $\gamma_+(x)\cap\gamma_-(x)\neq\emptyset$, i.e., if the forward orbit joins the negative orbit and "closes" the loop.
    \end{itemize}
    \item Classification of the orbits of $f$:
    \begin{enumerate}
        \item Fixed orbits (corresponding to a periodic point with period zero).
        \item Regular periodic orbits (corresponding to a periodic point with positive period).
        \item Non-closed orbits (not corresponding to a periodic point).
    \end{enumerate}
    \item \textbf{Positive lifetime} (of $x$): The positive ending limit point of the maximal interval of $x$. \emph{Denoted by} $\bm{T_+(x)}$. \emph{Given by}
    \begin{equation*}
        T_+(x) = \sup I_x
    \end{equation*}
    \item \textbf{Negative lifetime} (of $x$): The negative ending limit point of the maximal interval of $x$. \emph{Denoted by} $\bm{T_-(x)}$. \emph{Given by}
    \begin{equation*}
        T_-(x) = \inf I_x
    \end{equation*}
    \item \textbf{$\bm{\sigma}$ complete} (point): A point $x\in M$ for which $T_\sigma(x)=\sigma\infty$, where $\sigma\in\{\pm\}$.
    \item \textbf{Complete} (point): A point $x\in M$ that is both $+$ and $-$ complete.
    \item Lemma 6.3: Let $x\in M$ and suppose that the forward (resp. backward) orbit lies in a compact subset $C$ of $M$. Then $x$ is $+$ (resp. $-$) complete.
    \item Periodic points are complete.
    \item \textbf{Complete} (vector field): A vector field in which all points are complete.
    \item $f$ complete implies $\Phi$ is globally defined, that is, $W=\R\times M$.
    \item \textbf{$\bm{\sigma}$ invariant}: A set $U\subset M$ such that $\gamma_\sigma(x)\subset U$ for all $x\in U$, where $\sigma\in\{\pm\}$.
    \item $C\subset M$ a compact $\sigma$ invariant set implies (by Lemma 6.3) that all points in $C$ are $\sigma$ complete.
    \item Lemma 6.4:
    \begin{enumerate}
        \item Arbitrary intersections and unions of $\sigma$ invariant sets are $\sigma$ invariant. Moreover, the closure of a $\sigma$ invariant set is again $\sigma$ invariant.
        \item If $U,V$ are invariant, so is the complement $U\setminus V$.
    \end{enumerate}
    \begin{proof}
        Given.
    \end{proof}
    \item Goal: Describe the long-term asymptotics of solutions.
    \begin{itemize}
        \item Tool: We introduce the set where an orbit eventually accumulates.
    \end{itemize}
    \item \textbf{$\bm{\omega_\pm}$-limit set} (of $x$): The set of all points $y\in M$ for which there exists a sequence $\{t_n\}$ that converges to $\pm\infty$ and satisfies $\Phi(t_n,x)\to y$. \emph{Denoted by} $\bm{\omega_\pm(x)}$.
    \item By definition, $\omega_\pm(x)$ is empty unless $x$ is $\pm$ complete.
    \item $y\in\gamma(x)$ implies $\omega_\pm(x)=\omega_\pm(y)$.
    \begin{itemize}
        \item This is because the hypothesis shows that $y=\Phi(t,x)$ for some $t$, so
        \begin{equation*}
            \Phi(t_n,x) = \Phi(t_n-t,\Phi(t,x))
            = \Phi(t_n-t,y)
        \end{equation*}
        \item Hence, $\omega_\pm(x)$ depends only on the orbit $\gamma(x)$.
    \end{itemize}
    \item Lemma 6.5: The set $\omega_\pm(x)$ is a closed invariant set.
    \begin{proof}
        Given.
    \end{proof}
    \item Example: For $\dot{x}=-x$, $\omega_+(x)=\{0\}$ for all $x\in\R$ since every solution converges to 0 as $t\to +\infty$. Moreover, $\omega_-(x)=\emptyset$ for $x\neq 0$ and $\omega_-(0)=\{0\}$.
    \item Conclusion: Even for $x$ complete, the set $\omega_\pm(x)$ can be empty.
    \item Lemma 6.6: If $\gamma_\sigma(x)$ is contained in a compact set $C$, then $\omega_\sigma(x)$ is nonempty, compact, and connected.
    \begin{proof}
        Given.
    \end{proof}
    \item Lemma 6.7: Suppose $\gamma_\sigma(x)$ is contained in a compact set. Then we have
    \begin{equation*}
        \lim_{t\to\sigma\infty}d(\phi(t,x),\omega_\sigma(x)) = 0
    \end{equation*}
    \begin{proof}
        Given.
    \end{proof}
    \item \textcite{bib:Teschl} works through an example that proves that the compactness requirement is necessary.
    \item \textbf{Minimal} (set): A nonempty, compact, $\sigma$ invariant set that contains no proper $\sigma$ invariant subset possessing these three properties.
    \item Examples:
    \begin{itemize}
        \item The $\omega_\pm$-limit sets are minimal for all $x\in\omega_\pm(x)$.
        \item A periodic orbit.
        \begin{itemize}
            \item In 2D, this is the only example by Corollary 7.12.
            \item In three or more dimensions, orbits can be dense on a compact hypersurface, meaning that the hypersurface cnnot be split into smaller \emph{closed} invariant sets.
        \end{itemize}
    \end{itemize}
    \item Lemma 6.8: Every nonempty, compact $\sigma$ invariant set $C\subset M$ contains a minimal $\sigma$ invariant set.\par
    If in addition $C$ is homeomorphic to a closed $m$-dimensional disk (where $m$ is not necessarily the dimension of $M$), it contains a fixed point.
    \begin{proof}
        Given.
    \end{proof}
\end{itemize}


\subsection*{Section 6.4: The Poincar\'{e} Map}
\begin{itemize}
    \item Never covered in class.
\end{itemize}


\subsection*{Section 6.5: Stability of Fixed Points}
\begin{itemize}
    \item Herein, we continue investigating the long-term behavior of the dynamical system
    \begin{equation*}
        \dot{x} = f(x)
        ,\quad
        x(0) = x_0
    \end{equation*}
    \begin{itemize}
        \item In particular, we investigate whether or not a solution is \textbf{stable}.
    \end{itemize}
    \item \textbf{Stable} (fixed point): A fixed point $x_0$ of $f(x)$ such that for any given neighborhood $U(x_0)$, there exists another neighborhood $V(x_0)\subset U(x_0)$ such that any solution starting in $V(x_0)$ remains in $U(x_0)$ for all $t\geq 0$. \emph{Also known as} \textbf{Lyapunov stable}\footnote{\textcite{bib:Teschl} uses the alternate spelling "Liapunov;" I will continue using "Lyapunov" without further comment.}.
    \begin{itemize}
        \item If a solution remains in $U(x_0)$ for all $t\geq 0$, it remains in the compact set $\overline{U(x_0)}$ for all $t\geq 0$.
        \item Thus, by Lemma 6.3, said solution exists for all positive times.
    \end{itemize}
    \item \textbf{Unstable} (fixed point): A fixed point which is not stable.
    \item \textbf{Asymptotically stable} (fixed point): A fixed point $x_0$ of $f(x)$ that is stable and for which there exists a neighborhood $U(x_0)$ such that
    \begin{equation*}
        \lim_{t\to\infty}|\phi(t,x)-x_0| = 0
    \end{equation*}
    for all $x\in U(x_0)$.
    \item \textbf{Exponentially stable} (fixed point): A fixed point $x_0$ of $f(x)$ for which there exist constants $\alpha,\delta,C>0$ such that
    \begin{equation*}
        |\phi(t,x)-x_0| \leq C\e[-\alpha t]|x-x_0|
    \end{equation*}
    when $|x-x_0|\leq\delta$.
    \item Exponential stability implies both stability and asymptotic stability.
    \item Example: Consider $\dot{x}=ax$ in $\R^1$. Then $x_0=0$ is stable iff $a\leq 0$ and exponentially stable iff $a<0$.
    \item These definitions of stability agree with those we introduced for linear autonomous systems in Section 3.2.
    \item \textcite{bib:Teschl} goes over an alternate stability criterion adapted from Section 1.5.
    \item If $f'(x_0)\neq 0$, the stability of $x_0$ can be read off from the derivative of $f$ at $x_0$ alone.
    \begin{itemize}
        \item More generally, a fixed point is exponentially stable if this is true for the corresponding linearized system (the proof is not directly presented in \textcite{bib:Teschl} but is rather spread out, making it not of much use to me rn).
    \end{itemize}
    \item Theorem 6.10 (Exponential stability via linearization): Suppose $f\in C^1$ has a fixed point $x_0$ and suppose that all eigenvalues of the Jacobian matrix at $x_0$ have negative real part. Then $x_0$ is exponentially stable.
    \item \textbf{Bifurcation theory}: The systematic study of small changes in an ODEs parameters that induce large changes in qualitative behavior.
    \begin{itemize}
        \item Theorem 2.11 asserts that provided $f$ depends smoothly on $\mu$, so does the flow. Nevertheless, very small changes in parameters can induce large changes in the qualitative behavior.
        \item A few examples follow.
    \end{itemize}
    \item \textbf{Pitchfork bifurcation}: A stable fixed point for $\mu\leq 0$ which becomes unstable and splits off two stable fixed points at $\mu=0$.
    \emph{picture}
    \begin{itemize}
        \item Example: $\dot{x}=\mu x-x^3$.
    \end{itemize}
    \item \textbf{Transcritical bifurcation}: Two stable fixed points for $\mu\neq 0$ which collide and exchange stability at $\mu=0$.
    \emph{picture}
    \begin{itemize}
        \item Example: $\dot{x}=\mu x-x^2$.
    \end{itemize}
    \item \textbf{Saddle-node bifurcation}: One stable and one unstable fixed point for $\mu<0$ which collide at $\mu=0$ and vanish.
    \emph{picture}
    \begin{itemize}
        \item Example: $\dot{x}=\mu+x^2$.
    \end{itemize}
    \item Rest of the chapter: Good criteria for the stability of $\dot{x}=f(x)$ (since it cannot be solved explicitly in general).
\end{itemize}


\subsection*{Section 6.6: Stability via Lyapunov's Method}
\begin{itemize}
    \item For a fixed point $x_0$ of $f$ and an open neighborhood $U(x_0)$ of $x_0$, we may define the following.
    \item \textbf{Lyapunov function}: A continuous function $L:U(x_0)\to\R$ which is zero at $x_0$, positive for $x\neq x_0$, and satisfies
    \begin{equation*}
        L(\phi(t_0)) \geq L(\phi(t_1))
    \end{equation*}
    where $t_0<t_1$ and $\phi(t_j)\in U(x_0)\setminus\{x_0\}$ ($j=0,1$) for any solution $\phi(t)$.
    \item \textbf{Strict Lyapunov function}: A Lyapunov function for which the central inequality in the above definition is strict.
    \item Claim: If $L$ is strict, $U(x_0)\setminus\{x_0\}$ cannot contain any periodic orbits.
    \begin{proof}
        Suppose for the sake of contradiction that $\gamma(x)\subset U(x_0)\setminus\{x_0\}$. Since $\gamma(x)$ is a periodic orbit, $\phi(0,x)=\phi(T(x),x)$ where $T(x)>0$ by definition. Letting $t_0=0$ and $t_1=T(x)$, we have by the definition of a strict Lyapunov function that
        \begin{equation*}
            L(\phi(0,x)) > L(\phi(T(x),x))
            = L(\phi(0,x))
        \end{equation*}
        contradicting the fact that $L$ is well-defined.
    \end{proof}
    \item $\bm{S_\delta}$: The following set. \emph{Given by}
    \begin{equation*}
        S_\delta = \{x\in U(x_0)\mid L(x)\leq\delta\}
    \end{equation*}
    \begin{itemize}
        \item $S_\delta$ contains $x_0$.
        \item In general, $S_\delta$ need not be closed since it can share boundary with $U(x_0)$. In such a case, orbits can escape through this part of the boundary.
        \item Restricting $S_\delta$ to closed versions, though, we get the following lemma.
    \end{itemize}
    \item Lemma 6.11: If $S_\delta$ is closed, then it is positively invariant.
    \item Lemma 6.12: For every $\delta>0$, there is an $\varepsilon>0$ such that $S_\varepsilon\subset B_\delta(x_0)$ and $B_\varepsilon(x_0)\subset S_\delta$.
    \item Implication: Given any neighborhood $V(x_0)$, we can find an $\varepsilon$ such that $S_\varepsilon\subset V(x_0)$ is positively invariant. But this just means that $x_0$ is stable, and we have proven the following\footnote{Clever pedagogical tool: \textcite{bib:Teschl} weaves any really important proofs into the flow of the text so that you can't gloss over it.}.
    \item Theorem 6.13 (Lyapunov): Suppose $x_0$ is a fixed point of $f$. If there is a Lyapunov function $L$, then $x_0$ is stable.
    \item Theorem 6.14 (Krasovskii-LaSalle principle): Suppose $x_0$ is a fixed point of $f$. If there is a Lyapunov function $L$ which is not constant on any orbit lying entirely in $U(x_0)\setminus\{x_0\}$, then $x_0$ is asymptotically stable. This is for example the case if $L$ is a strict Lyapunov function. Moreover, every orbit lying entirely in $U(x_0)$ converges to $x_0$.
    \item Theorem 6.15: Let $L:U\to\R$ be continuous and bounded from below. If for some $x$ we have $\gamma_+(x)\subset U$ and
    \begin{equation*}
        L(\phi(t_0,x)) \geq L(\phi(t_1,x))
    \end{equation*}
    for $t_0<t_1$, then $L$ is constant on $\omega_+(x)\cap U$.
    \item Most Lypunov functions are differentiable.
    \item If $L$ is differentiable, then $L(\phi(t_0))\geq L(\phi(t_1))$ for all $t_0<t_1$ iff
    \begin{equation*}
        \dv{t}L(\phi(t,x)) = \nabla(L)(\phi(t,x))\dot{\phi}(t,x)
        = \nabla(L)(\phi(t,x))f(\phi(t,x))
        \leq 0
    \end{equation*}
    \item \textbf{Lie derivative} (of $L$ along $f$): The following expression. \emph{Given by}
    \begin{equation*}
        \nabla(L)(x)\cdot f(x)
    \end{equation*}
    \item \textbf{Constant of motion}: A function for which the Lie derivative vanishes and, hence, is constant on every orbit.
    \item Theorem 6.15 implies that all $\omega_\pm$-limit sets are contained in the set where the Lie derivative of $L$ vanishes.
    \item Example: Consider the system
    \begin{align*}
        \dot{x} &= y&
        \dot{y} &= -x
    \end{align*}
    with function
    \begin{equation*}
        L(x,y) = x^2+y^2
    \end{equation*}
    \begin{itemize}
        \item For $x\in\R^2$ arbitrary, the Lie derivative is
        \begin{equation*}
            \nabla(L)(x)\cdot f(x) =
            \begin{pmatrix}
                2x\\
                2y\\
            \end{pmatrix}
            \cdot
            \begin{pmatrix}
                y\\
                -x\\
            \end{pmatrix}
            = 2xy-2xy
            = 0
        \end{equation*}
        \item Thus, $L$ is a Lyapunov function.
        \item In particular, $L$ is a constant of motion.
        \item Thus, by Theorem 6.13, the origin is stable.
        \item Every level set $L(x,y)=\delta$ corresponds to an orbit, so the system is not asymptotically stable.
    \end{itemize}
    \item Takeaway:
    \begin{itemize}
        \item Extract properties of Lyapunov functions from the fact that they are monotonically decreasing on all orbits.
        \item Prove that a function is a Lyapunov function using the Lie derivative.
    \end{itemize}
\end{itemize}


\subsection*{Section 6.7: Newton's Equation in One Dimension}
\begin{itemize}
    \item Goal: Illustrate the results of Chapter 6 with a specific example.
    \item Recall that the motion of a particle moving in one dimension under the external force field $f(x)$ is described by Newton's equation
    \begin{equation*}
        \ddot{x} = f(x)
    \end{equation*}
    \item \textbf{Phase space}: The set $\R^2$, as referred to by physicists.
    \item \textbf{Phase point}: A point of the form $(x,\dot{x})$ in the phase space.
    \item \textbf{Phase curve}: A solution to the ODE.
    \item By the Picard-Lindel\"{o}f theorem (Theorem 2.2), precisely one phase curve passes through each phase point.
    \item \textbf{Kinetic energy}: The following quadratic form. \emph{Denoted by} $\bm{T(\dot{x})}$. \emph{Given by}
    \begin{equation*}
        T(\dot{x}) = \frac{\dot{x}^2}{2}
    \end{equation*}
    \item \textbf{Potential energy}: The following function. \emph{Denoted by} $\bm{U(x)}$. \emph{Given by}
    \begin{equation*}
        U(x) = -\int_{x_0}^xf(\xi)\dd\xi
    \end{equation*}
    \begin{itemize}
        \item Only determined up to an arbitrary constant.
    \end{itemize}
    \item \textbf{Energy} (of a Newtonian system): The sum of the kinetic and potential energies. \emph{Denoted by} $\bm{E}$. \emph{Given by}
    \begin{equation*}
        E = T(\dot{x})+U(x)
    \end{equation*}
    \begin{itemize}
        \item $E$ is constant along a phase curve. Indeed, if $x(t)$ satisfies $\ddot{x}=f(x)=U'(x)$, i.e., $\ddot{x}-f(x)=0$, then
        \begin{equation*}
            \dv{E}{t} = \dot{x}\ddot{x}+U'(x)\dot{x}
            = \dot{x}(\ddot{x}-f(x))
            = 0
        \end{equation*}
        as desired.
    \end{itemize}
    \item The solution corresponding to the initial conditions $x(0)=x_0$, $\dot{x}(0)=x_1$ can be given implicitly.
    \begin{itemize}
        \item First off, we have that
        \begin{align*}
            E &= T(\dot{x})+U(x)\\
            &= \frac{\dot{x}^2}{2}+U(x)\\
            \sqrt{2(E-U(x))} &= \dv{x}{t}\\
            \int_0^t\dd\tau &= \int_{x_0}^x\frac{\dd\xi}{\sqrt{2(E-U(\xi))}}\\
            t &= \sign(x_1)\int_{x_0}^x\frac{\dd\xi}{\sqrt{2(E-U(\xi))}}
        \end{align*}
        \begin{itemize}
            \item Why do we input $\sign(x_1)$ between the next-to-last and last lines??
        \end{itemize}
        \item Second, since $E$ is constant along the solution, its value at the start will be the same as its value at any other point. Thus, we can use the starting initial conditions to calculate it, as follows.
        \begin{equation*}
            E = \frac{{x_1}^2}{2}+U(x_0)
        \end{equation*}
        \item If $x_1=0$, then $\sign(x_1)$ must be replaced with $-\sign(U'(x_0))$.
    \end{itemize}
    \item Theorem 6.16: Newton's equation has a fixed point if and only if $\dot{x}=0$ and $U'(x)=0$ at this point. Moreover, a fixed point is stable if $U(x)$ has a local minimum there.
    \begin{itemize}
        \item If $U(x)$ has a local minimum at $x_0$, the energy $E-U(x_0)$ can be used as a Lyapunov function; we subtract $U(x_0)$ so that $E-U(x_0)$ evaluates to zero at $x_0$.
    \end{itemize}
    \item A fixed point cannot be asymptotically stable (due to conservation of energy).
    \item Example: Mathematical pendulum.
    \begin{equation*}
        \ddot{x} = -\sin(x)
    \end{equation*}
    \begin{itemize}
        \item $x$ describes the displacement angle from the position at rest ($x=0$).
        \item $x$ should be understood modulo $2\pi$.
        \item We have that
        \begin{equation*}
            U(x) = -\int_{x_0}^x(-\sin(\xi))\dd\xi
            = \cos(x_0)-\cos(x)
        \end{equation*}
        \begin{itemize}
            \item Since the constant is arbitrary, we may take $U(x)=1-\cos(x)$ for ease. This has the additional advantage that the energy is never negative.
        \end{itemize}
        \item We now begin the rigorous investigation.
        \item We restrict our attention to the interval $x\in(-\pi,\pi]$. Thus, the fixed points are $x=0,\pi$.
        \item By Theorem 6.16 and the fact that $U(0)$ is a minimum, 0 is a stable fixed point.
        \item As before in the general case, $E(x,\dot{x})=\text{constant}$ gives invariant level sets.
        \begin{itemize}
            \item $E=0$: The corresponding level set is the equilibrium position $(x,\dot{x})=(0,0)$.
            \item $0<E<2$: The level sets are homeomorphic to circles. Since these circles contains no fixed points, they are regular periodic orbits.
            \item $E=2$: The level set consists of the fixed point $\pi$ and two non-closed orbits connecting $-\pi$ and $\pi$. This is a \textbf{separatrix}.
            \item $E>2$: The level sets are again closed orbits (due to our modulo $2\pi$ perspective).
        \end{itemize}
        \item In a neighborhood of the equilibrium position $x=0$, the system is approximated by the linearization $\sin(x)=x+O(x^2)$ given by
        \begin{equation*}
            \ddot{x} = -x
        \end{equation*}
        and referred to as the \textbf{harmonic oscillator}.
        \begin{itemize}
            \item Here, we have
            \begin{equation*}
                E = \frac{\dot{x}^2}{2}+\frac{x^2}{2}
            \end{equation*}
            so the phase portrait consists of circle centered at 0.
        \end{itemize}
        \item More generally, if $U'(x_0)=0$ and $U''(x_0)=\omega^2/2>0$, then we should approximate our system with
        \begin{equation*}
            \ddot{y} = -\omega^2y
            ,\quad
            y(t) = x(t)-x_0
        \end{equation*}
        \item Lastly, if we use the momentum $p=\dot{x}$ (units chosen such that $m=1$) and the location $q=x$ as coordinates, then the energy
        \begin{equation*}
            H(p,q) = \frac{p^2}{2}+U(q)
        \end{equation*}
        is called the \textbf{Hamiltonian}.
        \begin{itemize}
            \item In this case, the equations of motion are
            \begin{align*}
                \dot{q} &= \pdv{H}{p}&
                \dot{p} &= -\pdv{H}{q}
            \end{align*}
            \item This formalism is called \textbf{Hamiltonian mechanics}.
            \item It is useful for systems with more than one degree of freedom.
            \item See Section 8.3 for more.
        \end{itemize}
    \end{itemize}
\end{itemize}



\section{Chapter 7: Planar Dynamical Systems}
\emph{From \textcite{bib:Teschl}.}
\subsection*{Section 7.1: Examples from Ecology}
\begin{itemize}
    \item \textcite{bib:Teschl} derives via ecological reasoning the \textbf{Lotka-Volterra} predator-prey equations.
    \item \textbf{Lotka-Volterra predator-prey equations}: The following system of differential equations. \emph{Given by}
    \begin{align*}
        \dot{x} &= (1-y)x&
        \dot{y} &= \alpha(x-1)y
    \end{align*}
    for $\alpha>0$.
    \item Two fixed points.
    \begin{itemize}
        \item $(0,0)$ gives rise to invariant subspaces along the $x$- and $y$-axes. Indeed,
        \begin{align*}
            \Phi(t,(0,y)) &= (0,y\e[-\alpha t])&
            \Phi(t,(x,0)) &= (x\e[t],0)
        \end{align*}
        \begin{itemize}
            \item Since no other solution can cross these lines, the first quadrant $Q=\{(x,y)\mid x,y>0\}$ is invariant. This is the region we are interested in.
        \end{itemize}
        \item $(1,1)$ is the other fixed point.
        \begin{itemize}
            \item Let's eliminate $t$ from the ODEs to get a single first-order equation for the orbits.
            \item Writing $y=y(x)$, we infer from the chain rule that
            \begin{equation*}
                \dv{y}{x} = \dv{y}{t}\left( \dv{x}{t} \right)^{-1}
                = \alpha\frac{(x-1)y}{(1-y)x}
            \end{equation*}
            \item This equation is separable. Solving it yields
            \begin{equation*}
                L(x,y) = f(y)+\alpha f(x) = \text{constant}
            \end{equation*}
            where
            \begin{equation*}
                f(a) = a-1-\log(a)
            \end{equation*}
            \begin{itemize}
                \item Note that $\log$ denotes the natural logarithm.
            \end{itemize}
            \item $f$ cannot be inverted in terms of elementary functions. However, $f$ is convex with global minimum at $x=1$, and $f\to\infty$ as $a\to 0,+\infty$. It follows that the level sets are portions of this curve near the bottom of the well in both dimensions, and thus they are compact.
            \item The exchange of energy from one to the other and back again also indicates that each orbit is periodic surrounding the fixed point $(1,1)$.
        \end{itemize}
    \end{itemize}
    \item Theorem 7.1: All orbits of the Lotka-Volterra equations in $Q$ are closed and encircle the only fixed point $(1,1)$.
    \item Modification: Let's assume each species' population can only grow so fast. Then we get
    \begin{align*}
        \dot{x} &= (1-y-\lambda x)x&
        \dot{y} &= \alpha(x-1-\mu y)y
    \end{align*}
    for $\alpha,\lambda,\mu>0$.
    \item We now have four fixed points:
    \begin{align*}
        (0,0)&&
        (\lambda^{-1},0)&&
        (0,-\mu^{-1})&&
        \left( \frac{1+\mu}{1+\mu\lambda},\frac{1-\lambda}{1+\mu\lambda} \right)
    \end{align*}
    \begin{itemize}
        \item The third lies outside of $\bar{Q}$, so we disregard it.
        \item The fourth lies outside of $\bar{Q}$ if $\lambda>1$. Thus, let's start with the case $\lambda\geq 1$ so that we only have to deal with one new fixed point.
        \item $\lambda\geq 1$.
        \emph{picture}
        \begin{itemize}
            \item Our new fixed point is $(\lambda^{-1},0)$.
            \item It is a hyperbolic sink if $\lambda>1$.
            \item If $\lambda=1$, one eigenvalue is 0 and we need a more thorough investigation.
            \item Idea: Split $Q$ into regions where $\dot{x},\dot{y}$ have definite signs and then use the elementary observation in Lemma 7.2.
            \item The regions where $\dot{x},\dot{y}$ have definite signs are separated by the two lines
            \begin{align*}
                L_1 &= \{(x,y)\mid y=1-\lambda x\}&
                L_2 &= \{(x,y)\mid\mu y=x-1\}
            \end{align*}
            \begin{itemize}
                \item We derive these by setting $1-y-\lambda x=0$ and $x-1-\mu y=0$.
            \end{itemize}
            \item Label the regions in $Q$ enclosed by these lines from left to right by $Q_1,Q_2,Q_3$.
            \item Observe that the lines are transversal, i.e., can only be crossed in the direction from $Q_3\to Q_2$ and $Q_2\to Q_1$. This can be seen from the solution curves in the picture.
            \item Suppose we start at $(x_0,y_0)\in Q_3$.
            \begin{itemize}
                \item Additional constraint: $x\leq x_0$ (the flow is to the left??).
                \item By Lemma 7.2: Either the trajectory enters $L_2$ or it converges to a fixed point in $\bar{Q}_3$. The latter case can only happen if $(\lambda^{-1},0)\in\bar{Q}_3$, i.e., if $\lambda=1$.
            \end{itemize}
            \item Similarly, starting in $Q_2$ either gets you across $L_1$ or to $(\lambda^{-1},0)$.
            \item Starting in $Q_1$ must take you to the fixed point.
            \item Thus, every trajectory converges to the fixed point.
        \end{itemize}
        \item Let $0<\lambda<1$.
        \begin{itemize}
            \item We apply the same strategy as before.
            \item We have four regions this time. Let $Q_4$ be the new (bottom) one. We can only pass through these in the order $Q_4\to Q_3\to Q_2\to Q_1\to Q_4$.
            \item Thus, we have to rule out the periodic case this time.
            \item For simplicity's sake, let
            \begin{equation*}
                (x_0,y_0) = \left( \frac{1+\mu}{1+\mu\lambda},\frac{1-\lambda}{1+\mu\lambda} \right)
            \end{equation*}
            \item To do so, introduce (inspired by the original) the Lyapunov function
            \begin{equation*}
                L(x,y) = \gamma_1f(\tfrac{y}{y_0})+\alpha\gamma_2f(\tfrac{x}{x_0})
            \end{equation*}
            where, as before, $f(a)=a-1-\log(a)$.
            \item We seek constraints on $\gamma_1,\gamma_2$ that will make $L$ strict.
            \item Calculate
            \begin{equation*}
                \dot{L} = \pdv{L}{x}\dot{x}+\pdv{L}{y}\dot{y}
                = -\alpha\left( \frac{\lambda\gamma_2}{x_0}\bar{x}^2+\frac{\mu\gamma_1}{y_0}\bar{y}^2+\left( \frac{\gamma_2}{x_0}-\frac{\gamma_1}{y_0} \right)\bar{x}\bar{y} \right)
            \end{equation*}
            where
            \begin{align*}
                \dot{x} &= (-\bar{y}-\lambda\bar{x})x&
                \dot{y} &= \alpha(\bar{x}-\mu\bar{y})y&
                \bar{x} &= x-x_0&
                \bar{y} &= y-y_0
            \end{align*}
            \item The the RHS will be negative if we choose $\gamma_1=y_0$ and $\gamma_2=x_0$, so choose this, and then $L$ is strictly decreasing, so all orbits starting in $Q$ converge to the fixed point $(x_0,y_0)$.
        \end{itemize}
    \end{itemize}
    \item Lemma 7.2: Let $\phi(t)=(x(t),y(t))$ be the solution of a planar system. Suppose $U$ is open and $\bar{U}$ is compact. If $x(t),y(t)$ are strictly monotone in $U$, then either $\phi$ hits the boundary at some finite time $t=t_0$ or $\phi(t)$ converges to a fixed point $(x_0,y_0)\in\bar{U}$.
    \item Therefore, after all of that, we have proven the following.
    \item Theorem 7.3: Suppose $\gamma\geq 1$. Then there is no fixed point of
    \begin{align*}
        \dot{x} &= (1-y-\lambda x)x&
        \dot{y} &= \alpha(x-1-\mu y)y
    \end{align*}
    in $Q$ and all trajectories in $Q$ converge to the point $(\lambda^{-1},0)$.\par
    If $0<\lambda<1$, then there is only one fixed point $(\frac{1+\mu}{1+\mu\lambda},\frac{1-\lambda}{1+\mu\lambda})$ in $Q$. It is asymptotically stable and all trajectories in $Q$ converge to this point.
    \item Ecological interpretation: Predators can only survive if their growth rate is positive at the limiting population $\lambda^{-1}$ of the prey species.
    \item \textcite{bib:Teschl} discusses cooperative and competing species.
\end{itemize}




\end{document}