\documentclass[../notes.tex]{subfiles}

\pagestyle{main}
\renewcommand{\chaptermark}[1]{\markboth{\chaptername\ \thechapter\ (#1)}{}}
\setcounter{chapter}{6}

\begin{document}




\chapter{Solution Existence and Stability}
\section{Peano Existence Theorem}
\begin{itemize}
    \item \marginnote{11/7:}Today: Peano Existence Theorem.
    \item For an IVP of a first-order differential system, as long as the RHS is continuous, we get at least one solution.
    \item The proof provides an algorithm that can be really useful in computing the solution provided that uniqueness exists.
    \item We will need a theorem from analysis to start.
    \item Theorem (Arzel\`{a}-Ascoli\footnote{This is not the full Arzel\`{a}-Ascoli theorem, but a special case. The proof is similar, regardless, though. See Honors Analysis in $\R^n$ I Notes.}): Let $h_k:[a,b]\to\R^n$ be a sequence of functions that is uniformly bounded and uniformly Lipschitz continuous wrt. $L$. Then $\{h_k\}$ contains a uniformly convergent subsequence and the limit has the same bound and Lipschitz constant.
    \begin{proof}
        Recall the property of sequential compactness\footnote{The Bolzano-Weierstrass Theorem/Theorem 15.18 from Honors Calculus IBL.}, i.e., that every bounded sequence of numbers contains a convergent subsequence. We want to prove this for a sequence of functions. To do so, we will need the Cantor diagonalization technique.\par
        $\Q$ is countable. Thus, we can enumerate the rationals in $[a,b]$ by $r_1,r_2,r_3,\dots$. Since $\{h_k(r_1)\}$ is a bounded sequence of numbers, we have by the above that there is a subsequence $C_1$ --- say $h_1^{(1)},h_2^{(1)},h_3^{(1)},\dots$ --- such that $C_1=\{h_k^{(1)}(r_1)\}$ is a convergent subsequence in $\R^n$ of the original sequence. Now $C_1$ is still a bounded sequence, so we can obtain a subsequence $C_2$ of \emph{it} --- say $h_1^{(2)},h_2^{(2)},h_3^{(2)},\dots$ --- such that $C_2=\{h_k^{(2)}(r_2)\}$ is a convergent subsequence in $\R^n$ at $r_2$ (and, by inductive hypothesis, at $r_1$!). Inductively, we can obtain $C_\ell=\{h_k^{(\ell)}\}_{\ell,k=1}^\infty$ convergent at $r_1,r_2,\dots,r_\ell$. We then write down the elements of the sequences as a table. (For example, the $k^\text{th}$ row of the table is a sequence that converges at $r_1,\dots,r_k$.)
        \begin{equation*}
            \begin{matrix}
                h_1^{(1)} & h_2^{(1)} & h_3^{(1)} & \cdots\\
                h_1^{(2)} & h_2^{(2)} & h_3^{(2)} & \cdots\\
                h_1^{(3)} & h_2^{(3)} & h_3^{(3)} & \cdots\\
                \vdots & \vdots & \vdots & \ddots\\
            \end{matrix}
        \end{equation*}
        Consider the diagonal sequence $\{f_\ell\}_{\ell=1}^\infty$ where $f_\ell=h_\ell^{(\ell)}$. By definition, it converges at all rational points. We now seek to prove that it converges uniformly at \emph{all} points.\par
        To prove that $\{f_\ell\}$ is a uniformly convergent sequence of functions, it will suffice to show that for all $\varepsilon>0$, there exists $N$ such that if $k,\ell>N$, then $|f_k(t)-f_\ell(t)|<\varepsilon$ for all $t\in[a,b]$. Let $\varepsilon>0$ be arbitrary. Divide $[a,b]$ into $m$ congruent subintervals $I_\alpha$ ($\alpha=1,\dots,m$) such that $|I_\alpha|\leq\varepsilon/3L$ for all $\alpha$. This guarantees that the oscillation of each $f_k$ on any $I_\alpha$ is $\leq\varepsilon/3$ since if $x,y\in I_\alpha$ for some $\alpha$, then
        \begin{equation*}
            |f_\ell(x)-f_\ell(y)| \leq L|x-y|
            \leq L\cdot\frac{\varepsilon}{3L}
            = \frac{\varepsilon}{3}
        \end{equation*}
        Using the fact that $\{f_\ell\}$ is convergent and hence Cauchy on the rationals, pick $N$ large enough so that $r_\alpha\in I_\alpha$ implies $|f_k(r_\alpha)-f_\ell(r_\alpha)|<\varepsilon/3$ for $k,\ell>N$. We will choose this $N$ to be our $N$. Now let $t\in[a,b]$ be arbitrary. By their definition, we know $t\in I_\alpha$ for some $\alpha$. Therefore,
        \begin{align*}
            |f_k(t)-f_\ell(t)| &\leq |f_k(t)-f_k(r_\alpha)|+|f_k(r_\alpha)-f_\ell(r_\alpha)|+|f_\ell(r_\alpha)-f_\ell(t)|\\
            &< \frac{\varepsilon}{3}+\frac{\varepsilon}{3}+\frac{\varepsilon}{3}\\
            &= \varepsilon
        \end{align*}
        as desired.\par
        Lastly, we can prove that the limit function $f$ of $\{f_\ell\}$ is $L$-Lipschitz as follows. Let $t,t'\in[a,b]$ be arbitrary. Then
        \begin{equation*}
            \left| \frac{f(t)-f(t')}{t-t'} \right| = \lim_{k\to\infty}\left| \frac{f_k(t)-f_k(t')}{t-t'} \right|
            \leq \lim_{k\to\infty}\left| \frac{L|t-t'|}{t-t'} \right|
            = \lim_{k\to\infty}|L|
            = L
        \end{equation*}
        as desired.
    \end{proof}
    \item Now we come to the proof of the Peano Existence Theorem.
    \item Theorem (Peano Existence Theorem): Let $f:[t_0,t_0+a]\times\bar{B}(y_0,b)\to\R^n$ be bounded ($|f(t,z)|\leq M$) and continuous. Then the IVP
    \begin{equation*}
        y'(t)=f(t,y(t))
        ,\quad
        y(t_0)=b
    \end{equation*}
    has at least one solution for $t\in[t_0,t_0+T]$ where $T=\min(a,b/M)$.
    \begin{proof}
        % Fix $T=\min(a,b/M)$. Divide $[t_0,t_0+T]$ into $m$ congruent subintervals $I_\alpha$ ($\alpha=1,\dots,m$). Suppose that $m$ is large so that every subinterval is small. In particular, on each $I_\alpha$, we may replace $y'(t)$ with the difference quotient $h=|I_\alpha|=I/m$. Thus, let $y_m(t_{\alpha+1})=y_m(t_\alpha)+f(t_\alpha,y_m(t_\alpha))h$. That is, we define inductively the value of the solution as the value at the previous node plus the increment. Very similar to Euler's method. The second term is approximately equal to $f'(t_\alpha,y(t_\alpha))h$. In between the nodes, we assume that the function is linear. We connect all nodes by linear functions and say that the piecewise linear function is approximately equal to the solution. We want to use the Arzel\`{a}-Ascoli theorem to prove that something converges uniformly on this time interval. Fix a scalar of error $\varepsilon>0$. Let $f(t,z)$ be uniformly continuous for $(t,z)$, i.e., $|f(t,z)-f(t',z')|\leq\varepsilon/T$ when $|t-t'|+|z-z'|\leq\delta$.


        Since there is no Lipschitz condition, we use another strategy to find approximate solutions.\par
        \emph{picture}
        Fix $T=\min(a,b/M)$. We divide $[t_0,t_0+T]$ into $m$ congruent closed subintervals $I_\alpha$ ($\alpha=0,\dots,m-1$), each of length $h_m=T/m$. Define a continuous function $y_m(t)$ as follows: The values at the nodes $t_\alpha$ (the intersection points of adjacent congruent subintervals) are defined inductively via
        \begin{equation*}
            y_m(t_{\alpha+1}) = y_m(t_\alpha)+f(t_\alpha,y_m(t_\alpha))h_m
        \end{equation*}
        for $\alpha=0,\dots,m-1$, and $y_m$ is taken to be linear between the nodes\footnote{Note that this construction is quite similar to that employed in Euler's method.}. The idea is that we replace the derivative $y'(t)$ by the difference quotient $[y(t+h)-y(t)]/h$. It follows by the construction that every function in the set $\{y_k(t):[t_0,t_0+T]\to\bar{B}(y_0,b)\}$ is piecewise linear (hence continuous), uniformly bounded, and uniformly $M$-Lipschitz continuous. Therefore, by the Arzel\'{a}-Ascoli theorem, $\{y_k\}$ contains a uniformly convergent subsequence $y_{m_k}\to y$.\par
        It remains to verify that $y$ is a solution to the integral equation
        \begin{equation*}
            y(t) = y_0+\int_{t_0}^tf(\tau,y(\tau))\dd\tau
        \end{equation*}
        Observe that the domain of $f$ is a closed and bounded subset of the real numbers. Thus, it is compact by the Heine-Borel theorem\footnote{Theorem 10.16 of Honors Calculus IBL.}. Moreover, since $f$ is a continuous function on a compact domain, we have by the Heine-Cantor theorem\footnote{Theorem 13.6 of Honors Calculus IBL.} that $f$ is uniformly continuous. Thus, for any $\varepsilon>0$, there exists $N$ such that if $m>N$, then
        \begin{equation*}
            |f(t,y_m(t))-f(t_\alpha,y_m(t_\alpha))| < \frac{\varepsilon}{T}
        \end{equation*}
        for all $\alpha=0,\dots,m-1$ and $t\in I_\alpha$. Additionally, observe that
        \begin{equation*}
            y_{m_k}(t) = y_0+\sum_{\alpha=0}^{m-1}\int_{t_\alpha}^{t_{\alpha+1}}\chi_t(\tau)f(t_\alpha,y_{m_k}(t_\alpha))\dd\tau
        \end{equation*}
        where $\chi_t(\tau)$ denotes the \textbf{characteristic function} of $[t_0,t]$. To see this, compare with the original inductive definition of $y_m(t_{\alpha+1})$.
        \emph{picture}
        We thus see that $y_0$ in the above equation corresponds to $y_m(t_0)=y(t_0)$, as we would expect. We see that we are summing a series of side-by-side integrals so that in the end, we integrate over all of $[t_0,t_0+T]$. We see that the characteristic function restricts us to integrating over the ODE only up until $t$, as we would want for an approximation $y_{m_k}(t)$ at $t$ using Euler's method. And we see that since $f(t_\alpha,y_{m_k}(t_\alpha))$ is constant and $h_m=t_{\alpha+1}-t_\alpha$, the integral does take on the expected value $f(t_\alpha,y_m(t_\alpha))h_m$. Moving right along, we see that
        \begin{align*}
            \left| y_{m_k}(t)-y_0-\int_{t_0}^tf(\tau,y_{m_k}(\tau))\dd\tau \right| &\leq \sum_{\alpha=0}^{m-1}\int_{t_\alpha}^{t_{\alpha+1}}\chi_t(\tau)|f(t_\alpha,y_{m_k}(t_\alpha))-f(\tau,y_{m_k}(\tau))|\dd\tau\\
            &< \int_{t_0}^{t_0+T}\chi_t(\tau)\cdot\frac{\varepsilon}{T}\dd\tau\\
            &= \int_{t_0}^t\frac{\varepsilon}{T}\dd\tau\\
            &= \varepsilon\cdot\frac{t-t_0}{T}\\
            &\leq \varepsilon
        \end{align*}
        Thus, by uniform convergence, $\int_{t_0}^tf(\tau,y_{m_k}(\tau))\dd\tau\to\int_{t_0}^tf(\tau,y(\tau))\dd\tau$ uniformly, so $y$ does satisfy the integral equation, as desired.
    \end{proof}
    \item \textbf{Characteristic function} (of $[a,b]$): The function defined as follows. \emph{Denoted by} $\bm{\chi_{[a,b]}}$. \emph{Given by}
    \begin{equation*}
        \chi_{[a,b]}(t) =
        \begin{cases}
            1 & x\in[a,b]\\
            0 & x\notin[a,b]
        \end{cases}
    \end{equation*}
    \item Utility of the Peano Existence Theorem: Proves the \emph{existence} of a solution, but the proof is not constructive; it does not give an algorithm for finding the desired sequence. Nor does the PET make any statement on uniqueness.
    \item We now look to use a related method to define a sequence of functions that will converge to the desired solution of the ODE.
    \begin{itemize}
        \item While the PET does not require it, in practice, most $f$ we would be interested in will satisfy an additional Lipschitz condition.
        \item Define the integral operator
        \begin{equation*}
            \Phi[u] = y_0+\int_{t_0}^tf(\tau,u(\tau))\dd\tau
        \end{equation*}
        We will prove that $\Phi$ is a contraction on the function space. This will imply that $\Phi^N[u]$ converges across the entire interval $[t_0,t_0+T]$ to the solution $y$ for any $u:[t_0,t_0+T]\to\bar{B}(y_0,b)$, giving us our desired computational strategy. Let's begin.
        \item To prove that $\Phi$ is a contraction, it will suffice to show that $\norm{\Phi^j[u_1]-\Phi^j[u_2]}\to 0$ as $j\to\infty$. Thus, we wish to put a bound on $\norm{\Phi^j[u_1]-\Phi^j[u_2]}$ that decreases as $j$ increases. To that end, we will prove that
        \begin{equation*}
            \norm{\Phi^j[u_1]-\Phi^j[u_2]} \leq \frac{(LT)^j}{j!}\cdot\norm{u_1-u_2}
        \end{equation*}
        for all $j$.
        \begin{itemize}
            \item We induct on $j$. For the base case $j=1$, we have that
            \begin{align*}
                |\Phi[u_1](t)-\Phi[u_2](t)| &\leq \int_{t_0}^tL|u_1(\tau)-u_2(\tau)|\dd\tau\\
                &\leq L(t-t_0)\norm{u_1-u_2}\\
                &\leq LT\norm{u_1-u_2}\\
                &= \frac{(LT)^1}{1!}\cdot\norm{u_1-u_2}
            \end{align*}
            for all $t$.
            \item Now suppose inductively that $\norm{\Phi^j[u_1]-\Phi^j[u_2]}\leq(LT)^j/j!\cdot\norm{u_1-u_2}$. Then we have that
            \begin{align*}
                |\Phi^{j+1}[u_1](t)-\Phi^{j+1}[u_2](t)| &\leq \int_{t_0}^tL|\Phi^j[u_1](\tau)-\Phi^j[u_2](\tau)|\dd\tau\\
                &\leq \int_{t_0}^tL\cdot\frac{(LT)^j}{j!}\cdot\norm{u_1-u_2}\dd\tau\\
                &= \cdots\\
                &\leq \frac{(LT)^{j+1}}{j!}\cdot\norm{u_1-u_2}
            \end{align*}
            for all $t$, implying the desired result.
        \end{itemize}
        \item We now estimate the error between $y_m$ and $y$ in terms of $y_m$, alone. Indeed, we have from the above that
        \begin{align*}
            \norm{y_m-\Phi^N[y_m]} &\leq \sum_{j=0}^{N-1}\norm{\Phi^j[y_m]-\Phi^{j+1}[y_m]}\\
            &\leq \norm{y_m-\Phi[y_m]}\sum_{j=0}^{N-1}\frac{(TL)^j}{j!}\\
            \norm{y_m-y} &\leq \norm{y_m-\Phi[y_m]}\e[TL]
        \end{align*}
        where we get from the second to the third line by letting $N\to\infty$.
        \begin{itemize}
            \item The proof of the PET guarantees that $\norm{y_m-\Phi[y_m]}$ is small when $m$ is large, no matter whether $y_m$ itself converges or not.
            \item In fact, when $f\in C^1$, the error is estimated as
            \begin{equation*}
                \norm{y_m-y} \leq \frac{LT\e[TL]}{m}
            \end{equation*}
            for $L=\norm{f}_{C^1}$.
        \end{itemize}
    \end{itemize}
    \item Takeaway: This polygon method gives rise to an algorithm to solve ODEs. Theoretically, it converges much slower than the Picard iteration, but in practice, it has the advantage that we do not need to do any numerical integration. Indeed, to obtain the desired precision using the Picard iteration, the numerical integration will need more and more steps and the total accumulated error will not be less than this polygon method.
    \item Better difference methods include Runge-Kutta or Heun, but please refer to monographs on numerical ODEs for these.
\end{itemize}



\section{Asymptotic Stability}
\begin{itemize}
    \item \marginnote{11/9:}Going forward, we restrict ourselves to autonomous ODEs $y'=f(y)$, where $f:\R^n\to\R^n$ is a smooth vector field.
    \item For every $x\in\R^n$, the IVP
    \begin{equation*}
        y' = f(y)
        ,\quad
        y(0) = x
    \end{equation*}
    has a unique maximal solution $\phi_t(x)$ for $t\in I_x$.
    \item \textbf{Orbit}: The following set. \emph{Given by}
    \begin{equation*}
        \{\phi_t(x):t\in I_x\}
    \end{equation*}
    \item Let $K\subset\R^n$ be compact.
    \begin{itemize}
        \item Then there exists $T_K\in\R$ such that $\phi_t(x)$ is defined for all $x\in K$ and $|t|\leq T_K$.
        \item Moreover, the map from $K\to\R^n$ defined by $x\mapsto\phi_t(x)$ is injective due to uniqueness (and therefore a \textbf{homeomorphism}). We get one such map for each $t$.
        \item Similar to the diffeomorphism idea from \textcite{bib:DifferentialForms}.
    \end{itemize}
    \item \textbf{Invariant set}: A subset of $\R^n$ such that any orbit starting within it never leaves it.
    \item Compact invariant sets are quite interesting.
    \item Proposition: Let $\Omega\subset\R^n$ be a domain with a piecewise smooth boundary $\partial\Omega$. Suppose $f(x)$ is transversal to $\partial\Omega$ and inward pointing: That is, if $\nu$ is the inward pointing unit normal, then $f(x)\cdot\nu(x)\geq 0$ for all $x\in\partial\Omega$. Then $\bar{\Omega}$ is an invariant set: That is, any orbit starting from a point $\bar{\Omega}$ exists throughout the time and never leaves $\bar{\Omega}$.
    \begin{proof}[Proof idea]
        $x\in\partial\Omega$ ensures that $\phi_t(x)$ must be in $\Omega$ for small $t$. Hence, it suffices to consider $x\in\Omega$. In that case, pick the smallest $T>0$ such that $\phi_T(x)\in\partial\Omega$. Then by transversality it must turn back into $\Omega$.
    \end{proof}
    \item This simple proposition is especially useful when establishing global attraction of the orbits.
    \item \textbf{Fixed point}: A point in $\R^n$ at which $f$ evaluates to zero. \emph{Denoted by} $\bm{x_0}$.
    \begin{itemize}
        \item This means that the vector at $x_0$ is zero.
    \end{itemize}
    \item \textbf{Lyapunov stable} (fixed point): A fixed point $x_0$ such that for any neighborhood $B(x_0,\varepsilon)$, there exists a neighborhood $B(x_0,\delta)$ such that $\phi_t(x)\in B(x_0,\varepsilon)$ for any $t\geq 0$ and $x\in B(x_0,\delta)$.
    \item \textbf{Asymptotically stable} (fixed point): A Lyapunov stable fixed point $x_0$ such that $\phi_t(x)\to x_0$ as $t\to+\infty$ for $x\in B(x_0,\delta)$.
    \item Example of a system that is Lyapunov stable but not asymptotically stable: The system
    \begin{equation*}
        y' =
        \begin{pmatrix}
            0 & 1\\
            -1 & 0\\
        \end{pmatrix}
        y
    \end{equation*}
    where $A$ denotes a rotation.
    \begin{itemize}
        \item The orbits are concentric circles and never converge to 0.
    \end{itemize}
    \item Investigation: The local behavior near a fixed point.
    \begin{itemize}
        \item Consider $y'=f(y)$ as a perturbation of the linearized system $y'=f'(x_0)y$. In this case,
        \begin{equation*}
            f(x) = f'(x_0)(x-x_0)+O(|x-x_0|^2)
        \end{equation*}
        as $x\to x_0$.
    \end{itemize}
    \item Theorem: Let $f(x_0)=0$. If the eigenvalues of the linearization $A=f'(x_0)$ all have negative real parts, then the fixed point $x=x_0$ is asymptotically stable.
    \begin{proof}
        WLOG let $x_0=0$. Write $f(x)=Ax+g(x)$, where $g(x)=O(|x|^2)$. Since every $\lambda\in\sigma(A)$ has negative real part, there exist $a,C>0$ (let $C>1$ WLOG) such that
        \begin{equation*}
            |\e[tA]x| \leq C\e[-at]|x|
        \end{equation*}
        The $C$ arises because the matrix norm of $\e[tA]$ is bounded as $t\to +\infty$ if all eigenvalues are negative. The $\e[-at]$ arises similarly, and reflects the exponential decrease in magnitude happening along all subspaces on which $\e[tA]$ acts.\par
        Let $\delta$ be such that $|g(x)|\leq a|x|/2C$ when $|x|\leq\delta$. Now consider the IVP
        \begin{equation*}
            y' = Ay+g(y)
            ,\quad
            y(0) \in \bar{B}\left( 0,\frac{\delta}{2C} \right)
        \end{equation*}
        Then at least for small $t$ (i.e., $t$ such that $|y(t)|\leq\delta$),
        \begin{equation*}
            |y(t)| \leq C\e[-at]|y(0)|+\frac{a}{2C}\int_0^t\e[-a(t-\tau)]|y(\tau)|\dd\tau
        \end{equation*}
        It follows from Gr\"{o}nwall's inequality that
        \begin{equation*}
            \e[at]|y(t)| \leq C|y(0)|\e[at/2]
        \end{equation*}
        hence
        \begin{equation*}
            |y(t)| \leq \frac{\delta}{2}\e[-at/2]
            < \delta
        \end{equation*}
        Hence, any orbit of the system starting from $\bar{B}(0,\delta/2C)$ stays in $\bar{B}(0,\delta)$. So the maximal time of existence $T$ is $+\infty$. This is because if not then, then the IVP starting from $y(T)$ is still solvable, contradicting the definition of $T$. Thus, we have proven that
        \begin{equation*}
            |y(t)| \leq \frac{\delta}{2}\e[-at/2]
        \end{equation*}
        for all $t\geq 0$ as long as $|y(0)|\leq\delta/2C$.
    \end{proof}
    \item This is the last rigorous proof given in this course.
    \item A similar theorem:
    \item Theorem: Let $f(0)=0$. If one of the eigenvalues of $A=f'(0)$ has positive real part, then the fixed point $x=0$ is not Lyapunov stable.
    \item Initial application: Nonlinear mechanical system with frictions, e.g., ideal pendulum with friction.
    \begin{equation*}
        ml\theta''+b\theta' = -mg\sin\theta
    \end{equation*}
    \begin{itemize}
        \item Substitute $\eta=b/ml$ and $\omega=\theta'$ to get a nonlinear system
        \begin{equation*}
            \begin{pmatrix}
                \theta\\
                \omega\\
            \end{pmatrix}'
            =
            \begin{pmatrix}
                \omega\\
                -\eta\omega-g/l\sin\theta\\
            \end{pmatrix}
        \end{equation*}
        \item At the equilibrium position $(\theta,\omega)=(0,0)$, we have
        \begin{equation*}
            A =
            \begin{pNiceMatrix}
                \pdv{\theta}(\omega) & \pdv{\omega}(\omega)\\
                \pdv{\theta}(-\eta\omega-g/l\sin\theta) & \pdv{\omega}(-\eta\omega-g/l\sin\theta)\\
            \end{pNiceMatrix}
            \approx
            \begin{pmatrix}
                0 & 1\\
                -g/l & -\eta\\
            \end{pmatrix}
        \end{equation*}
        i.e.,
        \begin{equation*}
            \begin{pmatrix}
                \theta\\
                \omega\\
            \end{pmatrix}'
            =
            \begin{pmatrix}
                0 & 1\\
                -g/l & -\eta\\
            \end{pmatrix}
            \begin{pmatrix}
                \theta\\
                \omega\\
            \end{pmatrix}
            +O(|\theta|^2+|\omega|^2)
        \end{equation*}
        \begin{itemize}
            \item Since $\eta>0$, the eigenvalues have a common negative real part, so the equilibrium is asymptotically stable.
        \end{itemize}
        \item At the equilibrium $(\pi,0)$, we have
        \begin{equation*}
            \begin{pmatrix}
                \theta\\
                \omega\\
            \end{pmatrix}'
            =
            \begin{pmatrix}
                0 & 1\\
                g/l & -\eta\\
            \end{pmatrix}
            \begin{pmatrix}
                \theta-\pi\\
                \omega\\
            \end{pmatrix}
            +O(|\theta-\pi|^2+|\omega|^2)
        \end{equation*}
        \begin{itemize}
            \item For $\eta\geq 0$, there is one positive and one negative eigenvalue, so this equilibrium is unstable.
        \end{itemize}
        \item These results should make intuitive sense: If a pendulum is resting at the bottom, that is a stable equilibrium. If a pendulum is resting at the top, that is not a stable equilibrium.
    \end{itemize}
\end{itemize}



\section{Applications of the Lyapunov Method}
\begin{itemize}
    \item \marginnote{11/11:}Purely imaginary eigenvalues can still lead to Lyapunov stability.
    \item \textbf{Lyapunov function} (of a system $y'=f(y)$ with fixed point $x_0$ near $x_0$): A continuous real function on $\R^n$ such that the following two axioms hold. \emph{Denoted by} $\bm{L}$.
    \begin{enumerate}
        \item $L(x_0)=0$ and $L(x)>0$ for all $x\in\mathring{B}(x_0,\delta)=B(x_0,\delta)\setminus\{x_0\}$.
        \item $\dot{L}(x)=\nabla L(x)\cdot f(x)\leq 0$ for all $x\in\mathring{B}(x_0,\delta)=B(x_0,\delta)\setminus\{x_0\}$.
    \end{enumerate}
    \item Since
    \begin{equation*}
        \dv{t}L(\phi_t(x)) = \nabla L(\phi_t(x))\cdot f(\phi_t(x))
    \end{equation*}
    the second condition is equivalent to saying that the function $L$ is decreasing along the orbits starting near $x_0$.
    \item \textbf{Strict} (Lyapunov function): A Lyapunov function for which the decreasing is strict.
    \item Theorem: For the autonomous system $y'=f(y)$, a fixed point $x_0$ is
    \begin{enumerate}
        \item Stable if there is a Lyapunov function near it;
        \begin{proof}
            Pick a small number $\delta>0$. Let\footnote{Intuitively (in 2D), we take a ring around $x_0$, find the nonzero value of $L(x)$ at each point on the ring, and take the minimum among them. Imagine a circular valley with hills rising all around the bottommost point; we are essentially looking for the hill that rises the least.}
            \begin{equation*}
                m := \min\{L(x):|x-x_0|=\delta\}
            \end{equation*}
            Since $x_0$ does not satisfy $|x-x_0|=\delta>0$, we know from the first constraint on Lyapunov functions that $L(x)>0$ for all $x$ satisfying said relation. Thus, $m>0$. Consequently, any orbit starting from $\{x\mid L(x)<m\}\cap B(x_0,\delta)$ can never meet $\partial B(x_0,\delta)$ since $L(x)$ is decreasing along any orbit (and we would have to go up to get to the boundary). So $L(\phi_t(x))<m$ for all $x\in\{x\mid L(x)<m\}\cap B(x_0,\delta)$. But this means that $\{x\mid L(x)<m\}\cap B(x_0,\delta)$ is in fact an invariant set. Therefore, $x_0$ is Lyapunov stable.
        \end{proof}
        \item Asymptotically stable if there is a strict Lyapunov function near it.
        \begin{proof}
            If $x\in\{x\mid L(x)<m\}\cap B(x_0,\delta)$, then $L(\phi_t(x))$ is strictly decreasing. As $t\to +\infty$, $\phi_t(x)$ has a partial limit $z_0$, say $\phi_{t_k}(x)\to z_0$ (Lemma 6.6 of \textcite{bib:Teschl}). If $z_0\neq x_0$, then the orbit $\{\phi_t(z_0)\mid t\in I_{z_0}\}$ is not a single point: Since $L$ is a strict Lyapunov function, we have $L(\phi_t(z_0))<L(z_0)$ for all $t>0$. When $k$ is large, $\phi_{t_k}(x)$ is close to $z_0$, so by continuity,
            \begin{equation*}
                L(\phi_{t+t_k}(x)) = L(\phi_t(\phi_{t_k}(x))) < L(z_0)
            \end{equation*}
            But this contradicts $L(\phi_t(x))>L(z_0)$ (which we must have if there are arbitrarily large $t$ such that $\phi_t(x)$ is close to $z_0$). Therefore, $x_0=z_0$.
        \end{proof}
    \end{enumerate}
    \item If all eigenvalues of $A$ have negative real parts, then the perturbed system
    \begin{equation*}
        y' = Ay+g(y)
    \end{equation*}
    has a strict Lyapunov function around the fixed point $x=0$.
    \begin{itemize}
        \item This observation yields another proof of the stability theorem.
    \end{itemize}
    \item Advantage of the Lyapunov function: Can be constructed globally and thus gives us global information on the system.
    \item Examples in studying the global behavior of a phase portrait:
    \begin{itemize}
        \item Consider a mass point moving along the real axis in a potential field $U(x)$. Then
        \begin{equation*}
            mx'' = -U'(x)
        \end{equation*}
        \begin{itemize}
            \item The total energy
            \begin{equation*}
                E = \frac{m}{2}|x'|^2+U(x)
            \end{equation*}
            is always a constant along any solution.
            \item Introducing the velocity allows us to obtain a planar system
            \begin{equation*}
                \begin{pmatrix}
                    x\\
                    v\\
                \end{pmatrix}'
                =
                \begin{pmatrix}
                    v\\
                    -U'(x)/m\\
                \end{pmatrix}
            \end{equation*}
            \item Thus, $E(x,v)$ is a global Lyapunov function.
            \item Any fixed point of the system must be of the form $(x_0,0)$, where $U'(x_0)=0$.
            \begin{itemize}
                \item Intuitively, this means that the velocity must be zero (that makes sense) and the position must be such that we are at a critical point of the potential.
            \end{itemize}
            \item Because of this, the linearization at a fixed point must be of the following form.
            \begin{equation*}
                \begin{pmatrix}
                    v\\
                    -U'(x)/m\\
                \end{pmatrix}
                =
                \begin{pmatrix}
                    0 & 1\\
                    -U''(x_0)/m & 0\\
                \end{pmatrix}
                \begin{pmatrix}
                    x-x_0\\
                    v\\
                \end{pmatrix}
                +O(|x-x_0|^2+|v|^2)
            \end{equation*}
            \item Thus, $(x_0,0)$ is Lyapunov stable if $U$ has a nondegenerate local minimum at $x_0$ and unstable if $U$ has a nondegenerate local maximum at $x_0$.
            \begin{itemize}
                \item In the former case, the orbits near $(x_0,0)$ are closed curves, corresponding to periodic oscillations near $x_0$ (e.g., harmonic oscillator and ideal pendulum again).
            \end{itemize}
        \end{itemize}
        \item Prey-predator model with capacity:
        \begin{equation*}
            \begin{pmatrix}
                x\\
                y\\
            \end{pmatrix}'
            =
            \begin{pmatrix}
                (1-y-\lambda x)x\\
                \alpha(x-1-\mu y)y\\
            \end{pmatrix}
        \end{equation*}
        $\alpha,\lambda,\mu>0$.
        \begin{itemize}
            \item $x$ is the number of rabbits and $y$ is the number of wolves.
            \item Different ranges of $\lambda$ induce different global behavior (thus, this is an example of \textbf{bifurcation}).
            \item General observation: $(x,y)=(0,0)$ is a saddle point since the linearization there is $\diag(1,-\alpha)$.
            \item For $x=0$ or $y=0$, the equation is of separable form; the positive $x,y$-axes are invariant sets.
            \begin{itemize}
                \item Implication: No orbit in the first quadrant can escape it (compatible with meaning as population).
            \end{itemize}
            \item Jacobian:
            \begin{equation*}
                \begin{pmatrix}
                    1-y-2\lambda x & -x\\
                    \alpha y & \alpha(x-1)-2\alpha\mu y\\
                \end{pmatrix}
            \end{equation*}
            \item When $\lambda,\mu=0$, we're back to the Lotka-Volterra system, where there is a single fixed point $(1,1)$.
            \begin{itemize}
                \item In that case,
                \begin{equation*}
                    (y-\log y-1)+\alpha(x-\log x-1)
                \end{equation*}
                is a Lyapunov function.
                \item However, it is not a strict Lyapunov function since it is constant along any orbit.
                \item Moreover, the function is convex, so all level sets are closed curves around the fixed point.
                \item This is, indeed, the behavior we observe in Figure \ref{fig:LotkaVolteraSolns}.
            \end{itemize}
            \item Other cases: $\lambda\geq 1$.
            \begin{itemize}
                \item There is only one additional fixed point of interest: $(1/\lambda,0)$. Note that there are other fixed points, but these do not lie in the first quadrant and thus we are not interested.
                \item For $\lambda>1$, the fixed point is stable (a sink) and when $\lambda=1$, one eigenvalue is 0 since the linearization at that point is $\diag(-1,\alpha(1/\lambda-1))$.
            \end{itemize}
            \item $0<\lambda<1$.
            \begin{itemize}
                \item $(1/\lambda,0)$ becomes a saddle point, and there is a third fixed point
                \begin{equation*}
                    (x_0,y_0) = \left( \frac{1+\mu}{1+\mu\lambda},\frac{1-\lambda}{1+\mu\lambda} \right)
                \end{equation*}
            \end{itemize}
            \item More on this case in Chapter 7 of \textcite{bib:Teschl}. This is relevant here!
        \end{itemize}
    \end{itemize}
\end{itemize}




\end{document}